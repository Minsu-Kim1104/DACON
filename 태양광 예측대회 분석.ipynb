{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train/train.csv')\n",
    "sub = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 로드\n",
    "for i in range(0,81):\n",
    "    s1 = \"test_%d = pd.read_csv('./test/%d.csv')\"%(i,i)\n",
    "    exec(s1)\n",
    "    \n",
    "test = pd.DataFrame()\n",
    "for i in range(0,81):\n",
    "    s1 = \"test = pd.concat([test, test_%d], axis = 0)\"%(i)\n",
    "    exec(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>DHI</th>\n",
       "      <th>DNI</th>\n",
       "      <th>WS</th>\n",
       "      <th>RH</th>\n",
       "      <th>T</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Day</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.027802</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.038477</td>\n",
       "      <td>-0.127688</td>\n",
       "      <td>0.072897</td>\n",
       "      <td>-0.002505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hour</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>-0.018094</td>\n",
       "      <td>-0.055903</td>\n",
       "      <td>-0.084275</td>\n",
       "      <td>0.105528</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minute</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>-0.009522</td>\n",
       "      <td>0.008839</td>\n",
       "      <td>-0.007646</td>\n",
       "      <td>-0.000240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DHI</th>\n",
       "      <td>-0.027802</td>\n",
       "      <td>0.029905</td>\n",
       "      <td>-0.001305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.288294</td>\n",
       "      <td>0.203286</td>\n",
       "      <td>-0.478503</td>\n",
       "      <td>0.457813</td>\n",
       "      <td>0.666908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNI</th>\n",
       "      <td>0.021901</td>\n",
       "      <td>-0.018094</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>0.288294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.219555</td>\n",
       "      <td>-0.611184</td>\n",
       "      <td>0.402460</td>\n",
       "      <td>0.833547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WS</th>\n",
       "      <td>0.038477</td>\n",
       "      <td>-0.055903</td>\n",
       "      <td>-0.009522</td>\n",
       "      <td>0.203286</td>\n",
       "      <td>0.219555</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.230035</td>\n",
       "      <td>0.027693</td>\n",
       "      <td>0.238521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RH</th>\n",
       "      <td>-0.127688</td>\n",
       "      <td>-0.084275</td>\n",
       "      <td>0.008839</td>\n",
       "      <td>-0.478503</td>\n",
       "      <td>-0.611184</td>\n",
       "      <td>-0.230035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.532777</td>\n",
       "      <td>-0.677178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>0.072897</td>\n",
       "      <td>0.105528</td>\n",
       "      <td>-0.007646</td>\n",
       "      <td>0.457813</td>\n",
       "      <td>0.402460</td>\n",
       "      <td>0.027693</td>\n",
       "      <td>-0.532777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.561990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TARGET</th>\n",
       "      <td>-0.002505</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.666908</td>\n",
       "      <td>0.833547</td>\n",
       "      <td>0.238521</td>\n",
       "      <td>-0.677178</td>\n",
       "      <td>0.561990</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Day      Hour    Minute       DHI       DNI        WS        RH  \\\n",
       "Day     1.000000  0.000000  0.000000 -0.027802  0.021901  0.038477 -0.127688   \n",
       "Hour    0.000000  1.000000  0.000000  0.029905 -0.018094 -0.055903 -0.084275   \n",
       "Minute  0.000000  0.000000  1.000000 -0.001305 -0.000997 -0.009522  0.008839   \n",
       "DHI    -0.027802  0.029905 -0.001305  1.000000  0.288294  0.203286 -0.478503   \n",
       "DNI     0.021901 -0.018094 -0.000997  0.288294  1.000000  0.219555 -0.611184   \n",
       "WS      0.038477 -0.055903 -0.009522  0.203286  0.219555  1.000000 -0.230035   \n",
       "RH     -0.127688 -0.084275  0.008839 -0.478503 -0.611184 -0.230035  1.000000   \n",
       "T       0.072897  0.105528 -0.007646  0.457813  0.402460  0.027693 -0.532777   \n",
       "TARGET -0.002505  0.003817 -0.000240  0.666908  0.833547  0.238521 -0.677178   \n",
       "\n",
       "               T    TARGET  \n",
       "Day     0.072897 -0.002505  \n",
       "Hour    0.105528  0.003817  \n",
       "Minute -0.007646 -0.000240  \n",
       "DHI     0.457813  0.666908  \n",
       "DNI     0.402460  0.833547  \n",
       "WS      0.027693  0.238521  \n",
       "RH     -0.532777 -0.677178  \n",
       "T       1.000000  0.561990  \n",
       "TARGET  0.561990  1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#타겟변수\n",
    "train_day_1 = train.loc[train['Day'] > 0].reset_index(drop = True)\n",
    "train_day_2 = train.loc[train['Day'] > 1].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_1 : 1일 뒤를 예측할 모델, train_2 : 2일 뒤를 예측할 모델\n",
    "train_1 = pd.concat([train, train_day_1.rename(columns = {'TARGET' : 'TARGET_1'})['TARGET_1']], axis = 1).dropna(axis = 0)\n",
    "train_2 = pd.concat([train, train_day_2.rename(columns = {'TARGET' : 'TARGET_2'})['TARGET_2']], axis = 1).dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kim\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#7일간격으로 Day변경\n",
    "day_list_2 = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "for j in day_list_2: \n",
    "    for k in range(0,2025,7):\n",
    "        train_1['Day'].loc[train_1['Day'] == j+k] = j\n",
    "        train_2['Day'].loc[train_2['Day'] == j+k] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = test.copy()\n",
    "test_2 = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TARGET의 흐름과 유사한 데이터만 뽑아서 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = test_1.loc[test_1['Day'] == 6].reset_index(drop = True)\n",
    "test_2 = test_2.loc[test_2['Day'] == 6].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = train_1.drop(['Day', 'Minute', 'TARGET_1'], axis = 1)\n",
    "x_train_2 = train_2.drop(['Day', 'Minute', 'TARGET_2'], axis = 1)\n",
    "\n",
    "y_train_1 = train_1['TARGET_1']\n",
    "y_train_2 = train_2['TARGET_2']\n",
    "\n",
    "x_test_1 = test_1.drop(['Day', 'Minute'], axis = 1)\n",
    "x_test_2 = test_2.drop(['Day', 'Minute'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kim\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#TARGET의 형태가 비슷한 범주끼리 0으로 처리\n",
    "num_list = [1, 2, 3, 4, 5, 20, 21, 22, 23]\n",
    "for i in num_list:\n",
    "    x_train_1['Hour'].loc[x_train_1['Hour'] == i] = 0\n",
    "    x_train_2['Hour'].loc[x_train_2['Hour'] == i] = 0\n",
    "    x_test_1['Hour'].loc[x_test_1['Hour'] == i] = 0\n",
    "    x_test_2['Hour'].loc[x_test_2['Hour'] == i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hour, DHI, DNI, WS, RH, T, TARGET만 넣고 돌렸을 때 (Hour만 전처리) = 1.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.40733\tvalid_1's quantile: 1.3862\n",
      "[1000]\ttraining's quantile: 1.39286\tvalid_1's quantile: 1.37484\n",
      "[1500]\ttraining's quantile: 1.3821\tvalid_1's quantile: 1.36711\n",
      "[2000]\ttraining's quantile: 1.37135\tvalid_1's quantile: 1.3605\n",
      "[2500]\ttraining's quantile: 1.36614\tvalid_1's quantile: 1.35765\n",
      "[3000]\ttraining's quantile: 1.35956\tvalid_1's quantile: 1.35315\n",
      "[3500]\ttraining's quantile: 1.35567\tvalid_1's quantile: 1.34928\n",
      "[4000]\ttraining's quantile: 1.35121\tvalid_1's quantile: 1.34793\n",
      "[4500]\ttraining's quantile: 1.3476\tvalid_1's quantile: 1.34577\n",
      "[5000]\ttraining's quantile: 1.34424\tvalid_1's quantile: 1.34461\n",
      "[5500]\ttraining's quantile: 1.34335\tvalid_1's quantile: 1.34416\n",
      "[6000]\ttraining's quantile: 1.34061\tvalid_1's quantile: 1.34353\n",
      "[6500]\ttraining's quantile: 1.33848\tvalid_1's quantile: 1.34301\n",
      "[7000]\ttraining's quantile: 1.33592\tvalid_1's quantile: 1.34265\n",
      "[7500]\ttraining's quantile: 1.33333\tvalid_1's quantile: 1.3422\n",
      "[8000]\ttraining's quantile: 1.33232\tvalid_1's quantile: 1.3418\n",
      "Early stopping, best iteration is:\n",
      "[8097]\ttraining's quantile: 1.33223\tvalid_1's quantile: 1.34177\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.26944\tvalid_1's quantile: 2.24884\n",
      "[1000]\ttraining's quantile: 2.23698\tvalid_1's quantile: 2.22999\n",
      "[1500]\ttraining's quantile: 2.21859\tvalid_1's quantile: 2.2199\n",
      "[2000]\ttraining's quantile: 2.20939\tvalid_1's quantile: 2.21326\n",
      "[2500]\ttraining's quantile: 2.2005\tvalid_1's quantile: 2.20584\n",
      "[3000]\ttraining's quantile: 2.19352\tvalid_1's quantile: 2.2003\n",
      "[3500]\ttraining's quantile: 2.18823\tvalid_1's quantile: 2.19625\n",
      "[4000]\ttraining's quantile: 2.18247\tvalid_1's quantile: 2.19265\n",
      "[4500]\ttraining's quantile: 2.17499\tvalid_1's quantile: 2.18919\n",
      "Early stopping, best iteration is:\n",
      "[4767]\ttraining's quantile: 2.17253\tvalid_1's quantile: 2.18823\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.72778\tvalid_1's quantile: 2.70809\n",
      "[1000]\ttraining's quantile: 2.67607\tvalid_1's quantile: 2.66972\n",
      "[1500]\ttraining's quantile: 2.65739\tvalid_1's quantile: 2.65743\n",
      "[2000]\ttraining's quantile: 2.64373\tvalid_1's quantile: 2.64979\n",
      "[2500]\ttraining's quantile: 2.63112\tvalid_1's quantile: 2.64274\n",
      "[3000]\ttraining's quantile: 2.61924\tvalid_1's quantile: 2.636\n",
      "[3500]\ttraining's quantile: 2.60803\tvalid_1's quantile: 2.62915\n",
      "[4000]\ttraining's quantile: 2.60031\tvalid_1's quantile: 2.62338\n",
      "[4500]\ttraining's quantile: 2.59489\tvalid_1's quantile: 2.62043\n",
      "[5000]\ttraining's quantile: 2.58719\tvalid_1's quantile: 2.61617\n",
      "[5500]\ttraining's quantile: 2.58283\tvalid_1's quantile: 2.61351\n",
      "[6000]\ttraining's quantile: 2.57811\tvalid_1's quantile: 2.61069\n",
      "[6500]\ttraining's quantile: 2.57425\tvalid_1's quantile: 2.60849\n",
      "[7000]\ttraining's quantile: 2.56908\tvalid_1's quantile: 2.60434\n",
      "[7500]\ttraining's quantile: 2.56453\tvalid_1's quantile: 2.60165\n",
      "[8000]\ttraining's quantile: 2.5615\tvalid_1's quantile: 2.59957\n",
      "[8500]\ttraining's quantile: 2.55955\tvalid_1's quantile: 2.59891\n",
      "[9000]\ttraining's quantile: 2.55721\tvalid_1's quantile: 2.5982\n",
      "[9500]\ttraining's quantile: 2.55436\tvalid_1's quantile: 2.59726\n",
      "[10000]\ttraining's quantile: 2.54919\tvalid_1's quantile: 2.59618\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.54919\tvalid_1's quantile: 2.59618\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.88958\tvalid_1's quantile: 2.87138\n",
      "[1000]\ttraining's quantile: 2.81609\tvalid_1's quantile: 2.80879\n",
      "[1500]\ttraining's quantile: 2.79208\tvalid_1's quantile: 2.78883\n",
      "[2000]\ttraining's quantile: 2.774\tvalid_1's quantile: 2.77507\n",
      "[2500]\ttraining's quantile: 2.76089\tvalid_1's quantile: 2.76574\n",
      "[3000]\ttraining's quantile: 2.75098\tvalid_1's quantile: 2.7599\n",
      "[3500]\ttraining's quantile: 2.74174\tvalid_1's quantile: 2.75467\n",
      "[4000]\ttraining's quantile: 2.73399\tvalid_1's quantile: 2.74958\n",
      "[4500]\ttraining's quantile: 2.72788\tvalid_1's quantile: 2.74578\n",
      "[5000]\ttraining's quantile: 2.72201\tvalid_1's quantile: 2.7419\n",
      "[5500]\ttraining's quantile: 2.71692\tvalid_1's quantile: 2.73891\n",
      "[6000]\ttraining's quantile: 2.71103\tvalid_1's quantile: 2.73438\n",
      "[6500]\ttraining's quantile: 2.70654\tvalid_1's quantile: 2.73146\n",
      "[7000]\ttraining's quantile: 2.70278\tvalid_1's quantile: 2.72995\n",
      "[7500]\ttraining's quantile: 2.69898\tvalid_1's quantile: 2.72766\n",
      "[8000]\ttraining's quantile: 2.69512\tvalid_1's quantile: 2.72538\n",
      "[8500]\ttraining's quantile: 2.68988\tvalid_1's quantile: 2.72172\n",
      "[9000]\ttraining's quantile: 2.68465\tvalid_1's quantile: 2.71787\n",
      "[9500]\ttraining's quantile: 2.67933\tvalid_1's quantile: 2.71612\n",
      "[10000]\ttraining's quantile: 2.67679\tvalid_1's quantile: 2.71461\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.67679\tvalid_1's quantile: 2.71461\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.84986\tvalid_1's quantile: 2.82509\n",
      "[1000]\ttraining's quantile: 2.73908\tvalid_1's quantile: 2.72573\n",
      "[1500]\ttraining's quantile: 2.70427\tvalid_1's quantile: 2.69878\n",
      "[2000]\ttraining's quantile: 2.68147\tvalid_1's quantile: 2.68092\n",
      "[2500]\ttraining's quantile: 2.66742\tvalid_1's quantile: 2.67105\n",
      "[3000]\ttraining's quantile: 2.65572\tvalid_1's quantile: 2.66442\n",
      "[3500]\ttraining's quantile: 2.64603\tvalid_1's quantile: 2.65764\n",
      "[4000]\ttraining's quantile: 2.63598\tvalid_1's quantile: 2.65083\n",
      "[4500]\ttraining's quantile: 2.62975\tvalid_1's quantile: 2.64697\n",
      "[5000]\ttraining's quantile: 2.6234\tvalid_1's quantile: 2.64319\n",
      "[5500]\ttraining's quantile: 2.61867\tvalid_1's quantile: 2.63917\n",
      "[6000]\ttraining's quantile: 2.61417\tvalid_1's quantile: 2.63688\n",
      "[6500]\ttraining's quantile: 2.61023\tvalid_1's quantile: 2.63425\n",
      "[7000]\ttraining's quantile: 2.60697\tvalid_1's quantile: 2.63285\n",
      "[7500]\ttraining's quantile: 2.6034\tvalid_1's quantile: 2.63048\n",
      "[8000]\ttraining's quantile: 2.60007\tvalid_1's quantile: 2.62877\n",
      "[8500]\ttraining's quantile: 2.59762\tvalid_1's quantile: 2.62762\n",
      "[9000]\ttraining's quantile: 2.59391\tvalid_1's quantile: 2.62544\n",
      "[9500]\ttraining's quantile: 2.59162\tvalid_1's quantile: 2.6239\n",
      "[10000]\ttraining's quantile: 2.58871\tvalid_1's quantile: 2.62245\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.58871\tvalid_1's quantile: 2.62245\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.61783\tvalid_1's quantile: 2.60638\n",
      "[1000]\ttraining's quantile: 2.48\tvalid_1's quantile: 2.47773\n",
      "[1500]\ttraining's quantile: 2.44638\tvalid_1's quantile: 2.44811\n",
      "[2000]\ttraining's quantile: 2.42144\tvalid_1's quantile: 2.4271\n",
      "[2500]\ttraining's quantile: 2.4046\tvalid_1's quantile: 2.4131\n",
      "[3000]\ttraining's quantile: 2.39549\tvalid_1's quantile: 2.40571\n",
      "[3500]\ttraining's quantile: 2.38469\tvalid_1's quantile: 2.39671\n",
      "[4000]\ttraining's quantile: 2.37636\tvalid_1's quantile: 2.38861\n",
      "[4500]\ttraining's quantile: 2.36998\tvalid_1's quantile: 2.38341\n",
      "[5000]\ttraining's quantile: 2.36362\tvalid_1's quantile: 2.37953\n",
      "[5500]\ttraining's quantile: 2.35899\tvalid_1's quantile: 2.37693\n",
      "[6000]\ttraining's quantile: 2.35516\tvalid_1's quantile: 2.37543\n",
      "[6500]\ttraining's quantile: 2.35193\tvalid_1's quantile: 2.37321\n",
      "[7000]\ttraining's quantile: 2.34879\tvalid_1's quantile: 2.37136\n",
      "[7500]\ttraining's quantile: 2.34586\tvalid_1's quantile: 2.3693\n",
      "[8000]\ttraining's quantile: 2.34219\tvalid_1's quantile: 2.36669\n",
      "[8500]\ttraining's quantile: 2.33868\tvalid_1's quantile: 2.36474\n",
      "[9000]\ttraining's quantile: 2.33556\tvalid_1's quantile: 2.36283\n",
      "[9500]\ttraining's quantile: 2.33396\tvalid_1's quantile: 2.36152\n",
      "[10000]\ttraining's quantile: 2.33187\tvalid_1's quantile: 2.35994\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.33187\tvalid_1's quantile: 2.35994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.22451\tvalid_1's quantile: 2.22858\n",
      "[1000]\ttraining's quantile: 2.08163\tvalid_1's quantile: 2.0898\n",
      "[1500]\ttraining's quantile: 2.04838\tvalid_1's quantile: 2.05621\n",
      "[2000]\ttraining's quantile: 2.02721\tvalid_1's quantile: 2.03413\n",
      "[2500]\ttraining's quantile: 2.0143\tvalid_1's quantile: 2.02213\n",
      "[3000]\ttraining's quantile: 2.00292\tvalid_1's quantile: 2.01138\n",
      "[3500]\ttraining's quantile: 1.99274\tvalid_1's quantile: 2.00229\n",
      "[4000]\ttraining's quantile: 1.98429\tvalid_1's quantile: 1.99454\n",
      "[4500]\ttraining's quantile: 1.97948\tvalid_1's quantile: 1.99099\n",
      "[5000]\ttraining's quantile: 1.97408\tvalid_1's quantile: 1.98676\n",
      "[5500]\ttraining's quantile: 1.96918\tvalid_1's quantile: 1.98371\n",
      "[6000]\ttraining's quantile: 1.96411\tvalid_1's quantile: 1.97979\n",
      "[6500]\ttraining's quantile: 1.95993\tvalid_1's quantile: 1.97726\n",
      "[7000]\ttraining's quantile: 1.95722\tvalid_1's quantile: 1.97542\n",
      "[7500]\ttraining's quantile: 1.95376\tvalid_1's quantile: 1.97304\n",
      "[8000]\ttraining's quantile: 1.95071\tvalid_1's quantile: 1.97117\n",
      "[8500]\ttraining's quantile: 1.94738\tvalid_1's quantile: 1.96911\n",
      "[9000]\ttraining's quantile: 1.9446\tvalid_1's quantile: 1.96795\n",
      "[9500]\ttraining's quantile: 1.94191\tvalid_1's quantile: 1.96649\n",
      "[10000]\ttraining's quantile: 1.93927\tvalid_1's quantile: 1.96538\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.93927\tvalid_1's quantile: 1.96538\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.70626\tvalid_1's quantile: 1.70932\n",
      "[1000]\ttraining's quantile: 1.57469\tvalid_1's quantile: 1.5726\n",
      "[1500]\ttraining's quantile: 1.54619\tvalid_1's quantile: 1.54529\n",
      "[2000]\ttraining's quantile: 1.53015\tvalid_1's quantile: 1.52889\n",
      "[2500]\ttraining's quantile: 1.51748\tvalid_1's quantile: 1.51736\n",
      "[3000]\ttraining's quantile: 1.50964\tvalid_1's quantile: 1.51028\n",
      "[3500]\ttraining's quantile: 1.50122\tvalid_1's quantile: 1.50167\n",
      "[4000]\ttraining's quantile: 1.49553\tvalid_1's quantile: 1.4965\n",
      "[4500]\ttraining's quantile: 1.48937\tvalid_1's quantile: 1.49047\n",
      "[5000]\ttraining's quantile: 1.48417\tvalid_1's quantile: 1.48514\n",
      "[5500]\ttraining's quantile: 1.47911\tvalid_1's quantile: 1.48117\n",
      "[6000]\ttraining's quantile: 1.47556\tvalid_1's quantile: 1.47853\n",
      "[6500]\ttraining's quantile: 1.47206\tvalid_1's quantile: 1.47629\n",
      "[7000]\ttraining's quantile: 1.46921\tvalid_1's quantile: 1.47438\n",
      "[7500]\ttraining's quantile: 1.46583\tvalid_1's quantile: 1.47179\n",
      "[8000]\ttraining's quantile: 1.46171\tvalid_1's quantile: 1.46935\n",
      "[8500]\ttraining's quantile: 1.45946\tvalid_1's quantile: 1.4679\n",
      "[9000]\ttraining's quantile: 1.457\tvalid_1's quantile: 1.46599\n",
      "[9500]\ttraining's quantile: 1.4551\tvalid_1's quantile: 1.46487\n",
      "[10000]\ttraining's quantile: 1.45323\tvalid_1's quantile: 1.46345\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.45323\tvalid_1's quantile: 1.46345\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.05172\tvalid_1's quantile: 1.05906\n",
      "[1000]\ttraining's quantile: 0.933308\tvalid_1's quantile: 0.936757\n",
      "[1500]\ttraining's quantile: 0.922598\tvalid_1's quantile: 0.926144\n",
      "[2000]\ttraining's quantile: 0.917012\tvalid_1's quantile: 0.920612\n",
      "[2500]\ttraining's quantile: 0.912274\tvalid_1's quantile: 0.915973\n",
      "[3000]\ttraining's quantile: 0.906939\tvalid_1's quantile: 0.91093\n",
      "[3500]\ttraining's quantile: 0.903274\tvalid_1's quantile: 0.907638\n",
      "[4000]\ttraining's quantile: 0.900634\tvalid_1's quantile: 0.905258\n",
      "[4500]\ttraining's quantile: 0.897219\tvalid_1's quantile: 0.902025\n",
      "[5000]\ttraining's quantile: 0.893974\tvalid_1's quantile: 0.899076\n",
      "[5500]\ttraining's quantile: 0.891486\tvalid_1's quantile: 0.896774\n",
      "[6000]\ttraining's quantile: 0.889297\tvalid_1's quantile: 0.894475\n",
      "[6500]\ttraining's quantile: 0.88785\tvalid_1's quantile: 0.892994\n",
      "[7000]\ttraining's quantile: 0.886049\tvalid_1's quantile: 0.891077\n",
      "[7500]\ttraining's quantile: 0.884793\tvalid_1's quantile: 0.889924\n",
      "[8000]\ttraining's quantile: 0.883652\tvalid_1's quantile: 0.888832\n",
      "[8500]\ttraining's quantile: 0.882321\tvalid_1's quantile: 0.887493\n",
      "[9000]\ttraining's quantile: 0.880895\tvalid_1's quantile: 0.886035\n",
      "[9500]\ttraining's quantile: 0.879513\tvalid_1's quantile: 0.884673\n",
      "[10000]\ttraining's quantile: 0.878532\tvalid_1's quantile: 0.883824\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.878532\tvalid_1's quantile: 0.883824\n",
      "1\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.40443\tvalid_1's quantile: 1.39656\n",
      "[1000]\ttraining's quantile: 1.3924\tvalid_1's quantile: 1.38701\n",
      "[1500]\ttraining's quantile: 1.38507\tvalid_1's quantile: 1.38123\n",
      "[2000]\ttraining's quantile: 1.3765\tvalid_1's quantile: 1.37534\n",
      "[2500]\ttraining's quantile: 1.37196\tvalid_1's quantile: 1.37351\n",
      "[3000]\ttraining's quantile: 1.36511\tvalid_1's quantile: 1.37002\n",
      "[3500]\ttraining's quantile: 1.36097\tvalid_1's quantile: 1.36872\n",
      "[4000]\ttraining's quantile: 1.35505\tvalid_1's quantile: 1.36503\n",
      "[4500]\ttraining's quantile: 1.35126\tvalid_1's quantile: 1.3633\n",
      "[5000]\ttraining's quantile: 1.34832\tvalid_1's quantile: 1.36249\n",
      "[5500]\ttraining's quantile: 1.34443\tvalid_1's quantile: 1.36139\n",
      "[6000]\ttraining's quantile: 1.34013\tvalid_1's quantile: 1.36023\n",
      "[6500]\ttraining's quantile: 1.33587\tvalid_1's quantile: 1.35921\n",
      "[7000]\ttraining's quantile: 1.3339\tvalid_1's quantile: 1.35849\n",
      "[7500]\ttraining's quantile: 1.3331\tvalid_1's quantile: 1.35813\n",
      "[8000]\ttraining's quantile: 1.3318\tvalid_1's quantile: 1.35763\n",
      "[8500]\ttraining's quantile: 1.32946\tvalid_1's quantile: 1.35656\n",
      "Early stopping, best iteration is:\n",
      "[8576]\ttraining's quantile: 1.32892\tvalid_1's quantile: 1.35628\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.27031\tvalid_1's quantile: 2.23905\n",
      "[1000]\ttraining's quantile: 2.23873\tvalid_1's quantile: 2.20952\n",
      "[1500]\ttraining's quantile: 2.22764\tvalid_1's quantile: 2.20394\n",
      "[2000]\ttraining's quantile: 2.21186\tvalid_1's quantile: 2.19605\n",
      "[2500]\ttraining's quantile: 2.19905\tvalid_1's quantile: 2.19123\n",
      "[3000]\ttraining's quantile: 2.19127\tvalid_1's quantile: 2.1874\n",
      "[3500]\ttraining's quantile: 2.18517\tvalid_1's quantile: 2.18513\n",
      "[4000]\ttraining's quantile: 2.1784\tvalid_1's quantile: 2.18296\n",
      "[4500]\ttraining's quantile: 2.17136\tvalid_1's quantile: 2.18124\n",
      "[5000]\ttraining's quantile: 2.16611\tvalid_1's quantile: 2.18045\n",
      "[5500]\ttraining's quantile: 2.15943\tvalid_1's quantile: 2.17775\n",
      "[6000]\ttraining's quantile: 2.15355\tvalid_1's quantile: 2.17528\n",
      "[6500]\ttraining's quantile: 2.14953\tvalid_1's quantile: 2.17411\n",
      "[7000]\ttraining's quantile: 2.14441\tvalid_1's quantile: 2.1728\n",
      "[7500]\ttraining's quantile: 2.13841\tvalid_1's quantile: 2.17106\n",
      "[8000]\ttraining's quantile: 2.13333\tvalid_1's quantile: 2.16884\n",
      "[8500]\ttraining's quantile: 2.13029\tvalid_1's quantile: 2.16736\n",
      "[9000]\ttraining's quantile: 2.12775\tvalid_1's quantile: 2.16643\n",
      "Early stopping, best iteration is:\n",
      "[8980]\ttraining's quantile: 2.12786\tvalid_1's quantile: 2.16641\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.73376\tvalid_1's quantile: 2.67975\n",
      "[1000]\ttraining's quantile: 2.68557\tvalid_1's quantile: 2.63258\n",
      "[1500]\ttraining's quantile: 2.66726\tvalid_1's quantile: 2.6198\n",
      "[2000]\ttraining's quantile: 2.64931\tvalid_1's quantile: 2.60834\n",
      "[2500]\ttraining's quantile: 2.63512\tvalid_1's quantile: 2.60061\n",
      "[3000]\ttraining's quantile: 2.62456\tvalid_1's quantile: 2.59514\n",
      "[3500]\ttraining's quantile: 2.61316\tvalid_1's quantile: 2.59037\n",
      "[4000]\ttraining's quantile: 2.60343\tvalid_1's quantile: 2.58557\n",
      "[4500]\ttraining's quantile: 2.59602\tvalid_1's quantile: 2.58181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000]\ttraining's quantile: 2.59089\tvalid_1's quantile: 2.57871\n",
      "[5500]\ttraining's quantile: 2.58585\tvalid_1's quantile: 2.57835\n",
      "Early stopping, best iteration is:\n",
      "[5316]\ttraining's quantile: 2.58757\tvalid_1's quantile: 2.57778\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.90008\tvalid_1's quantile: 2.84817\n",
      "[1000]\ttraining's quantile: 2.82711\tvalid_1's quantile: 2.77944\n",
      "[1500]\ttraining's quantile: 2.80069\tvalid_1's quantile: 2.75745\n",
      "[2000]\ttraining's quantile: 2.78333\tvalid_1's quantile: 2.74486\n",
      "[2500]\ttraining's quantile: 2.77043\tvalid_1's quantile: 2.73648\n",
      "[3000]\ttraining's quantile: 2.75782\tvalid_1's quantile: 2.73134\n",
      "[3500]\ttraining's quantile: 2.74823\tvalid_1's quantile: 2.72602\n",
      "[4000]\ttraining's quantile: 2.74163\tvalid_1's quantile: 2.72179\n",
      "[4500]\ttraining's quantile: 2.73507\tvalid_1's quantile: 2.71913\n",
      "[5000]\ttraining's quantile: 2.72841\tvalid_1's quantile: 2.71449\n",
      "[5500]\ttraining's quantile: 2.72278\tvalid_1's quantile: 2.71046\n",
      "[6000]\ttraining's quantile: 2.71684\tvalid_1's quantile: 2.70685\n",
      "[6500]\ttraining's quantile: 2.71062\tvalid_1's quantile: 2.70245\n",
      "[7000]\ttraining's quantile: 2.7058\tvalid_1's quantile: 2.7004\n",
      "[7500]\ttraining's quantile: 2.7019\tvalid_1's quantile: 2.69776\n",
      "[8000]\ttraining's quantile: 2.69787\tvalid_1's quantile: 2.69708\n",
      "Early stopping, best iteration is:\n",
      "[8071]\ttraining's quantile: 2.69742\tvalid_1's quantile: 2.69693\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.85453\tvalid_1's quantile: 2.81192\n",
      "[1000]\ttraining's quantile: 2.74578\tvalid_1's quantile: 2.70575\n",
      "[1500]\ttraining's quantile: 2.70744\tvalid_1's quantile: 2.67521\n",
      "[2000]\ttraining's quantile: 2.68758\tvalid_1's quantile: 2.6615\n",
      "[2500]\ttraining's quantile: 2.67524\tvalid_1's quantile: 2.65353\n",
      "[3000]\ttraining's quantile: 2.66564\tvalid_1's quantile: 2.64848\n",
      "[3500]\ttraining's quantile: 2.65629\tvalid_1's quantile: 2.64298\n",
      "[4000]\ttraining's quantile: 2.6477\tvalid_1's quantile: 2.6396\n",
      "[4500]\ttraining's quantile: 2.64135\tvalid_1's quantile: 2.63611\n",
      "[5000]\ttraining's quantile: 2.63439\tvalid_1's quantile: 2.63152\n",
      "[5500]\ttraining's quantile: 2.62937\tvalid_1's quantile: 2.62847\n",
      "[6000]\ttraining's quantile: 2.62429\tvalid_1's quantile: 2.62607\n",
      "[6500]\ttraining's quantile: 2.61998\tvalid_1's quantile: 2.62386\n",
      "[7000]\ttraining's quantile: 2.61552\tvalid_1's quantile: 2.62187\n",
      "[7500]\ttraining's quantile: 2.61242\tvalid_1's quantile: 2.62047\n",
      "[8000]\ttraining's quantile: 2.60913\tvalid_1's quantile: 2.61951\n",
      "[8500]\ttraining's quantile: 2.6057\tvalid_1's quantile: 2.61783\n",
      "[9000]\ttraining's quantile: 2.60325\tvalid_1's quantile: 2.61728\n",
      "[9500]\ttraining's quantile: 2.59974\tvalid_1's quantile: 2.61556\n",
      "[10000]\ttraining's quantile: 2.59595\tvalid_1's quantile: 2.6136\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.59595\tvalid_1's quantile: 2.6136\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.6144\tvalid_1's quantile: 2.59507\n",
      "[1000]\ttraining's quantile: 2.4752\tvalid_1's quantile: 2.46787\n",
      "[1500]\ttraining's quantile: 2.44176\tvalid_1's quantile: 2.44099\n",
      "[2000]\ttraining's quantile: 2.42038\tvalid_1's quantile: 2.42184\n",
      "[2500]\ttraining's quantile: 2.40555\tvalid_1's quantile: 2.4094\n",
      "[3000]\ttraining's quantile: 2.39558\tvalid_1's quantile: 2.40061\n",
      "[3500]\ttraining's quantile: 2.38786\tvalid_1's quantile: 2.39431\n",
      "[4000]\ttraining's quantile: 2.3813\tvalid_1's quantile: 2.38845\n",
      "[4500]\ttraining's quantile: 2.37406\tvalid_1's quantile: 2.38438\n",
      "[5000]\ttraining's quantile: 2.36788\tvalid_1's quantile: 2.37987\n",
      "[5500]\ttraining's quantile: 2.36233\tvalid_1's quantile: 2.37522\n",
      "[6000]\ttraining's quantile: 2.35653\tvalid_1's quantile: 2.37068\n",
      "[6500]\ttraining's quantile: 2.35271\tvalid_1's quantile: 2.36936\n",
      "Early stopping, best iteration is:\n",
      "[6574]\ttraining's quantile: 2.35235\tvalid_1's quantile: 2.3693\n",
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.21346\tvalid_1's quantile: 2.208\n",
      "[1000]\ttraining's quantile: 2.07446\tvalid_1's quantile: 2.07317\n",
      "[1500]\ttraining's quantile: 2.03894\tvalid_1's quantile: 2.04108\n",
      "[2000]\ttraining's quantile: 2.01849\tvalid_1's quantile: 2.02453\n",
      "[2500]\ttraining's quantile: 2.00583\tvalid_1's quantile: 2.01418\n",
      "[3000]\ttraining's quantile: 1.99438\tvalid_1's quantile: 2.00334\n",
      "[3500]\ttraining's quantile: 1.98763\tvalid_1's quantile: 1.9977\n",
      "[4000]\ttraining's quantile: 1.98\tvalid_1's quantile: 1.99072\n",
      "[4500]\ttraining's quantile: 1.9736\tvalid_1's quantile: 1.98536\n",
      "[5000]\ttraining's quantile: 1.9693\tvalid_1's quantile: 1.98206\n",
      "[5500]\ttraining's quantile: 1.96498\tvalid_1's quantile: 1.97823\n",
      "[6000]\ttraining's quantile: 1.96072\tvalid_1's quantile: 1.97503\n",
      "[6500]\ttraining's quantile: 1.95655\tvalid_1's quantile: 1.97212\n",
      "[7000]\ttraining's quantile: 1.95319\tvalid_1's quantile: 1.96938\n",
      "[7500]\ttraining's quantile: 1.95049\tvalid_1's quantile: 1.96696\n",
      "[8000]\ttraining's quantile: 1.94798\tvalid_1's quantile: 1.96481\n",
      "[8500]\ttraining's quantile: 1.94548\tvalid_1's quantile: 1.96317\n",
      "[9000]\ttraining's quantile: 1.94364\tvalid_1's quantile: 1.9621\n",
      "[9500]\ttraining's quantile: 1.93982\tvalid_1's quantile: 1.9585\n",
      "[10000]\ttraining's quantile: 1.93517\tvalid_1's quantile: 1.95449\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.93517\tvalid_1's quantile: 1.95449\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.70359\tvalid_1's quantile: 1.71158\n",
      "[1000]\ttraining's quantile: 1.56648\tvalid_1's quantile: 1.57416\n",
      "[1500]\ttraining's quantile: 1.53741\tvalid_1's quantile: 1.54706\n",
      "[2000]\ttraining's quantile: 1.51923\tvalid_1's quantile: 1.53134\n",
      "[2500]\ttraining's quantile: 1.50652\tvalid_1's quantile: 1.52035\n",
      "[3000]\ttraining's quantile: 1.49843\tvalid_1's quantile: 1.51369\n",
      "[3500]\ttraining's quantile: 1.49142\tvalid_1's quantile: 1.50808\n",
      "[4000]\ttraining's quantile: 1.48561\tvalid_1's quantile: 1.50417\n",
      "[4500]\ttraining's quantile: 1.48158\tvalid_1's quantile: 1.50214\n",
      "[5000]\ttraining's quantile: 1.47778\tvalid_1's quantile: 1.49991\n",
      "[5500]\ttraining's quantile: 1.47388\tvalid_1's quantile: 1.49777\n",
      "[6000]\ttraining's quantile: 1.4701\tvalid_1's quantile: 1.49502\n",
      "[6500]\ttraining's quantile: 1.46716\tvalid_1's quantile: 1.49338\n",
      "[7000]\ttraining's quantile: 1.4644\tvalid_1's quantile: 1.491\n",
      "[7500]\ttraining's quantile: 1.46113\tvalid_1's quantile: 1.48736\n",
      "[8000]\ttraining's quantile: 1.4585\tvalid_1's quantile: 1.48563\n",
      "[8500]\ttraining's quantile: 1.45631\tvalid_1's quantile: 1.48397\n",
      "[9000]\ttraining's quantile: 1.4541\tvalid_1's quantile: 1.48249\n",
      "[9500]\ttraining's quantile: 1.45158\tvalid_1's quantile: 1.48056\n",
      "[10000]\ttraining's quantile: 1.44905\tvalid_1's quantile: 1.47852\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.44905\tvalid_1's quantile: 1.47852\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.06101\tvalid_1's quantile: 1.06861\n",
      "[1000]\ttraining's quantile: 0.941247\tvalid_1's quantile: 0.955109\n",
      "[1500]\ttraining's quantile: 0.920815\tvalid_1's quantile: 0.934294\n",
      "[2000]\ttraining's quantile: 0.911073\tvalid_1's quantile: 0.925159\n",
      "[2500]\ttraining's quantile: 0.906079\tvalid_1's quantile: 0.920458\n",
      "[3000]\ttraining's quantile: 0.901489\tvalid_1's quantile: 0.916499\n",
      "[3500]\ttraining's quantile: 0.898886\tvalid_1's quantile: 0.913794\n",
      "[4000]\ttraining's quantile: 0.895549\tvalid_1's quantile: 0.9106\n",
      "[4500]\ttraining's quantile: 0.892448\tvalid_1's quantile: 0.907242\n",
      "[5000]\ttraining's quantile: 0.889028\tvalid_1's quantile: 0.903779\n",
      "[5500]\ttraining's quantile: 0.885403\tvalid_1's quantile: 0.900692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6000]\ttraining's quantile: 0.883521\tvalid_1's quantile: 0.898894\n",
      "[6500]\ttraining's quantile: 0.88136\tvalid_1's quantile: 0.897147\n",
      "[7000]\ttraining's quantile: 0.879221\tvalid_1's quantile: 0.894945\n",
      "[7500]\ttraining's quantile: 0.878003\tvalid_1's quantile: 0.893791\n",
      "[8000]\ttraining's quantile: 0.876852\tvalid_1's quantile: 0.892581\n",
      "[8500]\ttraining's quantile: 0.875698\tvalid_1's quantile: 0.89147\n",
      "[9000]\ttraining's quantile: 0.874785\tvalid_1's quantile: 0.89063\n",
      "[9500]\ttraining's quantile: 0.873759\tvalid_1's quantile: 0.889655\n",
      "[10000]\ttraining's quantile: 0.872797\tvalid_1's quantile: 0.888829\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.872797\tvalid_1's quantile: 0.888829\n",
      "2\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.39044\tvalid_1's quantile: 1.45407\n",
      "[1000]\ttraining's quantile: 1.37464\tvalid_1's quantile: 1.44186\n",
      "[1500]\ttraining's quantile: 1.36432\tvalid_1's quantile: 1.43587\n",
      "[2000]\ttraining's quantile: 1.35429\tvalid_1's quantile: 1.42924\n",
      "[2500]\ttraining's quantile: 1.34809\tvalid_1's quantile: 1.42597\n",
      "Early stopping, best iteration is:\n",
      "[2760]\ttraining's quantile: 1.34561\tvalid_1's quantile: 1.42475\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.25263\tvalid_1's quantile: 2.3082\n",
      "[1000]\ttraining's quantile: 2.21808\tvalid_1's quantile: 2.29107\n",
      "[1500]\ttraining's quantile: 2.20326\tvalid_1's quantile: 2.28664\n",
      "[2000]\ttraining's quantile: 2.19234\tvalid_1's quantile: 2.27965\n",
      "[2500]\ttraining's quantile: 2.18254\tvalid_1's quantile: 2.27546\n",
      "[3000]\ttraining's quantile: 2.17354\tvalid_1's quantile: 2.27111\n",
      "[3500]\ttraining's quantile: 2.16507\tvalid_1's quantile: 2.26951\n",
      "Early stopping, best iteration is:\n",
      "[3361]\ttraining's quantile: 2.16722\tvalid_1's quantile: 2.26894\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.71058\tvalid_1's quantile: 2.77248\n",
      "[1000]\ttraining's quantile: 2.65996\tvalid_1's quantile: 2.73745\n",
      "[1500]\ttraining's quantile: 2.64129\tvalid_1's quantile: 2.72902\n",
      "[2000]\ttraining's quantile: 2.62378\tvalid_1's quantile: 2.72157\n",
      "[2500]\ttraining's quantile: 2.60978\tvalid_1's quantile: 2.71125\n",
      "[3000]\ttraining's quantile: 2.59951\tvalid_1's quantile: 2.70372\n",
      "[3500]\ttraining's quantile: 2.58864\tvalid_1's quantile: 2.69896\n",
      "[4000]\ttraining's quantile: 2.58058\tvalid_1's quantile: 2.69419\n",
      "[4500]\ttraining's quantile: 2.57394\tvalid_1's quantile: 2.69021\n",
      "[5000]\ttraining's quantile: 2.57067\tvalid_1's quantile: 2.68838\n",
      "[5500]\ttraining's quantile: 2.56747\tvalid_1's quantile: 2.68663\n",
      "[6000]\ttraining's quantile: 2.56189\tvalid_1's quantile: 2.68363\n",
      "[6500]\ttraining's quantile: 2.55966\tvalid_1's quantile: 2.68276\n",
      "Early stopping, best iteration is:\n",
      "[6568]\ttraining's quantile: 2.55962\tvalid_1's quantile: 2.68273\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.87569\tvalid_1's quantile: 2.93948\n",
      "[1000]\ttraining's quantile: 2.80112\tvalid_1's quantile: 2.88333\n",
      "[1500]\ttraining's quantile: 2.779\tvalid_1's quantile: 2.86655\n",
      "[2000]\ttraining's quantile: 2.75955\tvalid_1's quantile: 2.85372\n",
      "[2500]\ttraining's quantile: 2.74777\tvalid_1's quantile: 2.84543\n",
      "[3000]\ttraining's quantile: 2.73726\tvalid_1's quantile: 2.83718\n",
      "[3500]\ttraining's quantile: 2.72899\tvalid_1's quantile: 2.83141\n",
      "[4000]\ttraining's quantile: 2.71977\tvalid_1's quantile: 2.82664\n",
      "[4500]\ttraining's quantile: 2.71191\tvalid_1's quantile: 2.82097\n",
      "[5000]\ttraining's quantile: 2.70604\tvalid_1's quantile: 2.81593\n",
      "[5500]\ttraining's quantile: 2.70084\tvalid_1's quantile: 2.81337\n",
      "[6000]\ttraining's quantile: 2.6968\tvalid_1's quantile: 2.81104\n",
      "[6500]\ttraining's quantile: 2.69305\tvalid_1's quantile: 2.80748\n",
      "[7000]\ttraining's quantile: 2.6896\tvalid_1's quantile: 2.80432\n",
      "[7500]\ttraining's quantile: 2.68541\tvalid_1's quantile: 2.80172\n",
      "[8000]\ttraining's quantile: 2.68176\tvalid_1's quantile: 2.79928\n",
      "[8500]\ttraining's quantile: 2.67716\tvalid_1's quantile: 2.79726\n",
      "[9000]\ttraining's quantile: 2.67357\tvalid_1's quantile: 2.79401\n",
      "[9500]\ttraining's quantile: 2.67094\tvalid_1's quantile: 2.7924\n",
      "[10000]\ttraining's quantile: 2.66722\tvalid_1's quantile: 2.79005\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.66722\tvalid_1's quantile: 2.79005\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.83312\tvalid_1's quantile: 2.89687\n",
      "[1000]\ttraining's quantile: 2.72406\tvalid_1's quantile: 2.79555\n",
      "[1500]\ttraining's quantile: 2.69036\tvalid_1's quantile: 2.76279\n",
      "[2000]\ttraining's quantile: 2.67147\tvalid_1's quantile: 2.74733\n",
      "[2500]\ttraining's quantile: 2.65638\tvalid_1's quantile: 2.73439\n",
      "[3000]\ttraining's quantile: 2.64673\tvalid_1's quantile: 2.72602\n",
      "[3500]\ttraining's quantile: 2.63744\tvalid_1's quantile: 2.71808\n",
      "[4000]\ttraining's quantile: 2.62886\tvalid_1's quantile: 2.71092\n",
      "[4500]\ttraining's quantile: 2.6239\tvalid_1's quantile: 2.70751\n",
      "[5000]\ttraining's quantile: 2.61942\tvalid_1's quantile: 2.70378\n",
      "[5500]\ttraining's quantile: 2.61452\tvalid_1's quantile: 2.70003\n",
      "[6000]\ttraining's quantile: 2.61087\tvalid_1's quantile: 2.69785\n",
      "[6500]\ttraining's quantile: 2.60853\tvalid_1's quantile: 2.69572\n",
      "[7000]\ttraining's quantile: 2.60548\tvalid_1's quantile: 2.69355\n",
      "[7500]\ttraining's quantile: 2.60207\tvalid_1's quantile: 2.69122\n",
      "[8000]\ttraining's quantile: 2.59844\tvalid_1's quantile: 2.68902\n",
      "[8500]\ttraining's quantile: 2.5953\tvalid_1's quantile: 2.68752\n",
      "[9000]\ttraining's quantile: 2.591\tvalid_1's quantile: 2.68493\n",
      "[9500]\ttraining's quantile: 2.58693\tvalid_1's quantile: 2.68238\n",
      "[10000]\ttraining's quantile: 2.58314\tvalid_1's quantile: 2.68038\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.58314\tvalid_1's quantile: 2.68038\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.60724\tvalid_1's quantile: 2.65538\n",
      "[1000]\ttraining's quantile: 2.46799\tvalid_1's quantile: 2.51354\n",
      "[1500]\ttraining's quantile: 2.43156\tvalid_1's quantile: 2.47694\n",
      "[2000]\ttraining's quantile: 2.40965\tvalid_1's quantile: 2.45771\n",
      "[2500]\ttraining's quantile: 2.39591\tvalid_1's quantile: 2.44621\n",
      "[3000]\ttraining's quantile: 2.38673\tvalid_1's quantile: 2.43922\n",
      "[3500]\ttraining's quantile: 2.37834\tvalid_1's quantile: 2.43211\n",
      "[4000]\ttraining's quantile: 2.36945\tvalid_1's quantile: 2.42543\n",
      "[4500]\ttraining's quantile: 2.36335\tvalid_1's quantile: 2.42078\n",
      "[5000]\ttraining's quantile: 2.35544\tvalid_1's quantile: 2.41538\n",
      "[5500]\ttraining's quantile: 2.35189\tvalid_1's quantile: 2.4129\n",
      "[6000]\ttraining's quantile: 2.34778\tvalid_1's quantile: 2.41067\n",
      "[6500]\ttraining's quantile: 2.34328\tvalid_1's quantile: 2.4082\n",
      "[7000]\ttraining's quantile: 2.34032\tvalid_1's quantile: 2.40712\n",
      "[7500]\ttraining's quantile: 2.33683\tvalid_1's quantile: 2.40543\n",
      "[8000]\ttraining's quantile: 2.33374\tvalid_1's quantile: 2.40377\n",
      "[8500]\ttraining's quantile: 2.33033\tvalid_1's quantile: 2.40174\n",
      "[9000]\ttraining's quantile: 2.32817\tvalid_1's quantile: 2.4005\n",
      "[9500]\ttraining's quantile: 2.32605\tvalid_1's quantile: 2.39953\n",
      "[10000]\ttraining's quantile: 2.32414\tvalid_1's quantile: 2.39848\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.32414\tvalid_1's quantile: 2.39848\n",
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.22411\tvalid_1's quantile: 2.24561\n",
      "[1000]\ttraining's quantile: 2.07779\tvalid_1's quantile: 2.10344\n",
      "[1500]\ttraining's quantile: 2.04338\tvalid_1's quantile: 2.07246\n",
      "[2000]\ttraining's quantile: 2.02426\tvalid_1's quantile: 2.05615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500]\ttraining's quantile: 2.00791\tvalid_1's quantile: 2.0419\n",
      "[3000]\ttraining's quantile: 1.99689\tvalid_1's quantile: 2.03205\n",
      "[3500]\ttraining's quantile: 1.98669\tvalid_1's quantile: 2.02318\n",
      "[4000]\ttraining's quantile: 1.9789\tvalid_1's quantile: 2.01707\n",
      "[4500]\ttraining's quantile: 1.97145\tvalid_1's quantile: 2.01072\n",
      "[5000]\ttraining's quantile: 1.96498\tvalid_1's quantile: 2.00495\n",
      "[5500]\ttraining's quantile: 1.95865\tvalid_1's quantile: 1.9997\n",
      "[6000]\ttraining's quantile: 1.95304\tvalid_1's quantile: 1.9954\n",
      "[6500]\ttraining's quantile: 1.94886\tvalid_1's quantile: 1.99224\n",
      "[7000]\ttraining's quantile: 1.94537\tvalid_1's quantile: 1.99012\n",
      "[7500]\ttraining's quantile: 1.9416\tvalid_1's quantile: 1.988\n",
      "[8000]\ttraining's quantile: 1.93855\tvalid_1's quantile: 1.98599\n",
      "[8500]\ttraining's quantile: 1.93619\tvalid_1's quantile: 1.98452\n",
      "[9000]\ttraining's quantile: 1.93357\tvalid_1's quantile: 1.98276\n",
      "[9500]\ttraining's quantile: 1.93152\tvalid_1's quantile: 1.98162\n",
      "[10000]\ttraining's quantile: 1.92985\tvalid_1's quantile: 1.98051\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.92985\tvalid_1's quantile: 1.98051\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.69106\tvalid_1's quantile: 1.71826\n",
      "[1000]\ttraining's quantile: 1.56587\tvalid_1's quantile: 1.59656\n",
      "[1500]\ttraining's quantile: 1.53931\tvalid_1's quantile: 1.57002\n",
      "[2000]\ttraining's quantile: 1.51685\tvalid_1's quantile: 1.54971\n",
      "[2500]\ttraining's quantile: 1.50259\tvalid_1's quantile: 1.53698\n",
      "[3000]\ttraining's quantile: 1.49369\tvalid_1's quantile: 1.52867\n",
      "[3500]\ttraining's quantile: 1.48527\tvalid_1's quantile: 1.52067\n",
      "[4000]\ttraining's quantile: 1.47977\tvalid_1's quantile: 1.51619\n",
      "[4500]\ttraining's quantile: 1.47472\tvalid_1's quantile: 1.51209\n",
      "[5000]\ttraining's quantile: 1.47156\tvalid_1's quantile: 1.51018\n",
      "[5500]\ttraining's quantile: 1.46855\tvalid_1's quantile: 1.50787\n",
      "[6000]\ttraining's quantile: 1.46621\tvalid_1's quantile: 1.50642\n",
      "[6500]\ttraining's quantile: 1.46378\tvalid_1's quantile: 1.50405\n",
      "[7000]\ttraining's quantile: 1.4597\tvalid_1's quantile: 1.50027\n",
      "[7500]\ttraining's quantile: 1.45628\tvalid_1's quantile: 1.49714\n",
      "[8000]\ttraining's quantile: 1.45337\tvalid_1's quantile: 1.49434\n",
      "[8500]\ttraining's quantile: 1.45086\tvalid_1's quantile: 1.49235\n",
      "[9000]\ttraining's quantile: 1.44767\tvalid_1's quantile: 1.48945\n",
      "[9500]\ttraining's quantile: 1.4455\tvalid_1's quantile: 1.4877\n",
      "[10000]\ttraining's quantile: 1.44283\tvalid_1's quantile: 1.48608\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.44283\tvalid_1's quantile: 1.48608\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.05991\tvalid_1's quantile: 1.06612\n",
      "[1000]\ttraining's quantile: 0.938841\tvalid_1's quantile: 0.945668\n",
      "[1500]\ttraining's quantile: 0.912604\tvalid_1's quantile: 0.92234\n",
      "[2000]\ttraining's quantile: 0.897877\tvalid_1's quantile: 0.908444\n",
      "[2500]\ttraining's quantile: 0.887549\tvalid_1's quantile: 0.898474\n",
      "[3000]\ttraining's quantile: 0.880429\tvalid_1's quantile: 0.891436\n",
      "[3500]\ttraining's quantile: 0.875374\tvalid_1's quantile: 0.886252\n",
      "[4000]\ttraining's quantile: 0.868897\tvalid_1's quantile: 0.880538\n",
      "[4500]\ttraining's quantile: 0.861445\tvalid_1's quantile: 0.875449\n",
      "[5000]\ttraining's quantile: 0.853335\tvalid_1's quantile: 0.869232\n",
      "[5500]\ttraining's quantile: 0.847517\tvalid_1's quantile: 0.864754\n",
      "[6000]\ttraining's quantile: 0.843524\tvalid_1's quantile: 0.861721\n",
      "[6500]\ttraining's quantile: 0.83978\tvalid_1's quantile: 0.85899\n",
      "[7000]\ttraining's quantile: 0.836387\tvalid_1's quantile: 0.856434\n",
      "[7500]\ttraining's quantile: 0.833419\tvalid_1's quantile: 0.85405\n",
      "[8000]\ttraining's quantile: 0.831087\tvalid_1's quantile: 0.852119\n",
      "[8500]\ttraining's quantile: 0.828835\tvalid_1's quantile: 0.850267\n",
      "[9000]\ttraining's quantile: 0.827139\tvalid_1's quantile: 0.849054\n",
      "[9500]\ttraining's quantile: 0.825241\tvalid_1's quantile: 0.847486\n",
      "[10000]\ttraining's quantile: 0.823234\tvalid_1's quantile: 0.845763\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.823234\tvalid_1's quantile: 0.845763\n",
      "3\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.40197\tvalid_1's quantile: 1.39358\n",
      "[1000]\ttraining's quantile: 1.38688\tvalid_1's quantile: 1.38475\n",
      "[1500]\ttraining's quantile: 1.37775\tvalid_1's quantile: 1.38089\n",
      "[2000]\ttraining's quantile: 1.3696\tvalid_1's quantile: 1.37725\n",
      "[2500]\ttraining's quantile: 1.3625\tvalid_1's quantile: 1.37287\n",
      "[3000]\ttraining's quantile: 1.35782\tvalid_1's quantile: 1.37033\n",
      "[3500]\ttraining's quantile: 1.3538\tvalid_1's quantile: 1.36782\n",
      "[4000]\ttraining's quantile: 1.3497\tvalid_1's quantile: 1.36615\n",
      "[4500]\ttraining's quantile: 1.34505\tvalid_1's quantile: 1.36481\n",
      "[5000]\ttraining's quantile: 1.34294\tvalid_1's quantile: 1.36394\n",
      "[5500]\ttraining's quantile: 1.34152\tvalid_1's quantile: 1.36335\n",
      "[6000]\ttraining's quantile: 1.33876\tvalid_1's quantile: 1.36253\n",
      "Early stopping, best iteration is:\n",
      "[6272]\ttraining's quantile: 1.33797\tvalid_1's quantile: 1.36206\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.26305\tvalid_1's quantile: 2.26674\n",
      "[1000]\ttraining's quantile: 2.22923\tvalid_1's quantile: 2.24881\n",
      "[1500]\ttraining's quantile: 2.21142\tvalid_1's quantile: 2.23841\n",
      "[2000]\ttraining's quantile: 2.20088\tvalid_1's quantile: 2.23381\n",
      "[2500]\ttraining's quantile: 2.19035\tvalid_1's quantile: 2.22869\n",
      "[3000]\ttraining's quantile: 2.18112\tvalid_1's quantile: 2.22454\n",
      "[3500]\ttraining's quantile: 2.17328\tvalid_1's quantile: 2.22137\n",
      "[4000]\ttraining's quantile: 2.16658\tvalid_1's quantile: 2.21764\n",
      "[4500]\ttraining's quantile: 2.16053\tvalid_1's quantile: 2.214\n",
      "[5000]\ttraining's quantile: 2.15446\tvalid_1's quantile: 2.21142\n",
      "[5500]\ttraining's quantile: 2.14861\tvalid_1's quantile: 2.20878\n",
      "[6000]\ttraining's quantile: 2.14508\tvalid_1's quantile: 2.20786\n",
      "[6500]\ttraining's quantile: 2.14126\tvalid_1's quantile: 2.20637\n",
      "[7000]\ttraining's quantile: 2.13761\tvalid_1's quantile: 2.20448\n",
      "[7500]\ttraining's quantile: 2.1347\tvalid_1's quantile: 2.20333\n",
      "Early stopping, best iteration is:\n",
      "[7510]\ttraining's quantile: 2.13463\tvalid_1's quantile: 2.2033\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.71934\tvalid_1's quantile: 2.73166\n",
      "[1000]\ttraining's quantile: 2.6707\tvalid_1's quantile: 2.69992\n",
      "[1500]\ttraining's quantile: 2.64999\tvalid_1's quantile: 2.68952\n",
      "[2000]\ttraining's quantile: 2.63331\tvalid_1's quantile: 2.67984\n",
      "[2500]\ttraining's quantile: 2.62016\tvalid_1's quantile: 2.67166\n",
      "[3000]\ttraining's quantile: 2.60837\tvalid_1's quantile: 2.665\n",
      "Early stopping, best iteration is:\n",
      "[3295]\ttraining's quantile: 2.6003\tvalid_1's quantile: 2.66242\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.88544\tvalid_1's quantile: 2.88893\n",
      "[1000]\ttraining's quantile: 2.81493\tvalid_1's quantile: 2.83517\n",
      "[1500]\ttraining's quantile: 2.78798\tvalid_1's quantile: 2.8177\n",
      "[2000]\ttraining's quantile: 2.76973\tvalid_1's quantile: 2.80589\n",
      "[2500]\ttraining's quantile: 2.75767\tvalid_1's quantile: 2.79875\n",
      "[3000]\ttraining's quantile: 2.74674\tvalid_1's quantile: 2.79197\n",
      "[3500]\ttraining's quantile: 2.73557\tvalid_1's quantile: 2.78769\n",
      "[4000]\ttraining's quantile: 2.72512\tvalid_1's quantile: 2.78527\n",
      "[4500]\ttraining's quantile: 2.71726\tvalid_1's quantile: 2.78242\n",
      "[5000]\ttraining's quantile: 2.71195\tvalid_1's quantile: 2.77899\n",
      "[5500]\ttraining's quantile: 2.70384\tvalid_1's quantile: 2.77496\n",
      "[6000]\ttraining's quantile: 2.69739\tvalid_1's quantile: 2.77288\n",
      "[6500]\ttraining's quantile: 2.69238\tvalid_1's quantile: 2.77029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7000]\ttraining's quantile: 2.68786\tvalid_1's quantile: 2.76871\n",
      "[7500]\ttraining's quantile: 2.68368\tvalid_1's quantile: 2.76795\n",
      "Early stopping, best iteration is:\n",
      "[7542]\ttraining's quantile: 2.6832\tvalid_1's quantile: 2.7679\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.84133\tvalid_1's quantile: 2.83718\n",
      "[1000]\ttraining's quantile: 2.73263\tvalid_1's quantile: 2.74503\n",
      "[1500]\ttraining's quantile: 2.7001\tvalid_1's quantile: 2.72147\n",
      "[2000]\ttraining's quantile: 2.68117\tvalid_1's quantile: 2.70721\n",
      "[2500]\ttraining's quantile: 2.66792\tvalid_1's quantile: 2.69993\n",
      "[3000]\ttraining's quantile: 2.65726\tvalid_1's quantile: 2.69605\n",
      "[3500]\ttraining's quantile: 2.64933\tvalid_1's quantile: 2.6915\n",
      "[4000]\ttraining's quantile: 2.64168\tvalid_1's quantile: 2.68793\n",
      "[4500]\ttraining's quantile: 2.63513\tvalid_1's quantile: 2.68632\n",
      "[5000]\ttraining's quantile: 2.62958\tvalid_1's quantile: 2.68451\n",
      "[5500]\ttraining's quantile: 2.62533\tvalid_1's quantile: 2.68207\n",
      "[6000]\ttraining's quantile: 2.62001\tvalid_1's quantile: 2.68007\n",
      "[6500]\ttraining's quantile: 2.61499\tvalid_1's quantile: 2.67699\n",
      "[7000]\ttraining's quantile: 2.61036\tvalid_1's quantile: 2.67499\n",
      "[7500]\ttraining's quantile: 2.60654\tvalid_1's quantile: 2.6732\n",
      "[8000]\ttraining's quantile: 2.60395\tvalid_1's quantile: 2.67182\n",
      "[8500]\ttraining's quantile: 2.60007\tvalid_1's quantile: 2.6692\n",
      "[9000]\ttraining's quantile: 2.59402\tvalid_1's quantile: 2.66573\n",
      "[9500]\ttraining's quantile: 2.59068\tvalid_1's quantile: 2.66387\n",
      "[10000]\ttraining's quantile: 2.58698\tvalid_1's quantile: 2.66195\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.58698\tvalid_1's quantile: 2.66195\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.6244\tvalid_1's quantile: 2.59393\n",
      "[1000]\ttraining's quantile: 2.47274\tvalid_1's quantile: 2.46999\n",
      "[1500]\ttraining's quantile: 2.43419\tvalid_1's quantile: 2.44449\n",
      "[2000]\ttraining's quantile: 2.41096\tvalid_1's quantile: 2.4282\n",
      "[2500]\ttraining's quantile: 2.39834\tvalid_1's quantile: 2.42157\n",
      "[3000]\ttraining's quantile: 2.38639\tvalid_1's quantile: 2.41331\n",
      "[3500]\ttraining's quantile: 2.37714\tvalid_1's quantile: 2.40723\n",
      "[4000]\ttraining's quantile: 2.36931\tvalid_1's quantile: 2.40274\n",
      "[4500]\ttraining's quantile: 2.36226\tvalid_1's quantile: 2.39845\n",
      "[5000]\ttraining's quantile: 2.35615\tvalid_1's quantile: 2.39464\n",
      "[5500]\ttraining's quantile: 2.35218\tvalid_1's quantile: 2.39174\n",
      "[6000]\ttraining's quantile: 2.34902\tvalid_1's quantile: 2.3897\n",
      "[6500]\ttraining's quantile: 2.34516\tvalid_1's quantile: 2.38788\n",
      "[7000]\ttraining's quantile: 2.34162\tvalid_1's quantile: 2.38568\n",
      "[7500]\ttraining's quantile: 2.33859\tvalid_1's quantile: 2.38433\n",
      "[8000]\ttraining's quantile: 2.33492\tvalid_1's quantile: 2.38202\n",
      "[8500]\ttraining's quantile: 2.33205\tvalid_1's quantile: 2.38064\n",
      "Early stopping, best iteration is:\n",
      "[8782]\ttraining's quantile: 2.32937\tvalid_1's quantile: 2.38038\n",
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.21562\tvalid_1's quantile: 2.1979\n",
      "[1000]\ttraining's quantile: 2.07549\tvalid_1's quantile: 2.07521\n",
      "[1500]\ttraining's quantile: 2.03859\tvalid_1's quantile: 2.04568\n",
      "[2000]\ttraining's quantile: 2.01734\tvalid_1's quantile: 2.02886\n",
      "[2500]\ttraining's quantile: 2.00209\tvalid_1's quantile: 2.01763\n",
      "[3000]\ttraining's quantile: 1.98916\tvalid_1's quantile: 2.0083\n",
      "[3500]\ttraining's quantile: 1.98221\tvalid_1's quantile: 2.00353\n",
      "[4000]\ttraining's quantile: 1.97517\tvalid_1's quantile: 1.99968\n",
      "[4500]\ttraining's quantile: 1.96965\tvalid_1's quantile: 1.99671\n",
      "[5000]\ttraining's quantile: 1.96648\tvalid_1's quantile: 1.995\n",
      "[5500]\ttraining's quantile: 1.96259\tvalid_1's quantile: 1.99309\n",
      "[6000]\ttraining's quantile: 1.95891\tvalid_1's quantile: 1.99091\n",
      "[6500]\ttraining's quantile: 1.95528\tvalid_1's quantile: 1.98871\n",
      "[7000]\ttraining's quantile: 1.95181\tvalid_1's quantile: 1.98728\n",
      "[7500]\ttraining's quantile: 1.94966\tvalid_1's quantile: 1.98607\n",
      "[8000]\ttraining's quantile: 1.94676\tvalid_1's quantile: 1.98408\n",
      "[8500]\ttraining's quantile: 1.94418\tvalid_1's quantile: 1.98264\n",
      "[9000]\ttraining's quantile: 1.94122\tvalid_1's quantile: 1.98159\n",
      "[9500]\ttraining's quantile: 1.93878\tvalid_1's quantile: 1.98028\n",
      "[10000]\ttraining's quantile: 1.93622\tvalid_1's quantile: 1.97877\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.93622\tvalid_1's quantile: 1.97877\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.69178\tvalid_1's quantile: 1.69566\n",
      "[1000]\ttraining's quantile: 1.56773\tvalid_1's quantile: 1.57848\n",
      "[1500]\ttraining's quantile: 1.53221\tvalid_1's quantile: 1.55147\n",
      "[2000]\ttraining's quantile: 1.51795\tvalid_1's quantile: 1.53813\n",
      "[2500]\ttraining's quantile: 1.50309\tvalid_1's quantile: 1.52492\n",
      "[3000]\ttraining's quantile: 1.49694\tvalid_1's quantile: 1.52015\n",
      "[3500]\ttraining's quantile: 1.49112\tvalid_1's quantile: 1.51602\n",
      "[4000]\ttraining's quantile: 1.48612\tvalid_1's quantile: 1.51283\n",
      "[4500]\ttraining's quantile: 1.48121\tvalid_1's quantile: 1.50892\n",
      "[5000]\ttraining's quantile: 1.4783\tvalid_1's quantile: 1.50688\n",
      "[5500]\ttraining's quantile: 1.47329\tvalid_1's quantile: 1.50277\n",
      "[6000]\ttraining's quantile: 1.46848\tvalid_1's quantile: 1.49944\n",
      "[6500]\ttraining's quantile: 1.46541\tvalid_1's quantile: 1.49733\n",
      "[7000]\ttraining's quantile: 1.46132\tvalid_1's quantile: 1.49431\n",
      "[7500]\ttraining's quantile: 1.45755\tvalid_1's quantile: 1.49192\n",
      "[8000]\ttraining's quantile: 1.45326\tvalid_1's quantile: 1.48948\n",
      "[8500]\ttraining's quantile: 1.45083\tvalid_1's quantile: 1.48806\n",
      "[9000]\ttraining's quantile: 1.44887\tvalid_1's quantile: 1.48666\n",
      "[9500]\ttraining's quantile: 1.44665\tvalid_1's quantile: 1.48542\n",
      "[10000]\ttraining's quantile: 1.44451\tvalid_1's quantile: 1.48399\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.44451\tvalid_1's quantile: 1.48399\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.05626\tvalid_1's quantile: 1.07749\n",
      "[1000]\ttraining's quantile: 0.937524\tvalid_1's quantile: 0.965898\n",
      "[1500]\ttraining's quantile: 0.910404\tvalid_1's quantile: 0.940404\n",
      "[2000]\ttraining's quantile: 0.893756\tvalid_1's quantile: 0.925279\n",
      "[2500]\ttraining's quantile: 0.882241\tvalid_1's quantile: 0.915123\n",
      "[3000]\ttraining's quantile: 0.874271\tvalid_1's quantile: 0.908407\n",
      "[3500]\ttraining's quantile: 0.867744\tvalid_1's quantile: 0.902617\n",
      "[4000]\ttraining's quantile: 0.861313\tvalid_1's quantile: 0.896925\n",
      "[4500]\ttraining's quantile: 0.857054\tvalid_1's quantile: 0.893215\n",
      "[5000]\ttraining's quantile: 0.853667\tvalid_1's quantile: 0.890794\n",
      "[5500]\ttraining's quantile: 0.849227\tvalid_1's quantile: 0.887118\n",
      "[6000]\ttraining's quantile: 0.847001\tvalid_1's quantile: 0.885476\n",
      "[6500]\ttraining's quantile: 0.844224\tvalid_1's quantile: 0.883459\n",
      "[7000]\ttraining's quantile: 0.841732\tvalid_1's quantile: 0.881257\n",
      "[7500]\ttraining's quantile: 0.839642\tvalid_1's quantile: 0.879235\n",
      "[8000]\ttraining's quantile: 0.837912\tvalid_1's quantile: 0.87768\n",
      "[8500]\ttraining's quantile: 0.836163\tvalid_1's quantile: 0.876324\n",
      "[9000]\ttraining's quantile: 0.834707\tvalid_1's quantile: 0.875093\n",
      "[9500]\ttraining's quantile: 0.832325\tvalid_1's quantile: 0.872932\n",
      "[10000]\ttraining's quantile: 0.83053\tvalid_1's quantile: 0.871451\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.83053\tvalid_1's quantile: 0.871451\n",
      "4\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.3938\tvalid_1's quantile: 1.43175\n",
      "[1000]\ttraining's quantile: 1.37891\tvalid_1's quantile: 1.42101\n",
      "[1500]\ttraining's quantile: 1.36827\tvalid_1's quantile: 1.41489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's quantile: 1.36212\tvalid_1's quantile: 1.41419\n",
      "[2500]\ttraining's quantile: 1.35522\tvalid_1's quantile: 1.41094\n",
      "[3000]\ttraining's quantile: 1.34922\tvalid_1's quantile: 1.40611\n",
      "[3500]\ttraining's quantile: 1.34457\tvalid_1's quantile: 1.40434\n",
      "[4000]\ttraining's quantile: 1.34301\tvalid_1's quantile: 1.40368\n",
      "[4500]\ttraining's quantile: 1.33961\tvalid_1's quantile: 1.40171\n",
      "[5000]\ttraining's quantile: 1.33736\tvalid_1's quantile: 1.40076\n",
      "Early stopping, best iteration is:\n",
      "[5051]\ttraining's quantile: 1.3372\tvalid_1's quantile: 1.40074\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.24977\tvalid_1's quantile: 2.32329\n",
      "[1000]\ttraining's quantile: 2.21686\tvalid_1's quantile: 2.29326\n",
      "[1500]\ttraining's quantile: 2.20599\tvalid_1's quantile: 2.28706\n",
      "[2000]\ttraining's quantile: 2.19252\tvalid_1's quantile: 2.27763\n",
      "[2500]\ttraining's quantile: 2.18057\tvalid_1's quantile: 2.27074\n",
      "[3000]\ttraining's quantile: 2.17245\tvalid_1's quantile: 2.26623\n",
      "[3500]\ttraining's quantile: 2.16549\tvalid_1's quantile: 2.26282\n",
      "[4000]\ttraining's quantile: 2.15912\tvalid_1's quantile: 2.25953\n",
      "[4500]\ttraining's quantile: 2.15477\tvalid_1's quantile: 2.25799\n",
      "[5000]\ttraining's quantile: 2.14786\tvalid_1's quantile: 2.25475\n",
      "[5500]\ttraining's quantile: 2.14383\tvalid_1's quantile: 2.25302\n",
      "[6000]\ttraining's quantile: 2.13807\tvalid_1's quantile: 2.25056\n",
      "[6500]\ttraining's quantile: 2.13427\tvalid_1's quantile: 2.24845\n",
      "[7000]\ttraining's quantile: 2.13125\tvalid_1's quantile: 2.24679\n",
      "[7500]\ttraining's quantile: 2.12751\tvalid_1's quantile: 2.24521\n",
      "[8000]\ttraining's quantile: 2.12474\tvalid_1's quantile: 2.24446\n",
      "[8500]\ttraining's quantile: 2.12272\tvalid_1's quantile: 2.24381\n",
      "[9000]\ttraining's quantile: 2.11985\tvalid_1's quantile: 2.24167\n",
      "[9500]\ttraining's quantile: 2.11518\tvalid_1's quantile: 2.23842\n",
      "[10000]\ttraining's quantile: 2.11075\tvalid_1's quantile: 2.23704\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.11075\tvalid_1's quantile: 2.23704\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.70589\tvalid_1's quantile: 2.78586\n",
      "[1000]\ttraining's quantile: 2.6573\tvalid_1's quantile: 2.73889\n",
      "[1500]\ttraining's quantile: 2.63919\tvalid_1's quantile: 2.72473\n",
      "[2000]\ttraining's quantile: 2.62265\tvalid_1's quantile: 2.71226\n",
      "[2500]\ttraining's quantile: 2.61172\tvalid_1's quantile: 2.70584\n",
      "[3000]\ttraining's quantile: 2.60332\tvalid_1's quantile: 2.70176\n",
      "[3500]\ttraining's quantile: 2.59203\tvalid_1's quantile: 2.69716\n",
      "[4000]\ttraining's quantile: 2.584\tvalid_1's quantile: 2.69258\n",
      "[4500]\ttraining's quantile: 2.57817\tvalid_1's quantile: 2.69005\n",
      "[5000]\ttraining's quantile: 2.57109\tvalid_1's quantile: 2.6856\n",
      "[5500]\ttraining's quantile: 2.56672\tvalid_1's quantile: 2.68259\n",
      "[6000]\ttraining's quantile: 2.56173\tvalid_1's quantile: 2.68067\n",
      "[6500]\ttraining's quantile: 2.55743\tvalid_1's quantile: 2.67885\n",
      "[7000]\ttraining's quantile: 2.55515\tvalid_1's quantile: 2.67714\n",
      "[7500]\ttraining's quantile: 2.55152\tvalid_1's quantile: 2.67551\n",
      "[8000]\ttraining's quantile: 2.54818\tvalid_1's quantile: 2.67344\n",
      "[8500]\ttraining's quantile: 2.543\tvalid_1's quantile: 2.67115\n",
      "[9000]\ttraining's quantile: 2.53981\tvalid_1's quantile: 2.67048\n",
      "[9500]\ttraining's quantile: 2.53589\tvalid_1's quantile: 2.66956\n",
      "[10000]\ttraining's quantile: 2.53107\tvalid_1's quantile: 2.66747\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.53107\tvalid_1's quantile: 2.66747\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.87341\tvalid_1's quantile: 2.9542\n",
      "[1000]\ttraining's quantile: 2.80064\tvalid_1's quantile: 2.87726\n",
      "[1500]\ttraining's quantile: 2.77338\tvalid_1's quantile: 2.85581\n",
      "[2000]\ttraining's quantile: 2.75353\tvalid_1's quantile: 2.84256\n",
      "[2500]\ttraining's quantile: 2.74273\tvalid_1's quantile: 2.83459\n",
      "[3000]\ttraining's quantile: 2.73256\tvalid_1's quantile: 2.82887\n",
      "[3500]\ttraining's quantile: 2.72408\tvalid_1's quantile: 2.8223\n",
      "[4000]\ttraining's quantile: 2.71699\tvalid_1's quantile: 2.81804\n",
      "[4500]\ttraining's quantile: 2.71023\tvalid_1's quantile: 2.81464\n",
      "[5000]\ttraining's quantile: 2.70434\tvalid_1's quantile: 2.80985\n",
      "[5500]\ttraining's quantile: 2.69807\tvalid_1's quantile: 2.80741\n",
      "[6000]\ttraining's quantile: 2.6936\tvalid_1's quantile: 2.80537\n",
      "[6500]\ttraining's quantile: 2.68994\tvalid_1's quantile: 2.80345\n",
      "[7000]\ttraining's quantile: 2.68608\tvalid_1's quantile: 2.8015\n",
      "[7500]\ttraining's quantile: 2.68231\tvalid_1's quantile: 2.79911\n",
      "[8000]\ttraining's quantile: 2.67951\tvalid_1's quantile: 2.79789\n",
      "[8500]\ttraining's quantile: 2.67661\tvalid_1's quantile: 2.79622\n",
      "[9000]\ttraining's quantile: 2.67361\tvalid_1's quantile: 2.79463\n",
      "[9500]\ttraining's quantile: 2.66931\tvalid_1's quantile: 2.79311\n",
      "[10000]\ttraining's quantile: 2.66685\tvalid_1's quantile: 2.79205\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.66685\tvalid_1's quantile: 2.79205\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.82988\tvalid_1's quantile: 2.9131\n",
      "[1000]\ttraining's quantile: 2.71877\tvalid_1's quantile: 2.79923\n",
      "[1500]\ttraining's quantile: 2.6839\tvalid_1's quantile: 2.7654\n",
      "[2000]\ttraining's quantile: 2.66691\tvalid_1's quantile: 2.75088\n",
      "[2500]\ttraining's quantile: 2.65595\tvalid_1's quantile: 2.74288\n",
      "[3000]\ttraining's quantile: 2.64589\tvalid_1's quantile: 2.73487\n",
      "[3500]\ttraining's quantile: 2.6372\tvalid_1's quantile: 2.7291\n",
      "[4000]\ttraining's quantile: 2.62859\tvalid_1's quantile: 2.72312\n",
      "[4500]\ttraining's quantile: 2.62172\tvalid_1's quantile: 2.71873\n",
      "[5000]\ttraining's quantile: 2.61478\tvalid_1's quantile: 2.71491\n",
      "[5500]\ttraining's quantile: 2.60969\tvalid_1's quantile: 2.71204\n",
      "[6000]\ttraining's quantile: 2.60464\tvalid_1's quantile: 2.70943\n",
      "[6500]\ttraining's quantile: 2.59944\tvalid_1's quantile: 2.70648\n",
      "[7000]\ttraining's quantile: 2.59388\tvalid_1's quantile: 2.70332\n",
      "[7500]\ttraining's quantile: 2.58884\tvalid_1's quantile: 2.70072\n",
      "[8000]\ttraining's quantile: 2.58379\tvalid_1's quantile: 2.69862\n",
      "[8500]\ttraining's quantile: 2.5796\tvalid_1's quantile: 2.69728\n",
      "[9000]\ttraining's quantile: 2.57645\tvalid_1's quantile: 2.69577\n",
      "[9500]\ttraining's quantile: 2.57356\tvalid_1's quantile: 2.69379\n",
      "[10000]\ttraining's quantile: 2.57059\tvalid_1's quantile: 2.69301\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.57059\tvalid_1's quantile: 2.69301\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.59783\tvalid_1's quantile: 2.68185\n",
      "[1000]\ttraining's quantile: 2.45651\tvalid_1's quantile: 2.53529\n",
      "[1500]\ttraining's quantile: 2.42203\tvalid_1's quantile: 2.50116\n",
      "[2000]\ttraining's quantile: 2.40466\tvalid_1's quantile: 2.48415\n",
      "[2500]\ttraining's quantile: 2.38942\tvalid_1's quantile: 2.47121\n",
      "[3000]\ttraining's quantile: 2.38032\tvalid_1's quantile: 2.46407\n",
      "[3500]\ttraining's quantile: 2.36994\tvalid_1's quantile: 2.45713\n",
      "[4000]\ttraining's quantile: 2.36282\tvalid_1's quantile: 2.45155\n",
      "[4500]\ttraining's quantile: 2.3575\tvalid_1's quantile: 2.44767\n",
      "[5000]\ttraining's quantile: 2.35357\tvalid_1's quantile: 2.44511\n",
      "[5500]\ttraining's quantile: 2.34603\tvalid_1's quantile: 2.43934\n",
      "[6000]\ttraining's quantile: 2.34228\tvalid_1's quantile: 2.43652\n",
      "[6500]\ttraining's quantile: 2.3391\tvalid_1's quantile: 2.43467\n",
      "[7000]\ttraining's quantile: 2.33558\tvalid_1's quantile: 2.43169\n",
      "[7500]\ttraining's quantile: 2.33177\tvalid_1's quantile: 2.42833\n",
      "[8000]\ttraining's quantile: 2.32812\tvalid_1's quantile: 2.42555\n",
      "[8500]\ttraining's quantile: 2.32593\tvalid_1's quantile: 2.42432\n",
      "[9000]\ttraining's quantile: 2.32312\tvalid_1's quantile: 2.42295\n",
      "[9500]\ttraining's quantile: 2.3211\tvalid_1's quantile: 2.42212\n",
      "[10000]\ttraining's quantile: 2.31871\tvalid_1's quantile: 2.42046\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.31871\tvalid_1's quantile: 2.42046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.20726\tvalid_1's quantile: 2.28758\n",
      "[1000]\ttraining's quantile: 2.07167\tvalid_1's quantile: 2.14555\n",
      "[1500]\ttraining's quantile: 2.04242\tvalid_1's quantile: 2.11632\n",
      "[2000]\ttraining's quantile: 2.02087\tvalid_1's quantile: 2.09433\n",
      "[2500]\ttraining's quantile: 2.00679\tvalid_1's quantile: 2.08029\n",
      "[3000]\ttraining's quantile: 1.99703\tvalid_1's quantile: 2.07126\n",
      "[3500]\ttraining's quantile: 1.98501\tvalid_1's quantile: 2.05968\n",
      "[4000]\ttraining's quantile: 1.97728\tvalid_1's quantile: 2.0523\n",
      "[4500]\ttraining's quantile: 1.97151\tvalid_1's quantile: 2.04715\n",
      "[5000]\ttraining's quantile: 1.96668\tvalid_1's quantile: 2.04274\n",
      "[5500]\ttraining's quantile: 1.96159\tvalid_1's quantile: 2.03768\n",
      "[6000]\ttraining's quantile: 1.95671\tvalid_1's quantile: 2.03282\n",
      "[6500]\ttraining's quantile: 1.95248\tvalid_1's quantile: 2.02916\n",
      "[7000]\ttraining's quantile: 1.94853\tvalid_1's quantile: 2.02571\n",
      "[7500]\ttraining's quantile: 1.94462\tvalid_1's quantile: 2.0229\n",
      "[8000]\ttraining's quantile: 1.94136\tvalid_1's quantile: 2.02018\n",
      "[8500]\ttraining's quantile: 1.93844\tvalid_1's quantile: 2.0182\n",
      "[9000]\ttraining's quantile: 1.93415\tvalid_1's quantile: 2.01451\n",
      "[9500]\ttraining's quantile: 1.9315\tvalid_1's quantile: 2.01246\n",
      "[10000]\ttraining's quantile: 1.92913\tvalid_1's quantile: 2.01082\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.92913\tvalid_1's quantile: 2.01082\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.69279\tvalid_1's quantile: 1.74206\n",
      "[1000]\ttraining's quantile: 1.57158\tvalid_1's quantile: 1.6247\n",
      "[1500]\ttraining's quantile: 1.53993\tvalid_1's quantile: 1.59138\n",
      "[2000]\ttraining's quantile: 1.52176\tvalid_1's quantile: 1.57375\n",
      "[2500]\ttraining's quantile: 1.50839\tvalid_1's quantile: 1.55927\n",
      "[3000]\ttraining's quantile: 1.49694\tvalid_1's quantile: 1.54721\n",
      "[3500]\ttraining's quantile: 1.4889\tvalid_1's quantile: 1.53988\n",
      "[4000]\ttraining's quantile: 1.48296\tvalid_1's quantile: 1.53514\n",
      "[4500]\ttraining's quantile: 1.47907\tvalid_1's quantile: 1.53236\n",
      "[5000]\ttraining's quantile: 1.47551\tvalid_1's quantile: 1.53015\n",
      "[5500]\ttraining's quantile: 1.46852\tvalid_1's quantile: 1.52483\n",
      "[6000]\ttraining's quantile: 1.46472\tvalid_1's quantile: 1.52222\n",
      "[6500]\ttraining's quantile: 1.46093\tvalid_1's quantile: 1.5195\n",
      "[7000]\ttraining's quantile: 1.45741\tvalid_1's quantile: 1.51728\n",
      "[7500]\ttraining's quantile: 1.45475\tvalid_1's quantile: 1.51547\n",
      "[8000]\ttraining's quantile: 1.45071\tvalid_1's quantile: 1.5126\n",
      "[8500]\ttraining's quantile: 1.44812\tvalid_1's quantile: 1.51061\n",
      "[9000]\ttraining's quantile: 1.44556\tvalid_1's quantile: 1.50907\n",
      "[9500]\ttraining's quantile: 1.44281\tvalid_1's quantile: 1.50708\n",
      "[10000]\ttraining's quantile: 1.44041\tvalid_1's quantile: 1.50527\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.44041\tvalid_1's quantile: 1.50527\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.05536\tvalid_1's quantile: 1.07593\n",
      "[1000]\ttraining's quantile: 0.932501\tvalid_1's quantile: 0.950565\n",
      "[1500]\ttraining's quantile: 0.905717\tvalid_1's quantile: 0.925745\n",
      "[2000]\ttraining's quantile: 0.888555\tvalid_1's quantile: 0.911021\n",
      "[2500]\ttraining's quantile: 0.877995\tvalid_1's quantile: 0.901228\n",
      "[3000]\ttraining's quantile: 0.870387\tvalid_1's quantile: 0.894265\n",
      "[3500]\ttraining's quantile: 0.863767\tvalid_1's quantile: 0.887692\n",
      "[4000]\ttraining's quantile: 0.855101\tvalid_1's quantile: 0.877519\n",
      "[4500]\ttraining's quantile: 0.846894\tvalid_1's quantile: 0.868887\n",
      "[5000]\ttraining's quantile: 0.840766\tvalid_1's quantile: 0.862916\n",
      "[5500]\ttraining's quantile: 0.836181\tvalid_1's quantile: 0.858322\n",
      "[6000]\ttraining's quantile: 0.832205\tvalid_1's quantile: 0.854599\n",
      "[6500]\ttraining's quantile: 0.829273\tvalid_1's quantile: 0.852106\n",
      "[7000]\ttraining's quantile: 0.826993\tvalid_1's quantile: 0.850386\n",
      "[7500]\ttraining's quantile: 0.824549\tvalid_1's quantile: 0.848293\n",
      "[8000]\ttraining's quantile: 0.821977\tvalid_1's quantile: 0.845944\n",
      "[8500]\ttraining's quantile: 0.819635\tvalid_1's quantile: 0.843991\n",
      "[9000]\ttraining's quantile: 0.817454\tvalid_1's quantile: 0.842154\n",
      "[9500]\ttraining's quantile: 0.815737\tvalid_1's quantile: 0.840739\n",
      "[10000]\ttraining's quantile: 0.813715\tvalid_1's quantile: 0.839266\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.813715\tvalid_1's quantile: 0.839266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.778748e-26</td>\n",
       "      <td>1.105514e-33</td>\n",
       "      <td>1.106803e-33</td>\n",
       "      <td>1.105470e-33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3888 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      q_0.1  q_0.2  q_0.3  q_0.4  q_0.5         q_0.6         q_0.7  \\\n",
       "0       0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "1       0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "2       0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "3       0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "4       0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "...     ...    ...    ...    ...    ...           ...           ...   \n",
       "3883    0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "3884    0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "3885    0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "3886    0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "3887    0.0    0.0    0.0    0.0    0.0  2.778748e-26  1.105514e-33   \n",
       "\n",
       "             q_0.8         q_0.9  \n",
       "0     1.106803e-33  1.105470e-33  \n",
       "1     1.106803e-33  1.105470e-33  \n",
       "2     1.106803e-33  1.105470e-33  \n",
       "3     1.106803e-33  1.105470e-33  \n",
       "4     1.106803e-33  1.105470e-33  \n",
       "...            ...           ...  \n",
       "3883  1.106803e-33  1.105470e-33  \n",
       "3884  1.106803e-33  1.105470e-33  \n",
       "3885  1.106803e-33  1.105470e-33  \n",
       "3886  1.106803e-33  1.105470e-33  \n",
       "3887  1.106803e-33  1.105470e-33  \n",
       "\n",
       "[3888 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1일 뒤를 예측하는 모델\n",
    "quantiles = [.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "pred_1 = pd.DataFrame()\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "for n_fold, (train_idx, val_idx) in enumerate(folds.split(x_train_1)):\n",
    "    train_x, train_y = x_train_1.loc[train_idx], y_train_1.loc[train_idx]\n",
    "    val_x, val_y = x_train_1.loc[val_idx], y_train_1.loc[val_idx]\n",
    "    print(n_fold)\n",
    "    for i in quantiles:\n",
    "        print(i)\n",
    "        model = LGBMRegressor(objective = 'quantile', boosting = 'gbdt', n_estimators = 10000, learning_rate = 0.009,\n",
    "                              colsample_bytree= 0.8, seed = 2020, max_depth = 3, num_leaves = 24, alpha = i)\n",
    "        model.fit(train_x, train_y, eval_metric = ['quantile'], eval_set = [(train_x, train_y), (val_x, val_y)], early_stopping_rounds = 200, verbose = 500)\n",
    "        pred_1 = pd.concat([pred_1, pd.DataFrame(model.predict(x_test_1))], axis = 1)\n",
    "\n",
    "\n",
    "result_1 = pred_1.iloc[:,0:9]\n",
    "result_2 = pred_1.iloc[:,9:18]\n",
    "result_3 = pred_1.iloc[:,18:27]\n",
    "result_4 = pred_1.iloc[:,27:36]\n",
    "result_5 = pred_1.iloc[:,36:45]\n",
    "\n",
    "result = (result_1+result_2+result_3+result_4+result_5)/5\n",
    "result.columns = sub.iloc[:,1:].columns\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[sub.id.str.contains(\"Day7\"), \"q_0.1\":] = result.sort_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.42945\tvalid_1's quantile: 1.42605\n",
      "[1000]\ttraining's quantile: 1.41769\tvalid_1's quantile: 1.42113\n",
      "[1500]\ttraining's quantile: 1.41138\tvalid_1's quantile: 1.41752\n",
      "[2000]\ttraining's quantile: 1.40401\tvalid_1's quantile: 1.41442\n",
      "[2500]\ttraining's quantile: 1.39865\tvalid_1's quantile: 1.41223\n",
      "[3000]\ttraining's quantile: 1.3919\tvalid_1's quantile: 1.41067\n",
      "[3500]\ttraining's quantile: 1.38782\tvalid_1's quantile: 1.40949\n",
      "[4000]\ttraining's quantile: 1.38574\tvalid_1's quantile: 1.40871\n",
      "[4500]\ttraining's quantile: 1.38424\tvalid_1's quantile: 1.40817\n",
      "Early stopping, best iteration is:\n",
      "[4642]\ttraining's quantile: 1.38406\tvalid_1's quantile: 1.40807\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.3549\tvalid_1's quantile: 2.3779\n",
      "[1000]\ttraining's quantile: 2.31942\tvalid_1's quantile: 2.35058\n",
      "[1500]\ttraining's quantile: 2.30275\tvalid_1's quantile: 2.33911\n",
      "[2000]\ttraining's quantile: 2.28812\tvalid_1's quantile: 2.331\n",
      "[2500]\ttraining's quantile: 2.282\tvalid_1's quantile: 2.3284\n",
      "[3000]\ttraining's quantile: 2.27605\tvalid_1's quantile: 2.32504\n",
      "[3500]\ttraining's quantile: 2.2702\tvalid_1's quantile: 2.32305\n",
      "[4000]\ttraining's quantile: 2.26593\tvalid_1's quantile: 2.32141\n",
      "Early stopping, best iteration is:\n",
      "[3882]\ttraining's quantile: 2.26662\tvalid_1's quantile: 2.32121\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.85819\tvalid_1's quantile: 2.88518\n",
      "[1000]\ttraining's quantile: 2.80993\tvalid_1's quantile: 2.84502\n",
      "[1500]\ttraining's quantile: 2.79094\tvalid_1's quantile: 2.82976\n",
      "[2000]\ttraining's quantile: 2.77879\tvalid_1's quantile: 2.82073\n",
      "[2500]\ttraining's quantile: 2.76469\tvalid_1's quantile: 2.81074\n",
      "[3000]\ttraining's quantile: 2.75498\tvalid_1's quantile: 2.80606\n",
      "[3500]\ttraining's quantile: 2.74666\tvalid_1's quantile: 2.80232\n",
      "[4000]\ttraining's quantile: 2.74165\tvalid_1's quantile: 2.79982\n",
      "[4500]\ttraining's quantile: 2.73464\tvalid_1's quantile: 2.79859\n",
      "Early stopping, best iteration is:\n",
      "[4369]\ttraining's quantile: 2.73649\tvalid_1's quantile: 2.79837\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 3.05134\tvalid_1's quantile: 3.06623\n",
      "[1000]\ttraining's quantile: 2.97656\tvalid_1's quantile: 2.98535\n",
      "[1500]\ttraining's quantile: 2.95247\tvalid_1's quantile: 2.9628\n",
      "[2000]\ttraining's quantile: 2.93301\tvalid_1's quantile: 2.94637\n",
      "[2500]\ttraining's quantile: 2.91777\tvalid_1's quantile: 2.93614\n",
      "[3000]\ttraining's quantile: 2.90849\tvalid_1's quantile: 2.9312\n",
      "[3500]\ttraining's quantile: 2.89855\tvalid_1's quantile: 2.92635\n",
      "[4000]\ttraining's quantile: 2.89031\tvalid_1's quantile: 2.92207\n",
      "[4500]\ttraining's quantile: 2.88304\tvalid_1's quantile: 2.91765\n",
      "[5000]\ttraining's quantile: 2.87663\tvalid_1's quantile: 2.91379\n",
      "[5500]\ttraining's quantile: 2.8705\tvalid_1's quantile: 2.91032\n",
      "[6000]\ttraining's quantile: 2.86476\tvalid_1's quantile: 2.90724\n",
      "[6500]\ttraining's quantile: 2.86033\tvalid_1's quantile: 2.90488\n",
      "[7000]\ttraining's quantile: 2.85586\tvalid_1's quantile: 2.90288\n",
      "[7500]\ttraining's quantile: 2.85226\tvalid_1's quantile: 2.90059\n",
      "[8000]\ttraining's quantile: 2.84846\tvalid_1's quantile: 2.89828\n",
      "[8500]\ttraining's quantile: 2.84462\tvalid_1's quantile: 2.89595\n",
      "[9000]\ttraining's quantile: 2.84163\tvalid_1's quantile: 2.89415\n",
      "[9500]\ttraining's quantile: 2.83926\tvalid_1's quantile: 2.89286\n",
      "[10000]\ttraining's quantile: 2.83647\tvalid_1's quantile: 2.89112\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.83647\tvalid_1's quantile: 2.89112\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 3.01217\tvalid_1's quantile: 3.01605\n",
      "[1000]\ttraining's quantile: 2.89403\tvalid_1's quantile: 2.87819\n",
      "[1500]\ttraining's quantile: 2.85998\tvalid_1's quantile: 2.84416\n",
      "[2000]\ttraining's quantile: 2.83755\tvalid_1's quantile: 2.82311\n",
      "[2500]\ttraining's quantile: 2.82304\tvalid_1's quantile: 2.80903\n",
      "[3000]\ttraining's quantile: 2.8131\tvalid_1's quantile: 2.80088\n",
      "[3500]\ttraining's quantile: 2.80398\tvalid_1's quantile: 2.79372\n",
      "[4000]\ttraining's quantile: 2.79479\tvalid_1's quantile: 2.78769\n",
      "[4500]\ttraining's quantile: 2.78862\tvalid_1's quantile: 2.78391\n",
      "[5000]\ttraining's quantile: 2.78452\tvalid_1's quantile: 2.78052\n",
      "[5500]\ttraining's quantile: 2.77952\tvalid_1's quantile: 2.77694\n",
      "[6000]\ttraining's quantile: 2.77523\tvalid_1's quantile: 2.77454\n",
      "[6500]\ttraining's quantile: 2.7713\tvalid_1's quantile: 2.77298\n",
      "[7000]\ttraining's quantile: 2.76723\tvalid_1's quantile: 2.77102\n",
      "[7500]\ttraining's quantile: 2.76335\tvalid_1's quantile: 2.76934\n",
      "[8000]\ttraining's quantile: 2.76126\tvalid_1's quantile: 2.76841\n",
      "[8500]\ttraining's quantile: 2.75897\tvalid_1's quantile: 2.76738\n",
      "[9000]\ttraining's quantile: 2.75539\tvalid_1's quantile: 2.76591\n",
      "[9500]\ttraining's quantile: 2.7527\tvalid_1's quantile: 2.76497\n",
      "[10000]\ttraining's quantile: 2.75075\tvalid_1's quantile: 2.76388\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.75075\tvalid_1's quantile: 2.76388\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.75665\tvalid_1's quantile: 2.74942\n",
      "[1000]\ttraining's quantile: 2.61685\tvalid_1's quantile: 2.6007\n",
      "[1500]\ttraining's quantile: 2.57859\tvalid_1's quantile: 2.56406\n",
      "[2000]\ttraining's quantile: 2.55128\tvalid_1's quantile: 2.53908\n",
      "[2500]\ttraining's quantile: 2.53601\tvalid_1's quantile: 2.52612\n",
      "[3000]\ttraining's quantile: 2.52585\tvalid_1's quantile: 2.51755\n",
      "[3500]\ttraining's quantile: 2.5154\tvalid_1's quantile: 2.50989\n",
      "[4000]\ttraining's quantile: 2.50682\tvalid_1's quantile: 2.50403\n",
      "[4500]\ttraining's quantile: 2.49968\tvalid_1's quantile: 2.49961\n",
      "[5000]\ttraining's quantile: 2.49328\tvalid_1's quantile: 2.49497\n",
      "[5500]\ttraining's quantile: 2.48744\tvalid_1's quantile: 2.49236\n",
      "[6000]\ttraining's quantile: 2.4816\tvalid_1's quantile: 2.4887\n",
      "[6500]\ttraining's quantile: 2.47644\tvalid_1's quantile: 2.48539\n",
      "[7000]\ttraining's quantile: 2.47198\tvalid_1's quantile: 2.48355\n",
      "[7500]\ttraining's quantile: 2.46779\tvalid_1's quantile: 2.48184\n",
      "[8000]\ttraining's quantile: 2.46482\tvalid_1's quantile: 2.48077\n",
      "[8500]\ttraining's quantile: 2.46274\tvalid_1's quantile: 2.47982\n",
      "[9000]\ttraining's quantile: 2.46\tvalid_1's quantile: 2.47792\n",
      "[9500]\ttraining's quantile: 2.45665\tvalid_1's quantile: 2.47614\n",
      "[10000]\ttraining's quantile: 2.45341\tvalid_1's quantile: 2.47428\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.45341\tvalid_1's quantile: 2.47428\n",
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.3632\tvalid_1's quantile: 2.3698\n",
      "[1000]\ttraining's quantile: 2.21142\tvalid_1's quantile: 2.21685\n",
      "[1500]\ttraining's quantile: 2.16498\tvalid_1's quantile: 2.16961\n",
      "[2000]\ttraining's quantile: 2.13874\tvalid_1's quantile: 2.1444\n",
      "[2500]\ttraining's quantile: 2.11779\tvalid_1's quantile: 2.1242\n",
      "[3000]\ttraining's quantile: 2.10226\tvalid_1's quantile: 2.10866\n",
      "[3500]\ttraining's quantile: 2.09199\tvalid_1's quantile: 2.09988\n",
      "[4000]\ttraining's quantile: 2.08056\tvalid_1's quantile: 2.08977\n",
      "[4500]\ttraining's quantile: 2.07347\tvalid_1's quantile: 2.08423\n",
      "[5000]\ttraining's quantile: 2.06569\tvalid_1's quantile: 2.07781\n",
      "[5500]\ttraining's quantile: 2.06086\tvalid_1's quantile: 2.07377\n",
      "[6000]\ttraining's quantile: 2.05676\tvalid_1's quantile: 2.07122\n",
      "[6500]\ttraining's quantile: 2.0533\tvalid_1's quantile: 2.06946\n",
      "[7000]\ttraining's quantile: 2.0492\tvalid_1's quantile: 2.067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7500]\ttraining's quantile: 2.04514\tvalid_1's quantile: 2.06402\n",
      "[8000]\ttraining's quantile: 2.04211\tvalid_1's quantile: 2.06166\n",
      "[8500]\ttraining's quantile: 2.03939\tvalid_1's quantile: 2.05991\n",
      "[9000]\ttraining's quantile: 2.03622\tvalid_1's quantile: 2.05861\n",
      "[9500]\ttraining's quantile: 2.0339\tvalid_1's quantile: 2.05758\n",
      "[10000]\ttraining's quantile: 2.03009\tvalid_1's quantile: 2.05427\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.03009\tvalid_1's quantile: 2.05427\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.77735\tvalid_1's quantile: 1.78621\n",
      "[1000]\ttraining's quantile: 1.64867\tvalid_1's quantile: 1.65557\n",
      "[1500]\ttraining's quantile: 1.61807\tvalid_1's quantile: 1.6221\n",
      "[2000]\ttraining's quantile: 1.60095\tvalid_1's quantile: 1.60449\n",
      "[2500]\ttraining's quantile: 1.58506\tvalid_1's quantile: 1.58826\n",
      "[3000]\ttraining's quantile: 1.57251\tvalid_1's quantile: 1.5753\n",
      "[3500]\ttraining's quantile: 1.56432\tvalid_1's quantile: 1.56748\n",
      "[4000]\ttraining's quantile: 1.5575\tvalid_1's quantile: 1.56122\n",
      "[4500]\ttraining's quantile: 1.55135\tvalid_1's quantile: 1.55607\n",
      "[5000]\ttraining's quantile: 1.54555\tvalid_1's quantile: 1.55151\n",
      "[5500]\ttraining's quantile: 1.54155\tvalid_1's quantile: 1.54792\n",
      "[6000]\ttraining's quantile: 1.53713\tvalid_1's quantile: 1.5439\n",
      "[6500]\ttraining's quantile: 1.53304\tvalid_1's quantile: 1.54081\n",
      "[7000]\ttraining's quantile: 1.52979\tvalid_1's quantile: 1.53877\n",
      "[7500]\ttraining's quantile: 1.52692\tvalid_1's quantile: 1.53689\n",
      "[8000]\ttraining's quantile: 1.52416\tvalid_1's quantile: 1.53489\n",
      "[8500]\ttraining's quantile: 1.52159\tvalid_1's quantile: 1.53295\n",
      "[9000]\ttraining's quantile: 1.51944\tvalid_1's quantile: 1.53174\n",
      "[9500]\ttraining's quantile: 1.51751\tvalid_1's quantile: 1.53044\n",
      "[10000]\ttraining's quantile: 1.51537\tvalid_1's quantile: 1.52889\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.51537\tvalid_1's quantile: 1.52889\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.0876\tvalid_1's quantile: 1.09845\n",
      "[1000]\ttraining's quantile: 0.968765\tvalid_1's quantile: 0.98221\n",
      "[1500]\ttraining's quantile: 0.944504\tvalid_1's quantile: 0.957455\n",
      "[2000]\ttraining's quantile: 0.927459\tvalid_1's quantile: 0.942005\n",
      "[2500]\ttraining's quantile: 0.917154\tvalid_1's quantile: 0.931428\n",
      "[3000]\ttraining's quantile: 0.910861\tvalid_1's quantile: 0.92654\n",
      "[3500]\ttraining's quantile: 0.904899\tvalid_1's quantile: 0.92185\n",
      "[4000]\ttraining's quantile: 0.898617\tvalid_1's quantile: 0.917263\n",
      "[4500]\ttraining's quantile: 0.892726\tvalid_1's quantile: 0.910823\n",
      "[5000]\ttraining's quantile: 0.887472\tvalid_1's quantile: 0.905422\n",
      "[5500]\ttraining's quantile: 0.882098\tvalid_1's quantile: 0.899963\n",
      "[6000]\ttraining's quantile: 0.876167\tvalid_1's quantile: 0.893221\n",
      "[6500]\ttraining's quantile: 0.871417\tvalid_1's quantile: 0.887404\n",
      "[7000]\ttraining's quantile: 0.866784\tvalid_1's quantile: 0.88206\n",
      "[7500]\ttraining's quantile: 0.86298\tvalid_1's quantile: 0.87775\n",
      "[8000]\ttraining's quantile: 0.859507\tvalid_1's quantile: 0.873742\n",
      "[8500]\ttraining's quantile: 0.856526\tvalid_1's quantile: 0.870467\n",
      "[9000]\ttraining's quantile: 0.853338\tvalid_1's quantile: 0.867244\n",
      "[9500]\ttraining's quantile: 0.850518\tvalid_1's quantile: 0.864428\n",
      "[10000]\ttraining's quantile: 0.847498\tvalid_1's quantile: 0.861989\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.847498\tvalid_1's quantile: 0.861989\n",
      "1\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.42901\tvalid_1's quantile: 1.43666\n",
      "[1000]\ttraining's quantile: 1.4133\tvalid_1's quantile: 1.42787\n",
      "[1500]\ttraining's quantile: 1.40539\tvalid_1's quantile: 1.424\n",
      "[2000]\ttraining's quantile: 1.39793\tvalid_1's quantile: 1.42079\n",
      "[2500]\ttraining's quantile: 1.39285\tvalid_1's quantile: 1.41892\n",
      "[3000]\ttraining's quantile: 1.38808\tvalid_1's quantile: 1.41732\n",
      "[3500]\ttraining's quantile: 1.38277\tvalid_1's quantile: 1.41641\n",
      "Early stopping, best iteration is:\n",
      "[3487]\ttraining's quantile: 1.38285\tvalid_1's quantile: 1.41637\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.35845\tvalid_1's quantile: 2.36272\n",
      "[1000]\ttraining's quantile: 2.32435\tvalid_1's quantile: 2.34193\n",
      "[1500]\ttraining's quantile: 2.30381\tvalid_1's quantile: 2.33233\n",
      "[2000]\ttraining's quantile: 2.2901\tvalid_1's quantile: 2.3262\n",
      "[2500]\ttraining's quantile: 2.27981\tvalid_1's quantile: 2.32134\n",
      "Early stopping, best iteration is:\n",
      "[2677]\ttraining's quantile: 2.27707\tvalid_1's quantile: 2.32028\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.86053\tvalid_1's quantile: 2.88099\n",
      "[1000]\ttraining's quantile: 2.81012\tvalid_1's quantile: 2.84187\n",
      "[1500]\ttraining's quantile: 2.79308\tvalid_1's quantile: 2.82651\n",
      "[2000]\ttraining's quantile: 2.77859\tvalid_1's quantile: 2.81651\n",
      "[2500]\ttraining's quantile: 2.76529\tvalid_1's quantile: 2.80818\n",
      "[3000]\ttraining's quantile: 2.75742\tvalid_1's quantile: 2.8043\n",
      "[3500]\ttraining's quantile: 2.75133\tvalid_1's quantile: 2.80174\n",
      "[4000]\ttraining's quantile: 2.7463\tvalid_1's quantile: 2.80003\n",
      "[4500]\ttraining's quantile: 2.7412\tvalid_1's quantile: 2.79863\n",
      "Early stopping, best iteration is:\n",
      "[4543]\ttraining's quantile: 2.74075\tvalid_1's quantile: 2.79836\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 3.04651\tvalid_1's quantile: 3.07794\n",
      "[1000]\ttraining's quantile: 2.96712\tvalid_1's quantile: 3.00353\n",
      "[1500]\ttraining's quantile: 2.94217\tvalid_1's quantile: 2.98534\n",
      "[2000]\ttraining's quantile: 2.92096\tvalid_1's quantile: 2.97314\n",
      "[2500]\ttraining's quantile: 2.90648\tvalid_1's quantile: 2.96774\n",
      "[3000]\ttraining's quantile: 2.89515\tvalid_1's quantile: 2.96305\n",
      "[3500]\ttraining's quantile: 2.88561\tvalid_1's quantile: 2.95892\n",
      "[4000]\ttraining's quantile: 2.87686\tvalid_1's quantile: 2.95477\n",
      "[4500]\ttraining's quantile: 2.86763\tvalid_1's quantile: 2.95132\n",
      "[5000]\ttraining's quantile: 2.86267\tvalid_1's quantile: 2.94781\n",
      "[5500]\ttraining's quantile: 2.85699\tvalid_1's quantile: 2.94509\n",
      "[6000]\ttraining's quantile: 2.85325\tvalid_1's quantile: 2.9439\n",
      "Early stopping, best iteration is:\n",
      "[5896]\ttraining's quantile: 2.85466\tvalid_1's quantile: 2.94344\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.99419\tvalid_1's quantile: 3.02493\n",
      "[1000]\ttraining's quantile: 2.87259\tvalid_1's quantile: 2.91447\n",
      "[1500]\ttraining's quantile: 2.84137\tvalid_1's quantile: 2.89169\n",
      "[2000]\ttraining's quantile: 2.82194\tvalid_1's quantile: 2.88077\n",
      "[2500]\ttraining's quantile: 2.80791\tvalid_1's quantile: 2.87367\n",
      "[3000]\ttraining's quantile: 2.7965\tvalid_1's quantile: 2.86847\n",
      "[3500]\ttraining's quantile: 2.78933\tvalid_1's quantile: 2.86562\n",
      "[4000]\ttraining's quantile: 2.78367\tvalid_1's quantile: 2.86256\n",
      "[4500]\ttraining's quantile: 2.77763\tvalid_1's quantile: 2.86069\n",
      "[5000]\ttraining's quantile: 2.77305\tvalid_1's quantile: 2.85917\n",
      "[5500]\ttraining's quantile: 2.76947\tvalid_1's quantile: 2.85791\n",
      "[6000]\ttraining's quantile: 2.76452\tvalid_1's quantile: 2.85516\n",
      "[6500]\ttraining's quantile: 2.75922\tvalid_1's quantile: 2.85295\n",
      "[7000]\ttraining's quantile: 2.75512\tvalid_1's quantile: 2.85141\n",
      "[7500]\ttraining's quantile: 2.75242\tvalid_1's quantile: 2.85051\n",
      "[8000]\ttraining's quantile: 2.7516\tvalid_1's quantile: 2.85026\n",
      "[8500]\ttraining's quantile: 2.75089\tvalid_1's quantile: 2.85004\n",
      "[9000]\ttraining's quantile: 2.74718\tvalid_1's quantile: 2.84777\n",
      "[9500]\ttraining's quantile: 2.74361\tvalid_1's quantile: 2.84601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000]\ttraining's quantile: 2.74027\tvalid_1's quantile: 2.84505\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.74027\tvalid_1's quantile: 2.84505\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.73649\tvalid_1's quantile: 2.77775\n",
      "[1000]\ttraining's quantile: 2.59618\tvalid_1's quantile: 2.64649\n",
      "[1500]\ttraining's quantile: 2.55997\tvalid_1's quantile: 2.61931\n",
      "[2000]\ttraining's quantile: 2.54233\tvalid_1's quantile: 2.60734\n",
      "[2500]\ttraining's quantile: 2.52485\tvalid_1's quantile: 2.59385\n",
      "[3000]\ttraining's quantile: 2.51317\tvalid_1's quantile: 2.58501\n",
      "[3500]\ttraining's quantile: 2.50389\tvalid_1's quantile: 2.58116\n",
      "[4000]\ttraining's quantile: 2.4949\tvalid_1's quantile: 2.57488\n",
      "[4500]\ttraining's quantile: 2.48775\tvalid_1's quantile: 2.56942\n",
      "[5000]\ttraining's quantile: 2.48004\tvalid_1's quantile: 2.56487\n",
      "[5500]\ttraining's quantile: 2.47442\tvalid_1's quantile: 2.56136\n",
      "[6000]\ttraining's quantile: 2.46878\tvalid_1's quantile: 2.55685\n",
      "[6500]\ttraining's quantile: 2.46315\tvalid_1's quantile: 2.55314\n",
      "[7000]\ttraining's quantile: 2.45894\tvalid_1's quantile: 2.55038\n",
      "[7500]\ttraining's quantile: 2.45578\tvalid_1's quantile: 2.54911\n",
      "[8000]\ttraining's quantile: 2.45254\tvalid_1's quantile: 2.54759\n",
      "[8500]\ttraining's quantile: 2.44914\tvalid_1's quantile: 2.54631\n",
      "[9000]\ttraining's quantile: 2.4458\tvalid_1's quantile: 2.54423\n",
      "[9500]\ttraining's quantile: 2.44256\tvalid_1's quantile: 2.54227\n",
      "[10000]\ttraining's quantile: 2.44139\tvalid_1's quantile: 2.54165\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.44139\tvalid_1's quantile: 2.54165\n",
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.34034\tvalid_1's quantile: 2.39019\n",
      "[1000]\ttraining's quantile: 2.18772\tvalid_1's quantile: 2.24163\n",
      "[1500]\ttraining's quantile: 2.14375\tvalid_1's quantile: 2.20355\n",
      "[2000]\ttraining's quantile: 2.11878\tvalid_1's quantile: 2.18238\n",
      "[2500]\ttraining's quantile: 2.10163\tvalid_1's quantile: 2.16644\n",
      "[3000]\ttraining's quantile: 2.08367\tvalid_1's quantile: 2.15057\n",
      "[3500]\ttraining's quantile: 2.07383\tvalid_1's quantile: 2.1444\n",
      "[4000]\ttraining's quantile: 2.06503\tvalid_1's quantile: 2.1393\n",
      "[4500]\ttraining's quantile: 2.05835\tvalid_1's quantile: 2.1363\n",
      "[5000]\ttraining's quantile: 2.05355\tvalid_1's quantile: 2.13295\n",
      "[5500]\ttraining's quantile: 2.04696\tvalid_1's quantile: 2.12808\n",
      "[6000]\ttraining's quantile: 2.03871\tvalid_1's quantile: 2.11983\n",
      "[6500]\ttraining's quantile: 2.03342\tvalid_1's quantile: 2.11575\n",
      "[7000]\ttraining's quantile: 2.02916\tvalid_1's quantile: 2.11292\n",
      "[7500]\ttraining's quantile: 2.02566\tvalid_1's quantile: 2.1116\n",
      "[8000]\ttraining's quantile: 2.02274\tvalid_1's quantile: 2.11015\n",
      "[8500]\ttraining's quantile: 2.01915\tvalid_1's quantile: 2.1074\n",
      "[9000]\ttraining's quantile: 2.01627\tvalid_1's quantile: 2.1061\n",
      "[9500]\ttraining's quantile: 2.01361\tvalid_1's quantile: 2.10416\n",
      "[10000]\ttraining's quantile: 2.01174\tvalid_1's quantile: 2.1027\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.01174\tvalid_1's quantile: 2.1027\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.75646\tvalid_1's quantile: 1.80747\n",
      "[1000]\ttraining's quantile: 1.63098\tvalid_1's quantile: 1.68709\n",
      "[1500]\ttraining's quantile: 1.5994\tvalid_1's quantile: 1.65701\n",
      "[2000]\ttraining's quantile: 1.58078\tvalid_1's quantile: 1.63925\n",
      "[2500]\ttraining's quantile: 1.56114\tvalid_1's quantile: 1.62028\n",
      "[3000]\ttraining's quantile: 1.54838\tvalid_1's quantile: 1.60886\n",
      "[3500]\ttraining's quantile: 1.53921\tvalid_1's quantile: 1.60011\n",
      "[4000]\ttraining's quantile: 1.53275\tvalid_1's quantile: 1.59406\n",
      "[4500]\ttraining's quantile: 1.52833\tvalid_1's quantile: 1.59006\n",
      "[5000]\ttraining's quantile: 1.52378\tvalid_1's quantile: 1.58759\n",
      "[5500]\ttraining's quantile: 1.51987\tvalid_1's quantile: 1.585\n",
      "[6000]\ttraining's quantile: 1.51684\tvalid_1's quantile: 1.58304\n",
      "[6500]\ttraining's quantile: 1.51412\tvalid_1's quantile: 1.58177\n",
      "[7000]\ttraining's quantile: 1.51085\tvalid_1's quantile: 1.57952\n",
      "[7500]\ttraining's quantile: 1.50838\tvalid_1's quantile: 1.57814\n",
      "[8000]\ttraining's quantile: 1.50569\tvalid_1's quantile: 1.57589\n",
      "[8500]\ttraining's quantile: 1.50342\tvalid_1's quantile: 1.57417\n",
      "[9000]\ttraining's quantile: 1.50158\tvalid_1's quantile: 1.57297\n",
      "[9500]\ttraining's quantile: 1.49976\tvalid_1's quantile: 1.57201\n",
      "[10000]\ttraining's quantile: 1.4978\tvalid_1's quantile: 1.57045\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.4978\tvalid_1's quantile: 1.57045\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.0775\tvalid_1's quantile: 1.11138\n",
      "[1000]\ttraining's quantile: 0.964639\tvalid_1's quantile: 0.999011\n",
      "[1500]\ttraining's quantile: 0.937053\tvalid_1's quantile: 0.970596\n",
      "[2000]\ttraining's quantile: 0.918561\tvalid_1's quantile: 0.952742\n",
      "[2500]\ttraining's quantile: 0.908558\tvalid_1's quantile: 0.943709\n",
      "[3000]\ttraining's quantile: 0.903226\tvalid_1's quantile: 0.938522\n",
      "[3500]\ttraining's quantile: 0.897687\tvalid_1's quantile: 0.933611\n",
      "[4000]\ttraining's quantile: 0.891844\tvalid_1's quantile: 0.927611\n",
      "[4500]\ttraining's quantile: 0.885581\tvalid_1's quantile: 0.921325\n",
      "[5000]\ttraining's quantile: 0.878058\tvalid_1's quantile: 0.913648\n",
      "[5500]\ttraining's quantile: 0.871302\tvalid_1's quantile: 0.906799\n",
      "[6000]\ttraining's quantile: 0.867831\tvalid_1's quantile: 0.903518\n",
      "[6500]\ttraining's quantile: 0.86368\tvalid_1's quantile: 0.89952\n",
      "[7000]\ttraining's quantile: 0.86061\tvalid_1's quantile: 0.896888\n",
      "[7500]\ttraining's quantile: 0.857592\tvalid_1's quantile: 0.894157\n",
      "[8000]\ttraining's quantile: 0.854724\tvalid_1's quantile: 0.891322\n",
      "[8500]\ttraining's quantile: 0.852216\tvalid_1's quantile: 0.888905\n",
      "[9000]\ttraining's quantile: 0.850022\tvalid_1's quantile: 0.887034\n",
      "[9500]\ttraining's quantile: 0.84784\tvalid_1's quantile: 0.885255\n",
      "[10000]\ttraining's quantile: 0.845923\tvalid_1's quantile: 0.883293\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.845923\tvalid_1's quantile: 0.883293\n",
      "2\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.42783\tvalid_1's quantile: 1.44673\n",
      "[1000]\ttraining's quantile: 1.4124\tvalid_1's quantile: 1.43754\n",
      "[1500]\ttraining's quantile: 1.40418\tvalid_1's quantile: 1.43456\n",
      "[2000]\ttraining's quantile: 1.39869\tvalid_1's quantile: 1.43294\n",
      "[2500]\ttraining's quantile: 1.39386\tvalid_1's quantile: 1.43198\n",
      "[3000]\ttraining's quantile: 1.39046\tvalid_1's quantile: 1.43085\n",
      "[3500]\ttraining's quantile: 1.38535\tvalid_1's quantile: 1.42867\n",
      "[4000]\ttraining's quantile: 1.38229\tvalid_1's quantile: 1.42772\n",
      "Early stopping, best iteration is:\n",
      "[3971]\ttraining's quantile: 1.3826\tvalid_1's quantile: 1.42762\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.3568\tvalid_1's quantile: 2.38176\n",
      "[1000]\ttraining's quantile: 2.32124\tvalid_1's quantile: 2.35324\n",
      "[1500]\ttraining's quantile: 2.30504\tvalid_1's quantile: 2.33936\n",
      "[2000]\ttraining's quantile: 2.29493\tvalid_1's quantile: 2.33286\n",
      "[2500]\ttraining's quantile: 2.28387\tvalid_1's quantile: 2.32676\n",
      "[3000]\ttraining's quantile: 2.27575\tvalid_1's quantile: 2.32332\n",
      "[3500]\ttraining's quantile: 2.2698\tvalid_1's quantile: 2.32093\n",
      "[4000]\ttraining's quantile: 2.26542\tvalid_1's quantile: 2.31895\n",
      "[4500]\ttraining's quantile: 2.26132\tvalid_1's quantile: 2.31654\n",
      "[5000]\ttraining's quantile: 2.25866\tvalid_1's quantile: 2.31539\n",
      "[5500]\ttraining's quantile: 2.25579\tvalid_1's quantile: 2.31448\n",
      "Early stopping, best iteration is:\n",
      "[5712]\ttraining's quantile: 2.25487\tvalid_1's quantile: 2.31427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.86161\tvalid_1's quantile: 2.87769\n",
      "[1000]\ttraining's quantile: 2.80958\tvalid_1's quantile: 2.83399\n",
      "[1500]\ttraining's quantile: 2.79433\tvalid_1's quantile: 2.82152\n",
      "[2000]\ttraining's quantile: 2.78151\tvalid_1's quantile: 2.81091\n",
      "[2500]\ttraining's quantile: 2.77137\tvalid_1's quantile: 2.80454\n",
      "[3000]\ttraining's quantile: 2.76088\tvalid_1's quantile: 2.79924\n",
      "[3500]\ttraining's quantile: 2.75501\tvalid_1's quantile: 2.79639\n",
      "[4000]\ttraining's quantile: 2.7511\tvalid_1's quantile: 2.79401\n",
      "[4500]\ttraining's quantile: 2.74724\tvalid_1's quantile: 2.7924\n",
      "[5000]\ttraining's quantile: 2.7415\tvalid_1's quantile: 2.78956\n",
      "[5500]\ttraining's quantile: 2.7354\tvalid_1's quantile: 2.78528\n",
      "[6000]\ttraining's quantile: 2.73083\tvalid_1's quantile: 2.78193\n",
      "[6500]\ttraining's quantile: 2.7279\tvalid_1's quantile: 2.78005\n",
      "[7000]\ttraining's quantile: 2.72485\tvalid_1's quantile: 2.77813\n",
      "[7500]\ttraining's quantile: 2.72203\tvalid_1's quantile: 2.77633\n",
      "[8000]\ttraining's quantile: 2.7188\tvalid_1's quantile: 2.77448\n",
      "[8500]\ttraining's quantile: 2.71628\tvalid_1's quantile: 2.77291\n",
      "[9000]\ttraining's quantile: 2.71312\tvalid_1's quantile: 2.77103\n",
      "[9500]\ttraining's quantile: 2.70994\tvalid_1's quantile: 2.76942\n",
      "[10000]\ttraining's quantile: 2.70617\tvalid_1's quantile: 2.76807\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.70617\tvalid_1's quantile: 2.76807\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 3.04343\tvalid_1's quantile: 3.05828\n",
      "[1000]\ttraining's quantile: 2.96878\tvalid_1's quantile: 2.99942\n",
      "[1500]\ttraining's quantile: 2.94346\tvalid_1's quantile: 2.97958\n",
      "[2000]\ttraining's quantile: 2.92703\tvalid_1's quantile: 2.96667\n",
      "[2500]\ttraining's quantile: 2.91605\tvalid_1's quantile: 2.95844\n",
      "[3000]\ttraining's quantile: 2.904\tvalid_1's quantile: 2.95069\n",
      "[3500]\ttraining's quantile: 2.89644\tvalid_1's quantile: 2.94508\n",
      "[4000]\ttraining's quantile: 2.88907\tvalid_1's quantile: 2.94159\n",
      "[4500]\ttraining's quantile: 2.87985\tvalid_1's quantile: 2.93843\n",
      "[5000]\ttraining's quantile: 2.87112\tvalid_1's quantile: 2.93464\n",
      "[5500]\ttraining's quantile: 2.86537\tvalid_1's quantile: 2.93251\n",
      "[6000]\ttraining's quantile: 2.86035\tvalid_1's quantile: 2.9307\n",
      "[6500]\ttraining's quantile: 2.85613\tvalid_1's quantile: 2.92796\n",
      "[7000]\ttraining's quantile: 2.85224\tvalid_1's quantile: 2.9262\n",
      "[7500]\ttraining's quantile: 2.84803\tvalid_1's quantile: 2.9244\n",
      "[8000]\ttraining's quantile: 2.84417\tvalid_1's quantile: 2.923\n",
      "[8500]\ttraining's quantile: 2.84102\tvalid_1's quantile: 2.92128\n",
      "[9000]\ttraining's quantile: 2.83774\tvalid_1's quantile: 2.91956\n",
      "[9500]\ttraining's quantile: 2.83465\tvalid_1's quantile: 2.91798\n",
      "[10000]\ttraining's quantile: 2.83177\tvalid_1's quantile: 2.91658\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.83177\tvalid_1's quantile: 2.91658\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.99192\tvalid_1's quantile: 3.0178\n",
      "[1000]\ttraining's quantile: 2.87801\tvalid_1's quantile: 2.92709\n",
      "[1500]\ttraining's quantile: 2.84454\tvalid_1's quantile: 2.90009\n",
      "[2000]\ttraining's quantile: 2.8257\tvalid_1's quantile: 2.88609\n",
      "[2500]\ttraining's quantile: 2.81401\tvalid_1's quantile: 2.8782\n",
      "[3000]\ttraining's quantile: 2.80544\tvalid_1's quantile: 2.87377\n",
      "[3500]\ttraining's quantile: 2.79834\tvalid_1's quantile: 2.86916\n",
      "[4000]\ttraining's quantile: 2.79211\tvalid_1's quantile: 2.86398\n",
      "[4500]\ttraining's quantile: 2.78593\tvalid_1's quantile: 2.85901\n",
      "[5000]\ttraining's quantile: 2.78051\tvalid_1's quantile: 2.85479\n",
      "[5500]\ttraining's quantile: 2.77602\tvalid_1's quantile: 2.85097\n",
      "[6000]\ttraining's quantile: 2.77238\tvalid_1's quantile: 2.84781\n",
      "[6500]\ttraining's quantile: 2.76825\tvalid_1's quantile: 2.84476\n",
      "[7000]\ttraining's quantile: 2.76403\tvalid_1's quantile: 2.84185\n",
      "[7500]\ttraining's quantile: 2.75917\tvalid_1's quantile: 2.83935\n",
      "[8000]\ttraining's quantile: 2.75385\tvalid_1's quantile: 2.83592\n",
      "[8500]\ttraining's quantile: 2.74855\tvalid_1's quantile: 2.83343\n",
      "[9000]\ttraining's quantile: 2.74471\tvalid_1's quantile: 2.8318\n",
      "[9500]\ttraining's quantile: 2.7409\tvalid_1's quantile: 2.82967\n",
      "[10000]\ttraining's quantile: 2.73736\tvalid_1's quantile: 2.82703\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.73736\tvalid_1's quantile: 2.82703\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.74268\tvalid_1's quantile: 2.78547\n",
      "[1000]\ttraining's quantile: 2.60159\tvalid_1's quantile: 2.6536\n",
      "[1500]\ttraining's quantile: 2.56552\tvalid_1's quantile: 2.62628\n",
      "[2000]\ttraining's quantile: 2.53933\tvalid_1's quantile: 2.60462\n",
      "[2500]\ttraining's quantile: 2.52325\tvalid_1's quantile: 2.59071\n",
      "[3000]\ttraining's quantile: 2.51236\tvalid_1's quantile: 2.5832\n",
      "[3500]\ttraining's quantile: 2.50336\tvalid_1's quantile: 2.57638\n",
      "[4000]\ttraining's quantile: 2.49513\tvalid_1's quantile: 2.57162\n",
      "[4500]\ttraining's quantile: 2.48831\tvalid_1's quantile: 2.56666\n",
      "[5000]\ttraining's quantile: 2.4823\tvalid_1's quantile: 2.56308\n",
      "[5500]\ttraining's quantile: 2.47703\tvalid_1's quantile: 2.5598\n",
      "[6000]\ttraining's quantile: 2.47173\tvalid_1's quantile: 2.55532\n",
      "[6500]\ttraining's quantile: 2.46724\tvalid_1's quantile: 2.55223\n",
      "[7000]\ttraining's quantile: 2.46314\tvalid_1's quantile: 2.54985\n",
      "[7500]\ttraining's quantile: 2.46076\tvalid_1's quantile: 2.54833\n",
      "[8000]\ttraining's quantile: 2.45793\tvalid_1's quantile: 2.54597\n",
      "[8500]\ttraining's quantile: 2.45459\tvalid_1's quantile: 2.54302\n",
      "[9000]\ttraining's quantile: 2.45061\tvalid_1's quantile: 2.5397\n",
      "[9500]\ttraining's quantile: 2.44679\tvalid_1's quantile: 2.53658\n",
      "[10000]\ttraining's quantile: 2.44395\tvalid_1's quantile: 2.5346\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.44395\tvalid_1's quantile: 2.5346\n",
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.34948\tvalid_1's quantile: 2.39294\n",
      "[1000]\ttraining's quantile: 2.19273\tvalid_1's quantile: 2.24154\n",
      "[1500]\ttraining's quantile: 2.14595\tvalid_1's quantile: 2.19897\n",
      "[2000]\ttraining's quantile: 2.11984\tvalid_1's quantile: 2.17252\n",
      "[2500]\ttraining's quantile: 2.09954\tvalid_1's quantile: 2.15589\n",
      "[3000]\ttraining's quantile: 2.0868\tvalid_1's quantile: 2.14563\n",
      "[3500]\ttraining's quantile: 2.07287\tvalid_1's quantile: 2.13322\n",
      "[4000]\ttraining's quantile: 2.06337\tvalid_1's quantile: 2.12601\n",
      "[4500]\ttraining's quantile: 2.05838\tvalid_1's quantile: 2.12259\n",
      "[5000]\ttraining's quantile: 2.05223\tvalid_1's quantile: 2.11726\n",
      "[5500]\ttraining's quantile: 2.04731\tvalid_1's quantile: 2.11318\n",
      "[6000]\ttraining's quantile: 2.04433\tvalid_1's quantile: 2.11141\n",
      "[6500]\ttraining's quantile: 2.0414\tvalid_1's quantile: 2.11002\n",
      "[7000]\ttraining's quantile: 2.03866\tvalid_1's quantile: 2.10844\n",
      "[7500]\ttraining's quantile: 2.03618\tvalid_1's quantile: 2.10668\n",
      "[8000]\ttraining's quantile: 2.0329\tvalid_1's quantile: 2.10384\n",
      "[8500]\ttraining's quantile: 2.02913\tvalid_1's quantile: 2.10084\n",
      "[9000]\ttraining's quantile: 2.02674\tvalid_1's quantile: 2.09951\n",
      "[9500]\ttraining's quantile: 2.02462\tvalid_1's quantile: 2.09789\n",
      "[10000]\ttraining's quantile: 2.02272\tvalid_1's quantile: 2.09695\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.02272\tvalid_1's quantile: 2.09695\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.75959\tvalid_1's quantile: 1.797\n",
      "[1000]\ttraining's quantile: 1.63213\tvalid_1's quantile: 1.67215\n",
      "[1500]\ttraining's quantile: 1.60134\tvalid_1's quantile: 1.64511\n",
      "[2000]\ttraining's quantile: 1.5855\tvalid_1's quantile: 1.6327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500]\ttraining's quantile: 1.57408\tvalid_1's quantile: 1.62365\n",
      "[3000]\ttraining's quantile: 1.56283\tvalid_1's quantile: 1.61455\n",
      "[3500]\ttraining's quantile: 1.55363\tvalid_1's quantile: 1.60589\n",
      "[4000]\ttraining's quantile: 1.5455\tvalid_1's quantile: 1.59869\n",
      "[4500]\ttraining's quantile: 1.54004\tvalid_1's quantile: 1.59401\n",
      "[5000]\ttraining's quantile: 1.53653\tvalid_1's quantile: 1.59138\n",
      "[5500]\ttraining's quantile: 1.53296\tvalid_1's quantile: 1.58851\n",
      "[6000]\ttraining's quantile: 1.53017\tvalid_1's quantile: 1.58603\n",
      "[6500]\ttraining's quantile: 1.52731\tvalid_1's quantile: 1.58351\n",
      "[7000]\ttraining's quantile: 1.52499\tvalid_1's quantile: 1.5816\n",
      "[7500]\ttraining's quantile: 1.52276\tvalid_1's quantile: 1.57975\n",
      "[8000]\ttraining's quantile: 1.51843\tvalid_1's quantile: 1.5757\n",
      "[8500]\ttraining's quantile: 1.51623\tvalid_1's quantile: 1.57391\n",
      "[9000]\ttraining's quantile: 1.51444\tvalid_1's quantile: 1.57234\n",
      "[9500]\ttraining's quantile: 1.51253\tvalid_1's quantile: 1.57114\n",
      "[10000]\ttraining's quantile: 1.5096\tvalid_1's quantile: 1.56869\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.5096\tvalid_1's quantile: 1.56869\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.08409\tvalid_1's quantile: 1.09664\n",
      "[1000]\ttraining's quantile: 0.974525\tvalid_1's quantile: 0.988779\n",
      "[1500]\ttraining's quantile: 0.941404\tvalid_1's quantile: 0.959868\n",
      "[2000]\ttraining's quantile: 0.923116\tvalid_1's quantile: 0.943332\n",
      "[2500]\ttraining's quantile: 0.913175\tvalid_1's quantile: 0.934565\n",
      "[3000]\ttraining's quantile: 0.906922\tvalid_1's quantile: 0.928173\n",
      "[3500]\ttraining's quantile: 0.901479\tvalid_1's quantile: 0.923569\n",
      "[4000]\ttraining's quantile: 0.89587\tvalid_1's quantile: 0.918792\n",
      "[4500]\ttraining's quantile: 0.888791\tvalid_1's quantile: 0.913229\n",
      "[5000]\ttraining's quantile: 0.883217\tvalid_1's quantile: 0.909007\n",
      "[5500]\ttraining's quantile: 0.87876\tvalid_1's quantile: 0.905631\n",
      "[6000]\ttraining's quantile: 0.873539\tvalid_1's quantile: 0.90147\n",
      "[6500]\ttraining's quantile: 0.870445\tvalid_1's quantile: 0.899214\n",
      "[7000]\ttraining's quantile: 0.867716\tvalid_1's quantile: 0.89727\n",
      "[7500]\ttraining's quantile: 0.864755\tvalid_1's quantile: 0.89522\n",
      "[8000]\ttraining's quantile: 0.861029\tvalid_1's quantile: 0.892125\n",
      "[8500]\ttraining's quantile: 0.858997\tvalid_1's quantile: 0.890581\n",
      "[9000]\ttraining's quantile: 0.856113\tvalid_1's quantile: 0.888474\n",
      "[9500]\ttraining's quantile: 0.854219\tvalid_1's quantile: 0.887333\n",
      "[10000]\ttraining's quantile: 0.852266\tvalid_1's quantile: 0.885922\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.852266\tvalid_1's quantile: 0.885922\n",
      "3\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.43184\tvalid_1's quantile: 1.42207\n",
      "[1000]\ttraining's quantile: 1.41566\tvalid_1's quantile: 1.4124\n",
      "[1500]\ttraining's quantile: 1.40817\tvalid_1's quantile: 1.40788\n",
      "[2000]\ttraining's quantile: 1.40444\tvalid_1's quantile: 1.40541\n",
      "[2500]\ttraining's quantile: 1.39965\tvalid_1's quantile: 1.40239\n",
      "[3000]\ttraining's quantile: 1.39665\tvalid_1's quantile: 1.40036\n",
      "[3500]\ttraining's quantile: 1.39409\tvalid_1's quantile: 1.39943\n",
      "[4000]\ttraining's quantile: 1.39068\tvalid_1's quantile: 1.39819\n",
      "[4500]\ttraining's quantile: 1.3872\tvalid_1's quantile: 1.39755\n",
      "[5000]\ttraining's quantile: 1.38498\tvalid_1's quantile: 1.39685\n",
      "Early stopping, best iteration is:\n",
      "[4968]\ttraining's quantile: 1.38505\tvalid_1's quantile: 1.39684\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.36406\tvalid_1's quantile: 2.33035\n",
      "[1000]\ttraining's quantile: 2.33236\tvalid_1's quantile: 2.31507\n",
      "[1500]\ttraining's quantile: 2.31391\tvalid_1's quantile: 2.30512\n",
      "[2000]\ttraining's quantile: 2.29995\tvalid_1's quantile: 2.29936\n",
      "[2500]\ttraining's quantile: 2.29172\tvalid_1's quantile: 2.29585\n",
      "Early stopping, best iteration is:\n",
      "[2618]\ttraining's quantile: 2.29029\tvalid_1's quantile: 2.29548\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.8683\tvalid_1's quantile: 2.83878\n",
      "[1000]\ttraining's quantile: 2.81709\tvalid_1's quantile: 2.80373\n",
      "[1500]\ttraining's quantile: 2.79823\tvalid_1's quantile: 2.79442\n",
      "[2000]\ttraining's quantile: 2.78288\tvalid_1's quantile: 2.78646\n",
      "[2500]\ttraining's quantile: 2.77265\tvalid_1's quantile: 2.78368\n",
      "[3000]\ttraining's quantile: 2.7634\tvalid_1's quantile: 2.77711\n",
      "[3500]\ttraining's quantile: 2.75559\tvalid_1's quantile: 2.77365\n",
      "[4000]\ttraining's quantile: 2.75056\tvalid_1's quantile: 2.7717\n",
      "[4500]\ttraining's quantile: 2.74518\tvalid_1's quantile: 2.7696\n",
      "[5000]\ttraining's quantile: 2.74002\tvalid_1's quantile: 2.76767\n",
      "[5500]\ttraining's quantile: 2.7332\tvalid_1's quantile: 2.76379\n",
      "[6000]\ttraining's quantile: 2.72783\tvalid_1's quantile: 2.76133\n",
      "[6500]\ttraining's quantile: 2.72257\tvalid_1's quantile: 2.75912\n",
      "[7000]\ttraining's quantile: 2.71814\tvalid_1's quantile: 2.75663\n",
      "[7500]\ttraining's quantile: 2.7147\tvalid_1's quantile: 2.75572\n",
      "[8000]\ttraining's quantile: 2.71027\tvalid_1's quantile: 2.75448\n",
      "[8500]\ttraining's quantile: 2.70721\tvalid_1's quantile: 2.75351\n",
      "[9000]\ttraining's quantile: 2.70392\tvalid_1's quantile: 2.75222\n",
      "[9500]\ttraining's quantile: 2.70012\tvalid_1's quantile: 2.75171\n",
      "Early stopping, best iteration is:\n",
      "[9314]\ttraining's quantile: 2.70166\tvalid_1's quantile: 2.75163\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 3.05616\tvalid_1's quantile: 3.02588\n",
      "[1000]\ttraining's quantile: 2.97569\tvalid_1's quantile: 2.96571\n",
      "[1500]\ttraining's quantile: 2.9465\tvalid_1's quantile: 2.94941\n",
      "[2000]\ttraining's quantile: 2.92807\tvalid_1's quantile: 2.94138\n",
      "[2500]\ttraining's quantile: 2.91604\tvalid_1's quantile: 2.9361\n",
      "[3000]\ttraining's quantile: 2.90459\tvalid_1's quantile: 2.93222\n",
      "[3500]\ttraining's quantile: 2.89447\tvalid_1's quantile: 2.9287\n",
      "[4000]\ttraining's quantile: 2.88402\tvalid_1's quantile: 2.9244\n",
      "[4500]\ttraining's quantile: 2.87781\tvalid_1's quantile: 2.9221\n",
      "[5000]\ttraining's quantile: 2.87311\tvalid_1's quantile: 2.92052\n",
      "[5500]\ttraining's quantile: 2.86831\tvalid_1's quantile: 2.91913\n",
      "[6000]\ttraining's quantile: 2.86287\tvalid_1's quantile: 2.91657\n",
      "[6500]\ttraining's quantile: 2.85963\tvalid_1's quantile: 2.91555\n",
      "[7000]\ttraining's quantile: 2.85474\tvalid_1's quantile: 2.91388\n",
      "[7500]\ttraining's quantile: 2.85079\tvalid_1's quantile: 2.91315\n",
      "[8000]\ttraining's quantile: 2.84575\tvalid_1's quantile: 2.91177\n",
      "[8500]\ttraining's quantile: 2.84319\tvalid_1's quantile: 2.91062\n",
      "[9000]\ttraining's quantile: 2.84105\tvalid_1's quantile: 2.90997\n",
      "[9500]\ttraining's quantile: 2.8392\tvalid_1's quantile: 2.90952\n",
      "[10000]\ttraining's quantile: 2.83626\tvalid_1's quantile: 2.90843\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.83626\tvalid_1's quantile: 2.90843\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 3.00359\tvalid_1's quantile: 2.96822\n",
      "[1000]\ttraining's quantile: 2.88363\tvalid_1's quantile: 2.87203\n",
      "[1500]\ttraining's quantile: 2.8474\tvalid_1's quantile: 2.85084\n",
      "[2000]\ttraining's quantile: 2.8303\tvalid_1's quantile: 2.84118\n",
      "[2500]\ttraining's quantile: 2.81627\tvalid_1's quantile: 2.83487\n",
      "[3000]\ttraining's quantile: 2.80482\tvalid_1's quantile: 2.83017\n",
      "[3500]\ttraining's quantile: 2.79815\tvalid_1's quantile: 2.82641\n",
      "[4000]\ttraining's quantile: 2.79248\tvalid_1's quantile: 2.8235\n",
      "[4500]\ttraining's quantile: 2.78771\tvalid_1's quantile: 2.82159\n",
      "[5000]\ttraining's quantile: 2.78278\tvalid_1's quantile: 2.81935\n",
      "[5500]\ttraining's quantile: 2.77686\tvalid_1's quantile: 2.81709\n",
      "[6000]\ttraining's quantile: 2.77042\tvalid_1's quantile: 2.81448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6500]\ttraining's quantile: 2.76519\tvalid_1's quantile: 2.81245\n",
      "[7000]\ttraining's quantile: 2.75958\tvalid_1's quantile: 2.81107\n",
      "[7500]\ttraining's quantile: 2.75501\tvalid_1's quantile: 2.80998\n",
      "[8000]\ttraining's quantile: 2.74921\tvalid_1's quantile: 2.80775\n",
      "[8500]\ttraining's quantile: 2.74532\tvalid_1's quantile: 2.80635\n",
      "[9000]\ttraining's quantile: 2.74217\tvalid_1's quantile: 2.80542\n",
      "[9500]\ttraining's quantile: 2.73939\tvalid_1's quantile: 2.80444\n",
      "[10000]\ttraining's quantile: 2.73603\tvalid_1's quantile: 2.80294\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.73603\tvalid_1's quantile: 2.80294\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.75221\tvalid_1's quantile: 2.70779\n",
      "[1000]\ttraining's quantile: 2.60954\tvalid_1's quantile: 2.58968\n",
      "[1500]\ttraining's quantile: 2.57153\tvalid_1's quantile: 2.56229\n",
      "[2000]\ttraining's quantile: 2.54517\tvalid_1's quantile: 2.54392\n",
      "[2500]\ttraining's quantile: 2.53096\tvalid_1's quantile: 2.5353\n",
      "[3000]\ttraining's quantile: 2.52046\tvalid_1's quantile: 2.5297\n",
      "[3500]\ttraining's quantile: 2.50993\tvalid_1's quantile: 2.52365\n",
      "[4000]\ttraining's quantile: 2.50162\tvalid_1's quantile: 2.51976\n",
      "[4500]\ttraining's quantile: 2.49509\tvalid_1's quantile: 2.51573\n",
      "[5000]\ttraining's quantile: 2.49064\tvalid_1's quantile: 2.51362\n",
      "[5500]\ttraining's quantile: 2.48628\tvalid_1's quantile: 2.51097\n",
      "[6000]\ttraining's quantile: 2.48074\tvalid_1's quantile: 2.50796\n",
      "[6500]\ttraining's quantile: 2.477\tvalid_1's quantile: 2.50647\n",
      "[7000]\ttraining's quantile: 2.47345\tvalid_1's quantile: 2.50486\n",
      "[7500]\ttraining's quantile: 2.47001\tvalid_1's quantile: 2.50282\n",
      "[8000]\ttraining's quantile: 2.46561\tvalid_1's quantile: 2.50142\n",
      "[8500]\ttraining's quantile: 2.46268\tvalid_1's quantile: 2.49978\n",
      "[9000]\ttraining's quantile: 2.45937\tvalid_1's quantile: 2.49845\n",
      "[9500]\ttraining's quantile: 2.45574\tvalid_1's quantile: 2.4968\n",
      "[10000]\ttraining's quantile: 2.45314\tvalid_1's quantile: 2.49558\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.45314\tvalid_1's quantile: 2.49558\n",
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.35793\tvalid_1's quantile: 2.30873\n",
      "[1000]\ttraining's quantile: 2.20636\tvalid_1's quantile: 2.17594\n",
      "[1500]\ttraining's quantile: 2.16047\tvalid_1's quantile: 2.13901\n",
      "[2000]\ttraining's quantile: 2.13419\tvalid_1's quantile: 2.11949\n",
      "[2500]\ttraining's quantile: 2.11158\tvalid_1's quantile: 2.10272\n",
      "[3000]\ttraining's quantile: 2.09798\tvalid_1's quantile: 2.09147\n",
      "[3500]\ttraining's quantile: 2.08734\tvalid_1's quantile: 2.08322\n",
      "[4000]\ttraining's quantile: 2.08036\tvalid_1's quantile: 2.07879\n",
      "[4500]\ttraining's quantile: 2.07225\tvalid_1's quantile: 2.07303\n",
      "[5000]\ttraining's quantile: 2.0667\tvalid_1's quantile: 2.06968\n",
      "[5500]\ttraining's quantile: 2.06123\tvalid_1's quantile: 2.06578\n",
      "[6000]\ttraining's quantile: 2.05748\tvalid_1's quantile: 2.06318\n",
      "[6500]\ttraining's quantile: 2.05355\tvalid_1's quantile: 2.06115\n",
      "Early stopping, best iteration is:\n",
      "[6420]\ttraining's quantile: 2.05393\tvalid_1's quantile: 2.06083\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.77097\tvalid_1's quantile: 1.73625\n",
      "[1000]\ttraining's quantile: 1.64397\tvalid_1's quantile: 1.62331\n",
      "[1500]\ttraining's quantile: 1.61047\tvalid_1's quantile: 1.59867\n",
      "[2000]\ttraining's quantile: 1.58638\tvalid_1's quantile: 1.5843\n",
      "[2500]\ttraining's quantile: 1.57491\tvalid_1's quantile: 1.57573\n",
      "[3000]\ttraining's quantile: 1.56304\tvalid_1's quantile: 1.56562\n",
      "[3500]\ttraining's quantile: 1.55136\tvalid_1's quantile: 1.55466\n",
      "[4000]\ttraining's quantile: 1.54258\tvalid_1's quantile: 1.54747\n",
      "[4500]\ttraining's quantile: 1.53671\tvalid_1's quantile: 1.54211\n",
      "[5000]\ttraining's quantile: 1.53188\tvalid_1's quantile: 1.53799\n",
      "[5500]\ttraining's quantile: 1.52675\tvalid_1's quantile: 1.53364\n",
      "[6000]\ttraining's quantile: 1.52272\tvalid_1's quantile: 1.53049\n",
      "[6500]\ttraining's quantile: 1.5185\tvalid_1's quantile: 1.5274\n",
      "[7000]\ttraining's quantile: 1.51519\tvalid_1's quantile: 1.52491\n",
      "[7500]\ttraining's quantile: 1.51211\tvalid_1's quantile: 1.52273\n",
      "[8000]\ttraining's quantile: 1.50957\tvalid_1's quantile: 1.52047\n",
      "[8500]\ttraining's quantile: 1.50709\tvalid_1's quantile: 1.51762\n",
      "[9000]\ttraining's quantile: 1.5051\tvalid_1's quantile: 1.51597\n",
      "[9500]\ttraining's quantile: 1.50335\tvalid_1's quantile: 1.51458\n",
      "[10000]\ttraining's quantile: 1.50142\tvalid_1's quantile: 1.51328\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.50142\tvalid_1's quantile: 1.51328\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.09214\tvalid_1's quantile: 1.09017\n",
      "[1000]\ttraining's quantile: 0.973904\tvalid_1's quantile: 0.979385\n",
      "[1500]\ttraining's quantile: 0.943066\tvalid_1's quantile: 0.950634\n",
      "[2000]\ttraining's quantile: 0.925484\tvalid_1's quantile: 0.935196\n",
      "[2500]\ttraining's quantile: 0.914949\tvalid_1's quantile: 0.925914\n",
      "[3000]\ttraining's quantile: 0.90672\tvalid_1's quantile: 0.918532\n",
      "[3500]\ttraining's quantile: 0.90041\tvalid_1's quantile: 0.91296\n",
      "[4000]\ttraining's quantile: 0.893607\tvalid_1's quantile: 0.907797\n",
      "[4500]\ttraining's quantile: 0.888187\tvalid_1's quantile: 0.903135\n",
      "[5000]\ttraining's quantile: 0.882425\tvalid_1's quantile: 0.89832\n",
      "[5500]\ttraining's quantile: 0.876682\tvalid_1's quantile: 0.893656\n",
      "[6000]\ttraining's quantile: 0.871678\tvalid_1's quantile: 0.889496\n",
      "[6500]\ttraining's quantile: 0.867382\tvalid_1's quantile: 0.885857\n",
      "[7000]\ttraining's quantile: 0.862311\tvalid_1's quantile: 0.881729\n",
      "[7500]\ttraining's quantile: 0.85792\tvalid_1's quantile: 0.878177\n",
      "[8000]\ttraining's quantile: 0.854356\tvalid_1's quantile: 0.875346\n",
      "[8500]\ttraining's quantile: 0.850389\tvalid_1's quantile: 0.872315\n",
      "[9000]\ttraining's quantile: 0.848085\tvalid_1's quantile: 0.870687\n",
      "[9500]\ttraining's quantile: 0.845551\tvalid_1's quantile: 0.868967\n",
      "[10000]\ttraining's quantile: 0.843215\tvalid_1's quantile: 0.867578\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.843215\tvalid_1's quantile: 0.867578\n",
      "4\n",
      "0.1\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.42266\tvalid_1's quantile: 1.46149\n",
      "[1000]\ttraining's quantile: 1.4073\tvalid_1's quantile: 1.45116\n",
      "[1500]\ttraining's quantile: 1.39739\tvalid_1's quantile: 1.44396\n",
      "[2000]\ttraining's quantile: 1.39054\tvalid_1's quantile: 1.4388\n",
      "[2500]\ttraining's quantile: 1.38615\tvalid_1's quantile: 1.43652\n",
      "[3000]\ttraining's quantile: 1.38359\tvalid_1's quantile: 1.43583\n",
      "[3500]\ttraining's quantile: 1.37919\tvalid_1's quantile: 1.43502\n",
      "Early stopping, best iteration is:\n",
      "[3771]\ttraining's quantile: 1.37752\tvalid_1's quantile: 1.43424\n",
      "0.2\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.34716\tvalid_1's quantile: 2.4164\n",
      "[1000]\ttraining's quantile: 2.31251\tvalid_1's quantile: 2.38689\n",
      "[1500]\ttraining's quantile: 2.29796\tvalid_1's quantile: 2.37846\n",
      "[2000]\ttraining's quantile: 2.28374\tvalid_1's quantile: 2.36933\n",
      "Early stopping, best iteration is:\n",
      "[2083]\ttraining's quantile: 2.28187\tvalid_1's quantile: 2.36825\n",
      "0.3\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.85043\tvalid_1's quantile: 2.9127\n",
      "[1000]\ttraining's quantile: 2.80197\tvalid_1's quantile: 2.86951\n",
      "[1500]\ttraining's quantile: 2.78189\tvalid_1's quantile: 2.85613\n",
      "[2000]\ttraining's quantile: 2.76855\tvalid_1's quantile: 2.84623\n",
      "[2500]\ttraining's quantile: 2.75962\tvalid_1's quantile: 2.83814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\ttraining's quantile: 2.75296\tvalid_1's quantile: 2.83404\n",
      "[3500]\ttraining's quantile: 2.74718\tvalid_1's quantile: 2.83014\n",
      "[4000]\ttraining's quantile: 2.74053\tvalid_1's quantile: 2.827\n",
      "[4500]\ttraining's quantile: 2.73252\tvalid_1's quantile: 2.82492\n",
      "[5000]\ttraining's quantile: 2.7264\tvalid_1's quantile: 2.82174\n",
      "[5500]\ttraining's quantile: 2.71928\tvalid_1's quantile: 2.81959\n",
      "[6000]\ttraining's quantile: 2.71106\tvalid_1's quantile: 2.81633\n",
      "[6500]\ttraining's quantile: 2.70547\tvalid_1's quantile: 2.81494\n",
      "Early stopping, best iteration is:\n",
      "[6315]\ttraining's quantile: 2.7064\tvalid_1's quantile: 2.81476\n",
      "0.4\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 3.03995\tvalid_1's quantile: 3.10729\n",
      "[1000]\ttraining's quantile: 2.96451\tvalid_1's quantile: 3.04116\n",
      "[1500]\ttraining's quantile: 2.9408\tvalid_1's quantile: 3.01977\n",
      "[2000]\ttraining's quantile: 2.92167\tvalid_1's quantile: 3.00112\n",
      "[2500]\ttraining's quantile: 2.9084\tvalid_1's quantile: 2.99114\n",
      "[3000]\ttraining's quantile: 2.90022\tvalid_1's quantile: 2.98423\n",
      "[3500]\ttraining's quantile: 2.89077\tvalid_1's quantile: 2.97905\n",
      "[4000]\ttraining's quantile: 2.88247\tvalid_1's quantile: 2.97338\n",
      "[4500]\ttraining's quantile: 2.87473\tvalid_1's quantile: 2.96741\n",
      "[5000]\ttraining's quantile: 2.86699\tvalid_1's quantile: 2.96101\n",
      "[5500]\ttraining's quantile: 2.86096\tvalid_1's quantile: 2.9558\n",
      "[6000]\ttraining's quantile: 2.85571\tvalid_1's quantile: 2.95158\n",
      "[6500]\ttraining's quantile: 2.85076\tvalid_1's quantile: 2.94981\n",
      "Early stopping, best iteration is:\n",
      "[6392]\ttraining's quantile: 2.85237\tvalid_1's quantile: 2.94942\n",
      "0.5\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.9916\tvalid_1's quantile: 3.05772\n",
      "[1000]\ttraining's quantile: 2.87751\tvalid_1's quantile: 2.94538\n",
      "[1500]\ttraining's quantile: 2.84172\tvalid_1's quantile: 2.90689\n",
      "[2000]\ttraining's quantile: 2.82309\tvalid_1's quantile: 2.88608\n",
      "[2500]\ttraining's quantile: 2.81048\tvalid_1's quantile: 2.87485\n",
      "[3000]\ttraining's quantile: 2.80021\tvalid_1's quantile: 2.86559\n",
      "[3500]\ttraining's quantile: 2.79253\tvalid_1's quantile: 2.85983\n",
      "[4000]\ttraining's quantile: 2.78553\tvalid_1's quantile: 2.85399\n",
      "[4500]\ttraining's quantile: 2.77829\tvalid_1's quantile: 2.84924\n",
      "[5000]\ttraining's quantile: 2.77192\tvalid_1's quantile: 2.84472\n",
      "[5500]\ttraining's quantile: 2.7665\tvalid_1's quantile: 2.84266\n",
      "[6000]\ttraining's quantile: 2.76127\tvalid_1's quantile: 2.83965\n",
      "[6500]\ttraining's quantile: 2.75757\tvalid_1's quantile: 2.83737\n",
      "[7000]\ttraining's quantile: 2.75378\tvalid_1's quantile: 2.8348\n",
      "[7500]\ttraining's quantile: 2.75119\tvalid_1's quantile: 2.83334\n",
      "[8000]\ttraining's quantile: 2.74855\tvalid_1's quantile: 2.83185\n",
      "[8500]\ttraining's quantile: 2.74514\tvalid_1's quantile: 2.83012\n",
      "[9000]\ttraining's quantile: 2.7413\tvalid_1's quantile: 2.82787\n",
      "[9500]\ttraining's quantile: 2.73754\tvalid_1's quantile: 2.82506\n",
      "[10000]\ttraining's quantile: 2.73425\tvalid_1's quantile: 2.82341\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.73425\tvalid_1's quantile: 2.82341\n",
      "0.6\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.73855\tvalid_1's quantile: 2.80302\n",
      "[1000]\ttraining's quantile: 2.59941\tvalid_1's quantile: 2.65101\n",
      "[1500]\ttraining's quantile: 2.5709\tvalid_1's quantile: 2.61639\n",
      "[2000]\ttraining's quantile: 2.54711\tvalid_1's quantile: 2.59061\n",
      "[2500]\ttraining's quantile: 2.52852\tvalid_1's quantile: 2.57028\n",
      "[3000]\ttraining's quantile: 2.5161\tvalid_1's quantile: 2.55681\n",
      "[3500]\ttraining's quantile: 2.50571\tvalid_1's quantile: 2.54601\n",
      "[4000]\ttraining's quantile: 2.49891\tvalid_1's quantile: 2.5392\n",
      "[4500]\ttraining's quantile: 2.49345\tvalid_1's quantile: 2.5351\n",
      "[5000]\ttraining's quantile: 2.48911\tvalid_1's quantile: 2.53201\n",
      "[5500]\ttraining's quantile: 2.48445\tvalid_1's quantile: 2.52873\n",
      "[6000]\ttraining's quantile: 2.48004\tvalid_1's quantile: 2.52526\n",
      "[6500]\ttraining's quantile: 2.47586\tvalid_1's quantile: 2.5222\n",
      "[7000]\ttraining's quantile: 2.47176\tvalid_1's quantile: 2.51801\n",
      "[7500]\ttraining's quantile: 2.46791\tvalid_1's quantile: 2.51499\n",
      "[8000]\ttraining's quantile: 2.46493\tvalid_1's quantile: 2.51335\n",
      "[8500]\ttraining's quantile: 2.46128\tvalid_1's quantile: 2.51092\n",
      "[9000]\ttraining's quantile: 2.45785\tvalid_1's quantile: 2.5091\n",
      "[9500]\ttraining's quantile: 2.45502\tvalid_1's quantile: 2.50758\n",
      "[10000]\ttraining's quantile: 2.45191\tvalid_1's quantile: 2.50545\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.45191\tvalid_1's quantile: 2.50545\n",
      "0.7\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 2.34416\tvalid_1's quantile: 2.38373\n",
      "[1000]\ttraining's quantile: 2.19898\tvalid_1's quantile: 2.22807\n",
      "[1500]\ttraining's quantile: 2.15563\tvalid_1's quantile: 2.18132\n",
      "[2000]\ttraining's quantile: 2.13589\tvalid_1's quantile: 2.16102\n",
      "[2500]\ttraining's quantile: 2.11731\tvalid_1's quantile: 2.1411\n",
      "[3000]\ttraining's quantile: 2.09874\tvalid_1's quantile: 2.12176\n",
      "[3500]\ttraining's quantile: 2.08685\tvalid_1's quantile: 2.11301\n",
      "[4000]\ttraining's quantile: 2.07815\tvalid_1's quantile: 2.10588\n",
      "[4500]\ttraining's quantile: 2.07073\tvalid_1's quantile: 2.10246\n",
      "[5000]\ttraining's quantile: 2.06559\tvalid_1's quantile: 2.0988\n",
      "[5500]\ttraining's quantile: 2.05957\tvalid_1's quantile: 2.09347\n",
      "[6000]\ttraining's quantile: 2.05533\tvalid_1's quantile: 2.08957\n",
      "[6500]\ttraining's quantile: 2.05194\tvalid_1's quantile: 2.08728\n",
      "[7000]\ttraining's quantile: 2.04859\tvalid_1's quantile: 2.08453\n",
      "[7500]\ttraining's quantile: 2.04476\tvalid_1's quantile: 2.08116\n",
      "[8000]\ttraining's quantile: 2.0417\tvalid_1's quantile: 2.07943\n",
      "[8500]\ttraining's quantile: 2.03764\tvalid_1's quantile: 2.07556\n",
      "[9000]\ttraining's quantile: 2.03464\tvalid_1's quantile: 2.07302\n",
      "[9500]\ttraining's quantile: 2.03199\tvalid_1's quantile: 2.07068\n",
      "[10000]\ttraining's quantile: 2.02989\tvalid_1's quantile: 2.06945\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 2.02989\tvalid_1's quantile: 2.06945\n",
      "0.8\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.76522\tvalid_1's quantile: 1.788\n",
      "[1000]\ttraining's quantile: 1.64019\tvalid_1's quantile: 1.65411\n",
      "[1500]\ttraining's quantile: 1.60734\tvalid_1's quantile: 1.62045\n",
      "[2000]\ttraining's quantile: 1.58422\tvalid_1's quantile: 1.59804\n",
      "[2500]\ttraining's quantile: 1.56661\tvalid_1's quantile: 1.58187\n",
      "[3000]\ttraining's quantile: 1.55446\tvalid_1's quantile: 1.57065\n",
      "[3500]\ttraining's quantile: 1.54839\tvalid_1's quantile: 1.56521\n",
      "[4000]\ttraining's quantile: 1.54028\tvalid_1's quantile: 1.55841\n",
      "[4500]\ttraining's quantile: 1.53485\tvalid_1's quantile: 1.55488\n",
      "[5000]\ttraining's quantile: 1.53193\tvalid_1's quantile: 1.55335\n",
      "[5500]\ttraining's quantile: 1.52797\tvalid_1's quantile: 1.55082\n",
      "[6000]\ttraining's quantile: 1.52431\tvalid_1's quantile: 1.54825\n",
      "[6500]\ttraining's quantile: 1.52188\tvalid_1's quantile: 1.54706\n",
      "[7000]\ttraining's quantile: 1.51926\tvalid_1's quantile: 1.54532\n",
      "[7500]\ttraining's quantile: 1.51657\tvalid_1's quantile: 1.54351\n",
      "[8000]\ttraining's quantile: 1.51478\tvalid_1's quantile: 1.5423\n",
      "[8500]\ttraining's quantile: 1.51279\tvalid_1's quantile: 1.54099\n",
      "[9000]\ttraining's quantile: 1.51055\tvalid_1's quantile: 1.53943\n",
      "[9500]\ttraining's quantile: 1.50906\tvalid_1's quantile: 1.53865\n",
      "[10000]\ttraining's quantile: 1.5074\tvalid_1's quantile: 1.53771\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 1.5074\tvalid_1's quantile: 1.53771\n",
      "0.9\n",
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[500]\ttraining's quantile: 1.0822\tvalid_1's quantile: 1.09454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's quantile: 0.967653\tvalid_1's quantile: 0.976246\n",
      "[1500]\ttraining's quantile: 0.937216\tvalid_1's quantile: 0.947614\n",
      "[2000]\ttraining's quantile: 0.922258\tvalid_1's quantile: 0.933944\n",
      "[2500]\ttraining's quantile: 0.912289\tvalid_1's quantile: 0.925368\n",
      "[3000]\ttraining's quantile: 0.904896\tvalid_1's quantile: 0.918243\n",
      "[3500]\ttraining's quantile: 0.898691\tvalid_1's quantile: 0.911938\n",
      "[4000]\ttraining's quantile: 0.892239\tvalid_1's quantile: 0.905482\n",
      "[4500]\ttraining's quantile: 0.884506\tvalid_1's quantile: 0.898476\n",
      "[5000]\ttraining's quantile: 0.878097\tvalid_1's quantile: 0.892996\n",
      "[5500]\ttraining's quantile: 0.872482\tvalid_1's quantile: 0.887863\n",
      "[6000]\ttraining's quantile: 0.868901\tvalid_1's quantile: 0.884546\n",
      "[6500]\ttraining's quantile: 0.865396\tvalid_1's quantile: 0.881369\n",
      "[7000]\ttraining's quantile: 0.861535\tvalid_1's quantile: 0.877499\n",
      "[7500]\ttraining's quantile: 0.85805\tvalid_1's quantile: 0.874162\n",
      "[8000]\ttraining's quantile: 0.855096\tvalid_1's quantile: 0.871769\n",
      "[8500]\ttraining's quantile: 0.850931\tvalid_1's quantile: 0.868124\n",
      "[9000]\ttraining's quantile: 0.848248\tvalid_1's quantile: 0.865855\n",
      "[9500]\ttraining's quantile: 0.845853\tvalid_1's quantile: 0.864054\n",
      "[10000]\ttraining's quantile: 0.843437\tvalid_1's quantile: 0.86218\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's quantile: 0.843437\tvalid_1's quantile: 0.86218\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3888 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      q_0.1  q_0.2  q_0.3  q_0.4  q_0.5         q_0.6         q_0.7  \\\n",
       "0       0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "1       0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "2       0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "3       0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "4       0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "...     ...    ...    ...    ...    ...           ...           ...   \n",
       "3883    0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "3884    0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "3885    0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "3886    0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "3887    0.0    0.0    0.0    0.0    0.0  1.105373e-33  2.900734e-25   \n",
       "\n",
       "             q_0.8         q_0.9  \n",
       "0     1.106410e-33  1.106345e-33  \n",
       "1     1.106410e-33  1.106345e-33  \n",
       "2     1.106410e-33  1.106345e-33  \n",
       "3     1.106410e-33  1.106345e-33  \n",
       "4     1.106410e-33  1.106345e-33  \n",
       "...            ...           ...  \n",
       "3883  1.106410e-33  1.106345e-33  \n",
       "3884  1.106410e-33  1.106345e-33  \n",
       "3885  1.106410e-33  1.106345e-33  \n",
       "3886  1.106410e-33  1.106345e-33  \n",
       "3887  1.106410e-33  1.106345e-33  \n",
       "\n",
       "[3888 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2일 뒤를 예측하는 모델\n",
    "quantiles = [.1, .2, .3, .4, .5, .6, .7, .8, .9]\n",
    "pred_2 = pd.DataFrame()\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "for n_fold, (train_idx, val_idx) in enumerate(folds.split(x_train_2)):\n",
    "    train_x, train_y = x_train_2.loc[train_idx], y_train_2.loc[train_idx]\n",
    "    val_x, val_y = x_train_2.loc[val_idx], y_train_2.loc[val_idx]\n",
    "    print(n_fold)\n",
    "    for i in quantiles:\n",
    "        print(i)\n",
    "        model = LGBMRegressor(objective = 'quantile', boosting = 'gbdt', n_estimators = 10000, learning_rate = 0.009,\n",
    "                              colsample_bytree= 0.8, seed = 2020, max_depth = 3, num_leaves = 24, alpha = i)\n",
    "        model.fit(train_x, train_y, eval_metric = ['quantile'], eval_set = [(train_x, train_y), (val_x, val_y)], early_stopping_rounds = 200, verbose = 500)\n",
    "        pred_2 = pd.concat([pred_2, pd.DataFrame(model.predict(x_test_2))], axis = 1)\n",
    "\n",
    "\n",
    "result_1 = pred_2.iloc[:,0:9]\n",
    "result_2 = pred_2.iloc[:,9:18]\n",
    "result_3 = pred_2.iloc[:,18:27]\n",
    "result_4 = pred_2.iloc[:,27:36]\n",
    "result_5 = pred_2.iloc[:,36:45]\n",
    "\n",
    "result = (result_1+result_2+result_3+result_4+result_5)/5\n",
    "result.columns = sub.iloc[:,1:].columns\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[sub.id.str.contains(\"Day8\"), \"q_0.1\":] = result.sort_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.520323</td>\n",
       "      <td>5.935774</td>\n",
       "      <td>7.679318</td>\n",
       "      <td>8.906169</td>\n",
       "      <td>9.834737</td>\n",
       "      <td>10.515792</td>\n",
       "      <td>11.014401</td>\n",
       "      <td>11.521004</td>\n",
       "      <td>12.122356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.078073</td>\n",
       "      <td>13.176904</td>\n",
       "      <td>16.618778</td>\n",
       "      <td>18.997819</td>\n",
       "      <td>20.810194</td>\n",
       "      <td>22.092160</td>\n",
       "      <td>22.958609</td>\n",
       "      <td>23.753857</td>\n",
       "      <td>24.663374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.152752</td>\n",
       "      <td>-1.472993</td>\n",
       "      <td>-1.504698</td>\n",
       "      <td>-2.698070</td>\n",
       "      <td>-2.881432</td>\n",
       "      <td>-4.116466</td>\n",
       "      <td>-5.282867</td>\n",
       "      <td>-9.133564</td>\n",
       "      <td>-14.783810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288641</td>\n",
       "      <td>0.430270</td>\n",
       "      <td>0.790429</td>\n",
       "      <td>1.063804</td>\n",
       "      <td>1.104885</td>\n",
       "      <td>1.132686</td>\n",
       "      <td>1.778964</td>\n",
       "      <td>3.232109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.703840</td>\n",
       "      <td>80.340962</td>\n",
       "      <td>93.864735</td>\n",
       "      <td>91.930091</td>\n",
       "      <td>97.047051</td>\n",
       "      <td>96.077966</td>\n",
       "      <td>96.980737</td>\n",
       "      <td>96.441509</td>\n",
       "      <td>97.921550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             q_0.1        q_0.2        q_0.3        q_0.4        q_0.5  \\\n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  7776.000000   \n",
       "mean      3.520323     5.935774     7.679318     8.906169     9.834737   \n",
       "std       8.078073    13.176904    16.618778    18.997819    20.810194   \n",
       "min      -2.152752    -1.472993    -1.504698    -2.698070    -2.881432   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.288641     0.430270     0.790429     1.063804   \n",
       "max      47.703840    80.340962    93.864735    91.930091    97.047051   \n",
       "\n",
       "             q_0.6        q_0.7        q_0.8        q_0.9  \n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  \n",
       "mean     10.515792    11.014401    11.521004    12.122356  \n",
       "std      22.092160    22.958609    23.753857    24.663374  \n",
       "min      -4.116466    -5.282867    -9.133564   -14.783810  \n",
       "25%       0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000  \n",
       "75%       1.104885     1.132686     1.778964     3.232109  \n",
       "max      96.077966    96.980737    96.441509    97.921550  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-9f77b97f5850>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.1'][i] = 0\n",
      "<ipython-input-33-9f77b97f5850>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.2'][i] = 0\n",
      "<ipython-input-33-9f77b97f5850>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.3'][i] = 0\n",
      "<ipython-input-33-9f77b97f5850>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.4'][i] = 0\n",
      "<ipython-input-33-9f77b97f5850>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.5'][i] = 0\n",
      "<ipython-input-33-9f77b97f5850>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.6'][i] = 0\n",
      "<ipython-input-33-9f77b97f5850>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.7'][i] = 0\n",
      "<ipython-input-33-9f77b97f5850>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.8'][i] = 0\n",
      "<ipython-input-33-9f77b97f5850>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred['q_0.9'][i] = 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>80.csv_Day8_21h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>80.csv_Day8_22h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7773</th>\n",
       "      <td>80.csv_Day8_22h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7774</th>\n",
       "      <td>80.csv_Day8_23h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7775</th>\n",
       "      <td>80.csv_Day8_23h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105373e-33</td>\n",
       "      <td>2.900734e-25</td>\n",
       "      <td>1.106410e-33</td>\n",
       "      <td>1.106345e-33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7776 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  q_0.1  q_0.2  q_0.3  q_0.4  q_0.5         q_0.6  \\\n",
       "0       0.csv_Day7_0h00m    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "1       0.csv_Day7_0h30m    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "2       0.csv_Day7_1h00m    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "3       0.csv_Day7_1h30m    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "4       0.csv_Day7_2h00m    0.0    0.0    0.0    0.0    0.0  0.000000e+00   \n",
       "...                  ...    ...    ...    ...    ...    ...           ...   \n",
       "7771  80.csv_Day8_21h30m    0.0    0.0    0.0    0.0    0.0  1.105373e-33   \n",
       "7772  80.csv_Day8_22h00m    0.0    0.0    0.0    0.0    0.0  1.105373e-33   \n",
       "7773  80.csv_Day8_22h30m    0.0    0.0    0.0    0.0    0.0  1.105373e-33   \n",
       "7774  80.csv_Day8_23h00m    0.0    0.0    0.0    0.0    0.0  1.105373e-33   \n",
       "7775  80.csv_Day8_23h30m    0.0    0.0    0.0    0.0    0.0  1.105373e-33   \n",
       "\n",
       "             q_0.7         q_0.8         q_0.9  \n",
       "0     0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "1     0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "2     0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "3     0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "4     0.000000e+00  0.000000e+00  0.000000e+00  \n",
       "...            ...           ...           ...  \n",
       "7771  2.900734e-25  1.106410e-33  1.106345e-33  \n",
       "7772  2.900734e-25  1.106410e-33  1.106345e-33  \n",
       "7773  2.900734e-25  1.106410e-33  1.106345e-33  \n",
       "7774  2.900734e-25  1.106410e-33  1.106345e-33  \n",
       "7775  2.900734e-25  1.106410e-33  1.106345e-33  \n",
       "\n",
       "[7776 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in pred.index:\n",
    "    if pred['q_0.1'][i] < 0:\n",
    "        pred['q_0.1'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.2'][i] < 0:\n",
    "        pred['q_0.2'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.3'][i] < 0:\n",
    "        pred['q_0.3'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.4'][i] < 0:\n",
    "        pred['q_0.4'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.5'][i] < 0:\n",
    "        pred['q_0.5'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.6'][i] < 0:\n",
    "        pred['q_0.6'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.7'][i] < 0:\n",
    "        pred['q_0.7'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.8'][i] < 0:\n",
    "        pred['q_0.8'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.9'][i] < 0:\n",
    "        pred['q_0.9'][i] = 0\n",
    "    \n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.523633</td>\n",
       "      <td>5.937689</td>\n",
       "      <td>7.682659</td>\n",
       "      <td>8.908821</td>\n",
       "      <td>9.837480</td>\n",
       "      <td>10.522082</td>\n",
       "      <td>11.023492</td>\n",
       "      <td>11.540718</td>\n",
       "      <td>12.157866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.076470</td>\n",
       "      <td>13.175998</td>\n",
       "      <td>16.617134</td>\n",
       "      <td>18.996492</td>\n",
       "      <td>20.808817</td>\n",
       "      <td>22.088853</td>\n",
       "      <td>22.953691</td>\n",
       "      <td>23.741830</td>\n",
       "      <td>24.639522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288641</td>\n",
       "      <td>0.430270</td>\n",
       "      <td>0.790429</td>\n",
       "      <td>1.063804</td>\n",
       "      <td>1.104885</td>\n",
       "      <td>1.132686</td>\n",
       "      <td>1.778964</td>\n",
       "      <td>3.232109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.703840</td>\n",
       "      <td>80.340962</td>\n",
       "      <td>93.864735</td>\n",
       "      <td>91.930091</td>\n",
       "      <td>97.047051</td>\n",
       "      <td>96.077966</td>\n",
       "      <td>96.980737</td>\n",
       "      <td>96.441509</td>\n",
       "      <td>97.921550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             q_0.1        q_0.2        q_0.3        q_0.4        q_0.5  \\\n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  7776.000000   \n",
       "mean      3.523633     5.937689     7.682659     8.908821     9.837480   \n",
       "std       8.076470    13.175998    16.617134    18.996492    20.808817   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.288641     0.430270     0.790429     1.063804   \n",
       "max      47.703840    80.340962    93.864735    91.930091    97.047051   \n",
       "\n",
       "             q_0.6        q_0.7        q_0.8        q_0.9  \n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  \n",
       "mean     10.522082    11.023492    11.540718    12.157866  \n",
       "std      22.088853    22.953691    23.741830    24.639522  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000  \n",
       "75%       1.104885     1.132686     1.778964     3.232109  \n",
       "max      96.077966    96.980737    96.441509    97.921550  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgbm = pred.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import get_custom_objects\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def tilted_loss(q,y,f):\n",
    "    e = (y-f)\n",
    "    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)\n",
    "\n",
    "class Mish(Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "def mish(x):\n",
    "    return x * K.tanh(K.softplus(x))\n",
    "\n",
    "get_custom_objects().update({'mish': Mish(mish)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "x_train_1_scaler = scaler.fit_transform(x_train_1)\n",
    "x_test_1_scaler = scaler.transform(x_test_1)\n",
    "\n",
    "x_train_2_scaler = scaler.fit_transform(x_train_2)\n",
    "x_test_2_scaler = scaler.transform(x_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=7))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(14))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1_scaler = pd.DataFrame(x_train_1_scaler, columns = x_train_1.columns)\n",
    "x_train_2_scaler = pd.DataFrame(x_train_2_scaler, columns = x_train_2.columns)\n",
    "x_test_1_scaler = pd.DataFrame(x_test_1_scaler, columns = x_test_1.columns)\n",
    "x_test_2_scaler = pd.DataFrame(x_test_2_scaler, columns = x_test_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.1\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 4s 4ms/step - loss: 1.7612 - val_loss: 1.4443\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4650 - val_loss: 1.4325\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4576 - val_loss: 1.4318\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4325 - val_loss: 1.4233\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4523 - val_loss: 1.4102\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 1.4610 - val_loss: 1.4011\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4294 - val_loss: 1.4018\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4192 - val_loss: 1.3901\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4219 - val_loss: 1.3881\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4273 - val_loss: 1.3866\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4030 - val_loss: 1.3835\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4206 - val_loss: 1.3807\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4259 - val_loss: 1.3775\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4081 - val_loss: 1.3763\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4222 - val_loss: 1.3800\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3887 - val_loss: 1.3730\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4139 - val_loss: 1.3759\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4102 - val_loss: 1.3707\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3895 - val_loss: 1.3706\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3785 - val_loss: 1.3823\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4159 - val_loss: 1.3651\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3764 - val_loss: 1.3683\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4011 - val_loss: 1.3637\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3938 - val_loss: 1.3649\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3938 - val_loss: 1.3644\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4014 - val_loss: 1.3622\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3966 - val_loss: 1.3643\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3950 - val_loss: 1.3596\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3766 - val_loss: 1.3587\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3802 - val_loss: 1.3608\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3919 - val_loss: 1.3619\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3933 - val_loss: 1.3555\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3841 - val_loss: 1.3553\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4035 - val_loss: 1.3588\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3943 - val_loss: 1.3625\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3994 - val_loss: 1.3542\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3781 - val_loss: 1.3541\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3962 - val_loss: 1.3534\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3960 - val_loss: 1.3527\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3740 - val_loss: 1.3807\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4142 - val_loss: 1.3535\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4108 - val_loss: 1.3712\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3767 - val_loss: 1.3530\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3877 - val_loss: 1.3520\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4268 - val_loss: 1.3544\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3779 - val_loss: 1.3547\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3957 - val_loss: 1.3497\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3961 - val_loss: 1.3559\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3975 - val_loss: 1.3487\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3986 - val_loss: 1.3499\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3794 - val_loss: 1.3483\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3871 - val_loss: 1.3477\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4052 - val_loss: 1.3538\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3760 - val_loss: 1.3487\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3780 - val_loss: 1.3579\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3894 - val_loss: 1.3498\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3730 - val_loss: 1.3499\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3983 - val_loss: 1.3515\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3862 - val_loss: 1.3523\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3959 - val_loss: 1.3494\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3862 - val_loss: 1.3538\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3965 - val_loss: 1.3542\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3996 - val_loss: 1.3497\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3570 - val_loss: 1.3566\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3648 - val_loss: 1.3467\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3927 - val_loss: 1.3483\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3858 - val_loss: 1.3544\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3929 - val_loss: 1.3472\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3881 - val_loss: 1.3523\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3911 - val_loss: 1.3501\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3822 - val_loss: 1.3571\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3880 - val_loss: 1.3499\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3787 - val_loss: 1.3479\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3876 - val_loss: 1.3466\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3635 - val_loss: 1.3466\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3796 - val_loss: 1.3472\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3724 - val_loss: 1.3480\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3830 - val_loss: 1.3495\n",
      "Epoch 79/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3811 - val_loss: 1.3476\n",
      "Epoch 80/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4068 - val_loss: 1.3470\n",
      "Epoch 81/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3751 - val_loss: 1.3564\n",
      "Epoch 82/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3841 - val_loss: 1.3456\n",
      "Epoch 83/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3787 - val_loss: 1.3454\n",
      "Epoch 84/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3805 - val_loss: 1.3443\n",
      "Epoch 85/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3850 - val_loss: 1.3485\n",
      "Epoch 86/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3680 - val_loss: 1.3505\n",
      "Epoch 87/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3708 - val_loss: 1.3453\n",
      "Epoch 88/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3843 - val_loss: 1.3458\n",
      "Epoch 89/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3866 - val_loss: 1.3465\n",
      "Epoch 90/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3764 - val_loss: 1.3435\n",
      "Epoch 91/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3756 - val_loss: 1.3450\n",
      "Epoch 92/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3929 - val_loss: 1.3420\n",
      "Epoch 93/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3768 - val_loss: 1.3436\n",
      "Epoch 94/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3544 - val_loss: 1.3477\n",
      "Epoch 95/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3746 - val_loss: 1.3419\n",
      "Epoch 96/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3557 - val_loss: 1.3486\n",
      "Epoch 97/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3761 - val_loss: 1.3442\n",
      "Epoch 98/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3709 - val_loss: 1.3413\n",
      "Epoch 99/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3827 - val_loss: 1.3439\n",
      "Epoch 100/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3669 - val_loss: 1.3483\n",
      "Epoch 101/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3685 - val_loss: 1.3464\n",
      "Epoch 102/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3850 - val_loss: 1.3441\n",
      "Epoch 103/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3703 - val_loss: 1.3443\n",
      "Epoch 104/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3742 - val_loss: 1.3407\n",
      "Epoch 105/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3671 - val_loss: 1.3460\n",
      "Epoch 106/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3733 - val_loss: 1.3457\n",
      "Epoch 107/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3718 - val_loss: 1.3524\n",
      "Epoch 108/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3791 - val_loss: 1.3437\n",
      "Epoch 109/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3864 - val_loss: 1.3497\n",
      "Epoch 110/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3907 - val_loss: 1.3459\n",
      "Epoch 111/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3721 - val_loss: 1.3400\n",
      "Epoch 112/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3870 - val_loss: 1.3437\n",
      "Epoch 113/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3818 - val_loss: 1.3450\n",
      "Epoch 114/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3688 - val_loss: 1.3482\n",
      "Epoch 115/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3696 - val_loss: 1.3430\n",
      "Epoch 116/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3600 - val_loss: 1.3420\n",
      "Epoch 117/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3755 - val_loss: 1.3466\n",
      "Epoch 118/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3796 - val_loss: 1.3462\n",
      "Epoch 119/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4011 - val_loss: 1.3480\n",
      "Epoch 120/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3758 - val_loss: 1.3549\n",
      "Epoch 121/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3748 - val_loss: 1.3463\n",
      "Epoch 122/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3796 - val_loss: 1.3459\n",
      "Epoch 123/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3494 - val_loss: 1.3420\n",
      "Epoch 124/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3612 - val_loss: 1.3437\n",
      "Epoch 125/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3631 - val_loss: 1.3446\n",
      "Epoch 126/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3846 - val_loss: 1.3390\n",
      "Epoch 127/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3587 - val_loss: 1.3401\n",
      "Epoch 128/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3757 - val_loss: 1.3400\n",
      "Epoch 129/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3659 - val_loss: 1.3409\n",
      "Epoch 130/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3701 - val_loss: 1.3432\n",
      "Epoch 131/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3817 - val_loss: 1.3450\n",
      "Epoch 132/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3695 - val_loss: 1.3430\n",
      "Epoch 133/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3869 - val_loss: 1.3477\n",
      "Epoch 134/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3830 - val_loss: 1.3452\n",
      "Epoch 135/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3636 - val_loss: 1.3489\n",
      "Epoch 136/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3685 - val_loss: 1.3442\n",
      "Epoch 137/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3750 - val_loss: 1.3430\n",
      "Epoch 138/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3754 - val_loss: 1.3474\n",
      "Epoch 139/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3775 - val_loss: 1.3482\n",
      "Epoch 140/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3718 - val_loss: 1.3626\n",
      "Epoch 141/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3872 - val_loss: 1.3450\n",
      "Epoch 00141: early stopping\n",
      "0.2\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 3s 3ms/step - loss: 2.2557 - val_loss: 2.1972\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2027 - val_loss: 2.1935\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2533 - val_loss: 2.1987\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 3ms/step - loss: 2.2069 - val_loss: 2.1857\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2277 - val_loss: 2.2043\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2257 - val_loss: 2.1867\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2206 - val_loss: 2.1802\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2068 - val_loss: 2.1865\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1927 - val_loss: 2.1781\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1801 - val_loss: 2.1780\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2040 - val_loss: 2.1751\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1781 - val_loss: 2.1779\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1781 - val_loss: 2.1791\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2327 - val_loss: 2.1928\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1960 - val_loss: 2.1756\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2159 - val_loss: 2.1738\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1918 - val_loss: 2.1916\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2083 - val_loss: 2.1760\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1944 - val_loss: 2.1811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2059 - val_loss: 2.1675\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1858 - val_loss: 2.1775\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1522 - val_loss: 2.1752\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1987 - val_loss: 2.1699\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1818 - val_loss: 2.1810\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2250 - val_loss: 2.1718\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1720 - val_loss: 2.1702\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1617 - val_loss: 2.1699\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1733 - val_loss: 2.1751\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1924 - val_loss: 2.1703\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2383 - val_loss: 2.1899\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1951 - val_loss: 2.1849\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1707 - val_loss: 2.1735\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1812 - val_loss: 2.1831\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2149 - val_loss: 2.1715\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2087 - val_loss: 2.1844\n",
      "Epoch 00035: early stopping\n",
      "0.3\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 4s 5ms/step - loss: 2.6106 - val_loss: 2.6210\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6588 - val_loss: 2.6218\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5983 - val_loss: 2.6167\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6579 - val_loss: 2.6258\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6535 - val_loss: 2.6339\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6168 - val_loss: 2.6208\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6520 - val_loss: 2.6140\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6300 - val_loss: 2.6136\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5887 - val_loss: 2.6080\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5936 - val_loss: 2.6158\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6447 - val_loss: 2.6034\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5731 - val_loss: 2.6110\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6593 - val_loss: 2.6070\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6277 - val_loss: 2.6048\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5966 - val_loss: 2.6040\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6046 - val_loss: 2.6218\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6257 - val_loss: 2.6181\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6215 - val_loss: 2.6087\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6220 - val_loss: 2.5999\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6029 - val_loss: 2.5995\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6230 - val_loss: 2.6267\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6319 - val_loss: 2.6120\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6013 - val_loss: 2.6131\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6090 - val_loss: 2.6010\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6514 - val_loss: 2.6095\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6308 - val_loss: 2.6039\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6451 - val_loss: 2.6043\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5928 - val_loss: 2.6286\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6298 - val_loss: 2.6045\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5890 - val_loss: 2.6081\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6620 - val_loss: 2.6060\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5980 - val_loss: 2.6033\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5913 - val_loss: 2.6126\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6098 - val_loss: 2.6043\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5812 - val_loss: 2.6064\n",
      "Epoch 00035: early stopping\n",
      "0.4\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 4ms/step - loss: 2.8025 - val_loss: 2.7396\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7550 - val_loss: 2.7344\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7330 - val_loss: 2.7621\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6847 - val_loss: 2.7397\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7528 - val_loss: 2.7352\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7521 - val_loss: 2.7596\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7369 - val_loss: 2.7391\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7225 - val_loss: 2.7338\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7563 - val_loss: 2.7453\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7723 - val_loss: 2.7393\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7602 - val_loss: 2.7368\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7114 - val_loss: 2.7251\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7490 - val_loss: 2.7368\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7156 - val_loss: 2.7268\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - ETA: 0s - loss: 2.694 - 1s 2ms/step - loss: 2.6949 - val_loss: 2.7327\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7181 - val_loss: 2.7222\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7189 - val_loss: 2.7234\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7108 - val_loss: 2.7235\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6965 - val_loss: 2.7251\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7330 - val_loss: 2.7233\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7179 - val_loss: 2.7281\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7377 - val_loss: 2.7247\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7477 - val_loss: 2.7239\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7511 - val_loss: 2.7267\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7362 - val_loss: 2.7319\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6908 - val_loss: 2.7180\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7171 - val_loss: 2.7422\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7176 - val_loss: 2.7300\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7503 - val_loss: 2.7484\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7489 - val_loss: 2.7296\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7303 - val_loss: 2.7204\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7356 - val_loss: 2.7214\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7300 - val_loss: 2.7166\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7435 - val_loss: 2.7196\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7064 - val_loss: 2.7196\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6810 - val_loss: 2.7183\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7240 - val_loss: 2.7206\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7183 - val_loss: 2.7190\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7084 - val_loss: 2.7176\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7306 - val_loss: 2.7206\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7227 - val_loss: 2.7318\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7317 - val_loss: 2.7130\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7226 - val_loss: 2.7249\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7293 - val_loss: 2.7217\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7606 - val_loss: 2.7454\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6947 - val_loss: 2.7280\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7045 - val_loss: 2.7209\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7192 - val_loss: 2.7139\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7472 - val_loss: 2.7321\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7639 - val_loss: 2.7105\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7072 - val_loss: 2.7268\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7216 - val_loss: 2.7137\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6452 - val_loss: 2.7197\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7448 - val_loss: 2.7119\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6854 - val_loss: 2.7295\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7325 - val_loss: 2.7080\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7354 - val_loss: 2.7164\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7014 - val_loss: 2.7113\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7276 - val_loss: 2.7204\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7367 - val_loss: 2.7194\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7168 - val_loss: 2.7118\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7020 - val_loss: 2.7086\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7006 - val_loss: 2.7039\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7122 - val_loss: 2.7102\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6867 - val_loss: 2.7156\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6846 - val_loss: 2.7087\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7012 - val_loss: 2.7084\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7267 - val_loss: 2.7235\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7405 - val_loss: 2.7079\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6934 - val_loss: 2.7186\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7370 - val_loss: 2.7178\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7522 - val_loss: 2.7069\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7421 - val_loss: 2.7047\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7213 - val_loss: 2.7325\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7131 - val_loss: 2.7060\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7195 - val_loss: 2.7126\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7200 - val_loss: 2.7095\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6888 - val_loss: 2.7069\n",
      "Epoch 00078: early stopping\n",
      "0.5\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 4s 4ms/step - loss: 2.6216 - val_loss: 2.6342\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6525 - val_loss: 2.6154\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6860 - val_loss: 2.6120\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6773 - val_loss: 2.6234\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6197 - val_loss: 2.6226\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6048 - val_loss: 2.6232\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6122 - val_loss: 2.6205\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6231 - val_loss: 2.6167\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6366 - val_loss: 2.6167\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6521 - val_loss: 2.6310\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6147 - val_loss: 2.6157\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6107 - val_loss: 2.6226\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6195 - val_loss: 2.6253\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6441 - val_loss: 2.6134\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5848 - val_loss: 2.6089\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6122 - val_loss: 2.6139\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6104 - val_loss: 2.6086\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5986 - val_loss: 2.6034\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6189 - val_loss: 2.6073\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5848 - val_loss: 2.6074\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6299 - val_loss: 2.6076\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5797 - val_loss: 2.6072\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6250 - val_loss: 2.6021\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5836 - val_loss: 2.6023\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6378 - val_loss: 2.6180\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6096 - val_loss: 2.6035\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5925 - val_loss: 2.6119\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6164 - val_loss: 2.6396\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6341 - val_loss: 2.6061\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6116 - val_loss: 2.6023\n",
      "Epoch 31/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5820 - val_loss: 2.6029\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6271 - val_loss: 2.6154\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5855 - val_loss: 2.6136\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6074 - val_loss: 2.6156\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6037 - val_loss: 2.6138\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5911 - val_loss: 2.6132\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6258 - val_loss: 2.6191\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6353 - val_loss: 2.6020\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5964 - val_loss: 2.6168\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6043 - val_loss: 2.6113\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6375 - val_loss: 2.6106\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6443 - val_loss: 2.6086\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5764 - val_loss: 2.6074\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6369 - val_loss: 2.5990\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6015 - val_loss: 2.6044\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6194 - val_loss: 2.5972\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6072 - val_loss: 2.6051\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6250 - val_loss: 2.5983\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6278 - val_loss: 2.6014\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6352 - val_loss: 2.6102\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6430 - val_loss: 2.6070\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5785 - val_loss: 2.6388\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5970 - val_loss: 2.5988\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - ETA: 0s - loss: 2.589 - 1s 2ms/step - loss: 2.5891 - val_loss: 2.6023\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6340 - val_loss: 2.6258\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5966 - val_loss: 2.5978\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5897 - val_loss: 2.5974\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5589 - val_loss: 2.6025\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5901 - val_loss: 2.6040\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6192 - val_loss: 2.5961\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6290 - val_loss: 2.6011\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5895 - val_loss: 2.6048\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6328 - val_loss: 2.5989\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5765 - val_loss: 2.6029\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5673 - val_loss: 2.6062\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5749 - val_loss: 2.5958\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6090 - val_loss: 2.5983\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6242 - val_loss: 2.5960\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5650 - val_loss: 2.5956\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6262 - val_loss: 2.5981\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6141 - val_loss: 2.6043\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5993 - val_loss: 2.5935\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5900 - val_loss: 2.6046\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6396 - val_loss: 2.5964\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5732 - val_loss: 2.5943\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6380 - val_loss: 2.6067\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5808 - val_loss: 2.6329\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6047 - val_loss: 2.5981\n",
      "Epoch 79/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6163 - val_loss: 2.6040\n",
      "Epoch 80/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5904 - val_loss: 2.5992\n",
      "Epoch 81/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5637 - val_loss: 2.6013\n",
      "Epoch 82/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6067 - val_loss: 2.5969\n",
      "Epoch 83/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5626 - val_loss: 2.6052\n",
      "Epoch 84/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5975 - val_loss: 2.5941\n",
      "Epoch 85/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6382 - val_loss: 2.5949\n",
      "Epoch 86/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6032 - val_loss: 2.5985\n",
      "Epoch 87/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6188 - val_loss: 2.5996\n",
      "Epoch 00087: early stopping\n",
      "0.6\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 3s 3ms/step - loss: 2.3729 - val_loss: 2.3495\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3353 - val_loss: 2.3563\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3579 - val_loss: 2.3369\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3508 - val_loss: 2.3363\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3485 - val_loss: 2.3350\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3469 - val_loss: 2.3399\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2942 - val_loss: 2.3313\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3592 - val_loss: 2.3272\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3165 - val_loss: 2.3341\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3272 - val_loss: 2.3299\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3143 - val_loss: 2.3228\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3480 - val_loss: 2.3264\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3425 - val_loss: 2.3259\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3411 - val_loss: 2.3286\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3187 - val_loss: 2.3504\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3393 - val_loss: 2.3291\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3440 - val_loss: 2.3227\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3451 - val_loss: 2.3200\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3420 - val_loss: 2.3264\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3283 - val_loss: 2.3290\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3329 - val_loss: 2.3223\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3100 - val_loss: 2.3339\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2942 - val_loss: 2.3276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2655 - val_loss: 2.3179\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3667 - val_loss: 2.3261\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3251 - val_loss: 2.3151\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2978 - val_loss: 2.3374\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2978 - val_loss: 2.3229\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3346 - val_loss: 2.3178\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3422 - val_loss: 2.3408\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2616 - val_loss: 2.3161\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2906 - val_loss: 2.3212\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3046 - val_loss: 2.3303\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3238 - val_loss: 2.3401\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3236 - val_loss: 2.3329\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3231 - val_loss: 2.3274\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3490 - val_loss: 2.3189\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3546 - val_loss: 2.3292\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3395 - val_loss: 2.3216\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3337 - val_loss: 2.3283\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3283 - val_loss: 2.3212\n",
      "Epoch 00041: early stopping\n",
      "0.7\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 3s 3ms/step - loss: 1.9250 - val_loss: 1.9334\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9761 - val_loss: 1.9353\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9418 - val_loss: 1.9202\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9813 - val_loss: 1.9215\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9065 - val_loss: 1.9224\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9265 - val_loss: 1.9207\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9393 - val_loss: 1.9293\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9823 - val_loss: 1.9180\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9311 - val_loss: 1.9151\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9397 - val_loss: 1.9071\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9163 - val_loss: 1.9166\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9515 - val_loss: 1.9322\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9157 - val_loss: 1.9054\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9136 - val_loss: 1.9146\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9101 - val_loss: 1.9060\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9148 - val_loss: 1.9064\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9456 - val_loss: 1.9054\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9413 - val_loss: 1.9083\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9126 - val_loss: 1.9085\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9499 - val_loss: 1.9025\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9376 - val_loss: 1.9268\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8936 - val_loss: 1.9030\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9243 - val_loss: 1.9056\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9296 - val_loss: 1.9092\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9215 - val_loss: 1.9053\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9200 - val_loss: 1.9122\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9411 - val_loss: 1.9103\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9063 - val_loss: 1.9043\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9398 - val_loss: 1.9038\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9280 - val_loss: 1.9008\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9105 - val_loss: 1.9042\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9394 - val_loss: 1.9126\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9327 - val_loss: 1.9162\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9361 - val_loss: 1.9149\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9460 - val_loss: 1.9002\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9311 - val_loss: 1.9021\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9012 - val_loss: 1.9005\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9297 - val_loss: 1.8958\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9381 - val_loss: 1.8977\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9329 - val_loss: 1.8991\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8968 - val_loss: 1.9020\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9220 - val_loss: 1.8953\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8980 - val_loss: 1.9035\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9182 - val_loss: 1.9043\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9089 - val_loss: 1.8978\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9195 - val_loss: 1.8952\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9188 - val_loss: 1.9086\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9310 - val_loss: 1.9085\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9137 - val_loss: 1.9013\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9147 - val_loss: 1.9211\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9109 - val_loss: 1.9248\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9283 - val_loss: 1.8991\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9453 - val_loss: 1.8948\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9478 - val_loss: 1.8984\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8840 - val_loss: 1.9007\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9061 - val_loss: 1.8935\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9429 - val_loss: 1.8988\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9271 - val_loss: 1.8950\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9111 - val_loss: 1.8999\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9287 - val_loss: 1.8954\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9287 - val_loss: 1.8974\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9203 - val_loss: 1.9146\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9089 - val_loss: 1.8959\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9215 - val_loss: 1.8991\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9042 - val_loss: 1.9159\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9231 - val_loss: 1.9077\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9318 - val_loss: 1.8915\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9147 - val_loss: 1.9058\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9543 - val_loss: 1.9087\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9424 - val_loss: 1.8977\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9291 - val_loss: 1.9053\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9207 - val_loss: 1.8961\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8923 - val_loss: 1.9149\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9009 - val_loss: 1.8966\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9184 - val_loss: 1.8974\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8979 - val_loss: 1.8943\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9139 - val_loss: 1.8958\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8851 - val_loss: 1.9192\n",
      "Epoch 79/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9294 - val_loss: 1.9191\n",
      "Epoch 80/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9510 - val_loss: 1.8947\n",
      "Epoch 81/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9192 - val_loss: 1.8994\n",
      "Epoch 82/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9076 - val_loss: 1.8908\n",
      "Epoch 83/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9244 - val_loss: 1.9123\n",
      "Epoch 84/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9099 - val_loss: 1.8946\n",
      "Epoch 85/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9056 - val_loss: 1.9180\n",
      "Epoch 86/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9074 - val_loss: 1.8943\n",
      "Epoch 87/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9309 - val_loss: 1.8992\n",
      "Epoch 88/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9278 - val_loss: 1.8960\n",
      "Epoch 89/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9328 - val_loss: 1.8942\n",
      "Epoch 90/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9356 - val_loss: 1.9121\n",
      "Epoch 91/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8950 - val_loss: 1.9041\n",
      "Epoch 92/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9131 - val_loss: 1.9082\n",
      "Epoch 93/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9341 - val_loss: 1.8973\n",
      "Epoch 94/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9001 - val_loss: 1.9016\n",
      "Epoch 95/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9026 - val_loss: 1.8976\n",
      "Epoch 96/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9033 - val_loss: 1.8876\n",
      "Epoch 97/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9427 - val_loss: 1.8922\n",
      "Epoch 98/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9185 - val_loss: 1.8963\n",
      "Epoch 99/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8997 - val_loss: 1.8971\n",
      "Epoch 100/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8987 - val_loss: 1.8962\n",
      "Epoch 101/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8983 - val_loss: 1.9401\n",
      "Epoch 102/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8623 - val_loss: 1.8935\n",
      "Epoch 103/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9106 - val_loss: 1.8926\n",
      "Epoch 104/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9031 - val_loss: 1.8975\n",
      "Epoch 105/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8700 - val_loss: 1.9116\n",
      "Epoch 106/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9261 - val_loss: 1.8945\n",
      "Epoch 107/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9163 - val_loss: 1.9033\n",
      "Epoch 108/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9322 - val_loss: 1.8905\n",
      "Epoch 109/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8853 - val_loss: 1.9008\n",
      "Epoch 110/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9264 - val_loss: 1.8934\n",
      "Epoch 111/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8725 - val_loss: 1.8908\n",
      "Epoch 00111: early stopping\n",
      "0.8\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 1.4325 - val_loss: 1.3937\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4118 - val_loss: 1.3918\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4332 - val_loss: 1.3883\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4319 - val_loss: 1.3876\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4356 - val_loss: 1.3972\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3799 - val_loss: 1.3938\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4334 - val_loss: 1.3869\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4137 - val_loss: 1.3878\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3956 - val_loss: 1.3867\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4362 - val_loss: 1.3870\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4215 - val_loss: 1.3925\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4053 - val_loss: 1.3911\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4183 - val_loss: 1.3856\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4053 - val_loss: 1.3906\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4267 - val_loss: 1.4040\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4041 - val_loss: 1.3842\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4025 - val_loss: 1.3797\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4007 - val_loss: 1.3963\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4115 - val_loss: 1.4099\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3969 - val_loss: 1.3898\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4205 - val_loss: 1.3959\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4129 - val_loss: 1.4309\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4328 - val_loss: 1.3840\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4108 - val_loss: 1.4078\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4051 - val_loss: 1.3966\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4226 - val_loss: 1.3865\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3829 - val_loss: 1.3977\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4114 - val_loss: 1.4035\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4147 - val_loss: 1.3904\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4017 - val_loss: 1.3901\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4046 - val_loss: 1.3812\n",
      "Epoch 32/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4109 - val_loss: 1.3902\n",
      "Epoch 00032: early stopping\n",
      "0.9\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 3ms/step - loss: 0.8070 - val_loss: 0.7893\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.8031 - val_loss: 0.7778\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7817 - val_loss: 0.7830\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.8027 - val_loss: 0.7877\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.8040 - val_loss: 0.7791\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.8014 - val_loss: 0.7835\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7959 - val_loss: 0.7925\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7990 - val_loss: 0.7764\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.8025 - val_loss: 0.7737\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7986 - val_loss: 0.7720\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7833 - val_loss: 0.7719\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7833 - val_loss: 0.7771\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7966 - val_loss: 0.7685\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7958 - val_loss: 0.7688\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7797 - val_loss: 0.7968\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7839 - val_loss: 0.7678\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7885 - val_loss: 0.7686\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7832 - val_loss: 0.7751\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7859 - val_loss: 0.7737\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7878 - val_loss: 0.7701\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7779 - val_loss: 0.7672\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7850 - val_loss: 0.7737\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7722 - val_loss: 0.7637\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7756 - val_loss: 0.7729\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7820 - val_loss: 0.7667\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7955 - val_loss: 0.7658\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7864 - val_loss: 0.7829\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7807 - val_loss: 0.7750\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7861 - val_loss: 0.7897\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7844 - val_loss: 0.7669\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7707 - val_loss: 0.7677\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7714 - val_loss: 0.7651\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7888 - val_loss: 0.7672\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7688 - val_loss: 0.7766\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7741 - val_loss: 0.7689\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7849 - val_loss: 0.7658\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7877 - val_loss: 0.7683\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7797 - val_loss: 0.7733\n",
      "Epoch 00038: early stopping\n",
      "1\n",
      "0.1\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.2397 - val_loss: 1.4229\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4423 - val_loss: 1.4101\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4310 - val_loss: 1.4052\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4239 - val_loss: 1.3965\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4239 - val_loss: 1.3910\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4225 - val_loss: 1.3976\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4217 - val_loss: 1.3825\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4350 - val_loss: 1.3833\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4092 - val_loss: 1.3815\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3950 - val_loss: 1.3817\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4000 - val_loss: 1.3789\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4144 - val_loss: 1.3797\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3770 - val_loss: 1.3818\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3968 - val_loss: 1.3770\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4045 - val_loss: 1.3782\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3644 - val_loss: 1.3738\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3940 - val_loss: 1.3755\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3603 - val_loss: 1.3740\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3825 - val_loss: 1.3744\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4091 - val_loss: 1.3760\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3943 - val_loss: 1.3773\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4000 - val_loss: 1.3766\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3619 - val_loss: 1.3713\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3935 - val_loss: 1.3731\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3843 - val_loss: 1.3734\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3730 - val_loss: 1.3718\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3702 - val_loss: 1.3749\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3909 - val_loss: 1.3866\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4107 - val_loss: 1.3738\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3737 - val_loss: 1.3745\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3943 - val_loss: 1.3721\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3672 - val_loss: 1.3720\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3742 - val_loss: 1.3733\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3701 - val_loss: 1.3712\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3636 - val_loss: 1.3713\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3886 - val_loss: 1.3748\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3840 - val_loss: 1.3718\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3625 - val_loss: 1.3678\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3621 - val_loss: 1.3724\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3587 - val_loss: 1.3713\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3961 - val_loss: 1.3730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3743 - val_loss: 1.3738\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3756 - val_loss: 1.3691\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3724 - val_loss: 1.3757\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3829 - val_loss: 1.3700\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3787 - val_loss: 1.3669\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3684 - val_loss: 1.3770\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3821 - val_loss: 1.3713\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3853 - val_loss: 1.3718\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3695 - val_loss: 1.3752\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3535 - val_loss: 1.3692\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3710 - val_loss: 1.3725\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3852 - val_loss: 1.3663\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3676 - val_loss: 1.3668\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3869 - val_loss: 1.3683\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3617 - val_loss: 1.3812\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3690 - val_loss: 1.3703\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3813 - val_loss: 1.3745\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3767 - val_loss: 1.3675\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3596 - val_loss: 1.3708\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3766 - val_loss: 1.3683\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3806 - val_loss: 1.3708\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3863 - val_loss: 1.3718\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3539 - val_loss: 1.3639\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3798 - val_loss: 1.3745\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3809 - val_loss: 1.3668\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3495 - val_loss: 1.3720\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3718 - val_loss: 1.3778\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3752 - val_loss: 1.3767\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3741 - val_loss: 1.3662\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3619 - val_loss: 1.3668\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3627 - val_loss: 1.3689\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3583 - val_loss: 1.3644\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3697 - val_loss: 1.3639\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3660 - val_loss: 1.3719\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3660 - val_loss: 1.3655\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3692 - val_loss: 1.3707\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3539 - val_loss: 1.3723\n",
      "Epoch 79/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3722 - val_loss: 1.3642\n",
      "Epoch 80/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3744 - val_loss: 1.3635\n",
      "Epoch 81/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3520 - val_loss: 1.3712\n",
      "Epoch 82/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3564 - val_loss: 1.3661\n",
      "Epoch 83/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3579 - val_loss: 1.3668\n",
      "Epoch 84/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3774 - val_loss: 1.3625\n",
      "Epoch 85/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3652 - val_loss: 1.3643\n",
      "Epoch 86/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3653 - val_loss: 1.3721\n",
      "Epoch 87/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3776 - val_loss: 1.3633\n",
      "Epoch 88/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3521 - val_loss: 1.3782\n",
      "Epoch 89/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3493 - val_loss: 1.3646\n",
      "Epoch 90/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3740 - val_loss: 1.3634\n",
      "Epoch 91/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3854 - val_loss: 1.3692\n",
      "Epoch 92/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3399 - val_loss: 1.3695\n",
      "Epoch 93/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3724 - val_loss: 1.3633\n",
      "Epoch 94/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3663 - val_loss: 1.3649\n",
      "Epoch 95/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3679 - val_loss: 1.3680\n",
      "Epoch 96/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3751 - val_loss: 1.3640\n",
      "Epoch 97/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3659 - val_loss: 1.3692\n",
      "Epoch 98/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3356 - val_loss: 1.3711\n",
      "Epoch 99/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3523 - val_loss: 1.3768\n",
      "Epoch 00099: early stopping\n",
      "0.2\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 27s 81ms/step - loss: 2.2564 - val_loss: 2.1938\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1914 - val_loss: 2.1857\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1699 - val_loss: 2.1841\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2143 - val_loss: 2.1873\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2052 - val_loss: 2.1825\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2248 - val_loss: 2.1752\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1633 - val_loss: 2.1845\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2298 - val_loss: 2.1826\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1705 - val_loss: 2.1957\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2198 - val_loss: 2.1818\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2290 - val_loss: 2.1919\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1991 - val_loss: 2.1895\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2027 - val_loss: 2.1879\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1968 - val_loss: 2.1918\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1700 - val_loss: 2.1898\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1939 - val_loss: 2.2037\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2200 - val_loss: 2.1798\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1921 - val_loss: 2.1739\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2169 - val_loss: 2.1938\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1860 - val_loss: 2.1809\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2210 - val_loss: 2.1915\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1509 - val_loss: 2.1905\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1975 - val_loss: 2.1837\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1806 - val_loss: 2.1857\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1942 - val_loss: 2.1809\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1948 - val_loss: 2.1951\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2073 - val_loss: 2.1749\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2209 - val_loss: 2.1814\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1826 - val_loss: 2.1797\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1834 - val_loss: 2.1961\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1863 - val_loss: 2.1923\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2192 - val_loss: 2.1770\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1944 - val_loss: 2.1792\n",
      "Epoch 00033: early stopping\n",
      "0.3\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.6132 - val_loss: 2.6219\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6692 - val_loss: 2.5909\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6285 - val_loss: 2.5944\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6403 - val_loss: 2.6061\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6297 - val_loss: 2.5991\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5878 - val_loss: 2.6032\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6386 - val_loss: 2.6007\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5586 - val_loss: 2.6040\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6391 - val_loss: 2.5910\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6165 - val_loss: 2.5900\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6416 - val_loss: 2.6069\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6423 - val_loss: 2.5886\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6071 - val_loss: 2.6027\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6176 - val_loss: 2.6023\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6262 - val_loss: 2.5886\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5892 - val_loss: 2.5896\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6365 - val_loss: 2.5855\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6453 - val_loss: 2.5928\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5964 - val_loss: 2.5925\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6521 - val_loss: 2.6004\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6031 - val_loss: 2.6057\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6074 - val_loss: 2.6025\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5788 - val_loss: 2.5901\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6075 - val_loss: 2.5834\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6209 - val_loss: 2.5873\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6326 - val_loss: 2.5808\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6111 - val_loss: 2.5912\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5867 - val_loss: 2.5916\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6069 - val_loss: 2.5896\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5978 - val_loss: 2.6019\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6173 - val_loss: 2.5855\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5820 - val_loss: 2.5981\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6111 - val_loss: 2.5880\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6074 - val_loss: 2.5847\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5969 - val_loss: 2.5828\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5863 - val_loss: 2.5811\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6265 - val_loss: 2.5906\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5510 - val_loss: 2.5891\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6151 - val_loss: 2.5973\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5896 - val_loss: 2.5850\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6275 - val_loss: 2.5994\n",
      "Epoch 00041: early stopping\n",
      "0.4\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.7446 - val_loss: 2.7130\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7161 - val_loss: 2.7147\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7040 - val_loss: 2.7138\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7291 - val_loss: 2.7113\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7356 - val_loss: 2.7258\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6942 - val_loss: 2.7067\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6872 - val_loss: 2.7150\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7152 - val_loss: 2.7081\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6967 - val_loss: 2.7095\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7372 - val_loss: 2.7080\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7379 - val_loss: 2.7044\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7124 - val_loss: 2.7047\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6976 - val_loss: 2.7138\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7246 - val_loss: 2.7117\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7181 - val_loss: 2.7075\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6870 - val_loss: 2.7055\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7273 - val_loss: 2.7056\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7031 - val_loss: 2.7006\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7028 - val_loss: 2.7086\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7267 - val_loss: 2.7183\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7473 - val_loss: 2.7238\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7150 - val_loss: 2.7071\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6341 - val_loss: 2.7016\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6630 - val_loss: 2.7060\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7474 - val_loss: 2.7166\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7274 - val_loss: 2.7008\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6855 - val_loss: 2.7017\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6724 - val_loss: 2.7091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7497 - val_loss: 2.7059\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7472 - val_loss: 2.6987\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7040 - val_loss: 2.7442\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6857 - val_loss: 2.7000\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7097 - val_loss: 2.7077\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6831 - val_loss: 2.7185\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7008 - val_loss: 2.7016\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7444 - val_loss: 2.6955\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6960 - val_loss: 2.7078\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7541 - val_loss: 2.7023\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6715 - val_loss: 2.7043\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7170 - val_loss: 2.7047\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7281 - val_loss: 2.7001\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7465 - val_loss: 2.7076\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7346 - val_loss: 2.6896\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7285 - val_loss: 2.6943\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7436 - val_loss: 2.7026\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7289 - val_loss: 2.6953\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7184 - val_loss: 2.6967\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7051 - val_loss: 2.7015\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6801 - val_loss: 2.7137\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7210 - val_loss: 2.7158\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7378 - val_loss: 2.6952\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7262 - val_loss: 2.7099\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7226 - val_loss: 2.6936\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7049 - val_loss: 2.6932\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7007 - val_loss: 2.7050\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7088 - val_loss: 2.7032\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7071 - val_loss: 2.6984\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6788 - val_loss: 2.7008\n",
      "Epoch 00058: early stopping\n",
      "0.5\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.5918 - val_loss: 2.5975\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6154 - val_loss: 2.6195\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6143 - val_loss: 2.5946\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6163 - val_loss: 2.6070\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5732 - val_loss: 2.5870\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5718 - val_loss: 2.5924\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6400 - val_loss: 2.5821\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5561 - val_loss: 2.5843\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5895 - val_loss: 2.5923\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6181 - val_loss: 2.5898\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6165 - val_loss: 2.5885\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5928 - val_loss: 2.5799\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6161 - val_loss: 2.5749\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6310 - val_loss: 2.6052\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5879 - val_loss: 2.5928\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5893 - val_loss: 2.5987\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5665 - val_loss: 2.5769\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6277 - val_loss: 2.5792\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5604 - val_loss: 2.5922\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6292 - val_loss: 2.5912\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5933 - val_loss: 2.5895\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6519 - val_loss: 2.6026\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6031 - val_loss: 2.5910\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5746 - val_loss: 2.5769\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6121 - val_loss: 2.5773\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5998 - val_loss: 2.5931\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5878 - val_loss: 2.5735\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6095 - val_loss: 2.5803\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5575 - val_loss: 2.5825\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5763 - val_loss: 2.5773\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5884 - val_loss: 2.5918\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6005 - val_loss: 2.5821\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5965 - val_loss: 2.6094\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5818 - val_loss: 2.5772\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5587 - val_loss: 2.5842\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5712 - val_loss: 2.5870\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5908 - val_loss: 2.5776\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5827 - val_loss: 2.5789\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5941 - val_loss: 2.5727\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6127 - val_loss: 2.5984\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5728 - val_loss: 2.5806\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6149 - val_loss: 2.5758\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6175 - val_loss: 2.5809\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5551 - val_loss: 2.5829\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5622 - val_loss: 2.5755\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5537 - val_loss: 2.5837\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5645 - val_loss: 2.5714\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5921 - val_loss: 2.5840\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6274 - val_loss: 2.5810\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6380 - val_loss: 2.5688\n",
      "Epoch 51/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5986 - val_loss: 2.5823\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6022 - val_loss: 2.5698\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5934 - val_loss: 2.5869\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6047 - val_loss: 2.5956\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5283 - val_loss: 2.5830\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5563 - val_loss: 2.5846\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5892 - val_loss: 2.5898\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5795 - val_loss: 2.5869\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5616 - val_loss: 2.5768\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5470 - val_loss: 2.5881\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6477 - val_loss: 2.5684\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5802 - val_loss: 2.5705\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5371 - val_loss: 2.5830\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6079 - val_loss: 2.5686\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5781 - val_loss: 2.5974\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5688 - val_loss: 2.5790\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5704 - val_loss: 2.5963\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6113 - val_loss: 2.5717\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5570 - val_loss: 2.5818\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6011 - val_loss: 2.5741\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6015 - val_loss: 2.5762\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6027 - val_loss: 2.5806\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6125 - val_loss: 2.5864\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5641 - val_loss: 2.5964\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5841 - val_loss: 2.6217\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6169 - val_loss: 2.5748\n",
      "Epoch 00076: early stopping\n",
      "0.6\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.3111 - val_loss: 2.3354\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2861 - val_loss: 2.3201\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3226 - val_loss: 2.3171\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3170 - val_loss: 2.3214\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3215 - val_loss: 2.3249\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3226 - val_loss: 2.3150\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3080 - val_loss: 2.3049\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3548 - val_loss: 2.3235\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3600 - val_loss: 2.3085\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3403 - val_loss: 2.3067\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3124 - val_loss: 2.3209\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2804 - val_loss: 2.3120\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3386 - val_loss: 2.3210\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2872 - val_loss: 2.3190\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3097 - val_loss: 2.3196\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3348 - val_loss: 2.3147\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3414 - val_loss: 2.3255\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3365 - val_loss: 2.3062\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3121 - val_loss: 2.3233\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2894 - val_loss: 2.3098\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3144 - val_loss: 2.3291\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3586 - val_loss: 2.3112\n",
      "Epoch 00022: early stopping\n",
      "0.7\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 1.9363 - val_loss: 1.9325\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8843 - val_loss: 1.9193\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9306 - val_loss: 1.9193\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9258 - val_loss: 1.9416\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9381 - val_loss: 1.9269\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8980 - val_loss: 1.9113\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9246 - val_loss: 1.9122\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8899 - val_loss: 1.9121\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9129 - val_loss: 1.9254\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8865 - val_loss: 1.9206\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9095 - val_loss: 1.9195\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9346 - val_loss: 1.9108\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9330 - val_loss: 1.9125\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9242 - val_loss: 1.9057\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8920 - val_loss: 1.9073\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8561 - val_loss: 1.9057\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8864 - val_loss: 1.9060\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8996 - val_loss: 1.9143\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8945 - val_loss: 1.8996\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8995 - val_loss: 1.9251\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8773 - val_loss: 1.8996\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8700 - val_loss: 1.9078\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9220 - val_loss: 1.8999\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9152 - val_loss: 1.9255\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8950 - val_loss: 1.8936\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8788 - val_loss: 1.8967\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9103 - val_loss: 1.8954\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8689 - val_loss: 1.9125\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9181 - val_loss: 1.9018\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8939 - val_loss: 1.9027\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8962 - val_loss: 1.8973\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9013 - val_loss: 1.8936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8893 - val_loss: 1.9041\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9103 - val_loss: 1.8876\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8825 - val_loss: 1.9191\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9074 - val_loss: 1.9017\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8951 - val_loss: 1.9066\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9032 - val_loss: 1.8955\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9297 - val_loss: 1.9342\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9168 - val_loss: 1.9201\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9021 - val_loss: 1.8911\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8967 - val_loss: 1.8964\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8958 - val_loss: 1.9042\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9096 - val_loss: 1.8971\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9128 - val_loss: 1.9048\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9090 - val_loss: 1.9065\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9129 - val_loss: 1.8948\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9128 - val_loss: 1.9025\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8941 - val_loss: 1.8984\n",
      "Epoch 00049: early stopping\n",
      "0.8\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 1.3994 - val_loss: 1.4017\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4007 - val_loss: 1.3999\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3962 - val_loss: 1.4018\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4110 - val_loss: 1.3979\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3959 - val_loss: 1.4099\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3965 - val_loss: 1.4160\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4138 - val_loss: 1.4079\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4009 - val_loss: 1.4163\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3843 - val_loss: 1.4023\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3754 - val_loss: 1.3975\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3943 - val_loss: 1.4057\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3901 - val_loss: 1.3946\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3678 - val_loss: 1.3989\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3704 - val_loss: 1.3896\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3882 - val_loss: 1.4067\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4088 - val_loss: 1.3936\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3993 - val_loss: 1.3961\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4156 - val_loss: 1.3915\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3969 - val_loss: 1.4039\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4106 - val_loss: 1.3906\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3678 - val_loss: 1.3951\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3688 - val_loss: 1.3927\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4122 - val_loss: 1.4010\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4010 - val_loss: 1.4095\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3721 - val_loss: 1.4108\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3678 - val_loss: 1.3951\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3943 - val_loss: 1.3924\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3647 - val_loss: 1.4071\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3939 - val_loss: 1.3925\n",
      "Epoch 00029: early stopping\n",
      "0.9\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 0.7989 - val_loss: 0.8040\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7734 - val_loss: 0.7988\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7662 - val_loss: 0.7897\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7884 - val_loss: 0.7895\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7841 - val_loss: 0.7839\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7734 - val_loss: 0.8008\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7595 - val_loss: 0.7978\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7778 - val_loss: 0.8006\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7612 - val_loss: 0.7998\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7858 - val_loss: 0.7876\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7658 - val_loss: 0.8077\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7788 - val_loss: 0.7988\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7783 - val_loss: 0.7924\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7721 - val_loss: 0.7866\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7810 - val_loss: 0.7831\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7699 - val_loss: 0.7889\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7648 - val_loss: 0.7930\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7591 - val_loss: 0.7989\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7842 - val_loss: 0.7956\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7739 - val_loss: 0.7921\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7560 - val_loss: 0.7893\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7800 - val_loss: 0.7863\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7759 - val_loss: 0.7964\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7680 - val_loss: 0.7929\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7622 - val_loss: 0.7809\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7616 - val_loss: 0.7975\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7653 - val_loss: 0.7887\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7666 - val_loss: 0.7794\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7594 - val_loss: 0.7873\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7649 - val_loss: 0.7866\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7753 - val_loss: 0.8023\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7602 - val_loss: 0.7925\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7786 - val_loss: 0.7769\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7698 - val_loss: 0.7797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7687 - val_loss: 0.8014\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7507 - val_loss: 0.7766\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7837 - val_loss: 0.7761\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7708 - val_loss: 0.7811\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7652 - val_loss: 0.7761\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7834 - val_loss: 0.7844\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7803 - val_loss: 0.7804\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7687 - val_loss: 0.7734\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7797 - val_loss: 0.7748\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7613 - val_loss: 0.7775\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7662 - val_loss: 0.7734\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7714 - val_loss: 0.7757\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7609 - val_loss: 0.7735\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7666 - val_loss: 0.7851\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7541 - val_loss: 0.7836\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7638 - val_loss: 0.7796\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7765 - val_loss: 0.7750\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7714 - val_loss: 0.7756\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7602 - val_loss: 0.7865\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7638 - val_loss: 0.7806\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7439 - val_loss: 0.7745\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7789 - val_loss: 0.7737\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7602 - val_loss: 0.7876\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7687 - val_loss: 0.7727\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7524 - val_loss: 0.7869\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7627 - val_loss: 0.7805\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7554 - val_loss: 0.7823\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7847 - val_loss: 0.7821\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7684 - val_loss: 0.7792\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7686 - val_loss: 0.7735\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7650 - val_loss: 0.7783\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7604 - val_loss: 0.7720\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7607 - val_loss: 0.7753\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7499 - val_loss: 0.7738\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7738 - val_loss: 0.7752\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7633 - val_loss: 0.7687\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7607 - val_loss: 0.7739\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7586 - val_loss: 0.7923\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7712 - val_loss: 0.7764\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7640 - val_loss: 0.7825\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7650 - val_loss: 0.7673\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7554 - val_loss: 0.7847\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7691 - val_loss: 0.7742\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7633 - val_loss: 0.7731\n",
      "Epoch 79/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7548 - val_loss: 0.7718\n",
      "Epoch 80/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7571 - val_loss: 0.7711\n",
      "Epoch 81/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7515 - val_loss: 0.7752\n",
      "Epoch 82/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7709 - val_loss: 0.7737\n",
      "Epoch 83/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7617 - val_loss: 0.7874\n",
      "Epoch 84/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7610 - val_loss: 0.7702\n",
      "Epoch 85/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7567 - val_loss: 0.7702\n",
      "Epoch 86/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7725 - val_loss: 0.7774\n",
      "Epoch 87/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7585 - val_loss: 0.7790\n",
      "Epoch 88/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7694 - val_loss: 0.7739\n",
      "Epoch 89/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7483 - val_loss: 0.7696\n",
      "Epoch 90/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7666 - val_loss: 0.7690\n",
      "Epoch 00090: early stopping\n",
      "2\n",
      "0.1\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.3463 - val_loss: 1.4655\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4343 - val_loss: 1.4535\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4146 - val_loss: 1.4529\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4144 - val_loss: 1.4476\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3901 - val_loss: 1.4774\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3904 - val_loss: 1.4496\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3698 - val_loss: 1.4551\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3761 - val_loss: 1.4463\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3720 - val_loss: 1.4407\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3915 - val_loss: 1.4397\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3920 - val_loss: 1.4368\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3806 - val_loss: 1.4353\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3761 - val_loss: 1.4387\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3631 - val_loss: 1.4409\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3735 - val_loss: 1.4367\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3631 - val_loss: 1.4399\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3588 - val_loss: 1.4327\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - ETA: 0s - loss: 1.357 - 1s 2ms/step - loss: 1.3578 - val_loss: 1.4443\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3585 - val_loss: 1.4349\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3665 - val_loss: 1.4374\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3917 - val_loss: 1.4307\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3634 - val_loss: 1.4418\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3694 - val_loss: 1.4333\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3697 - val_loss: 1.4306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3664 - val_loss: 1.4306\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3596 - val_loss: 1.4308\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3655 - val_loss: 1.4378\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3453 - val_loss: 1.4319\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3767 - val_loss: 1.4301\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3496 - val_loss: 1.4312\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3577 - val_loss: 1.4307\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3772 - val_loss: 1.4307\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3592 - val_loss: 1.4301\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3624 - val_loss: 1.4329\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3780 - val_loss: 1.4307\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3690 - val_loss: 1.4281\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3656 - val_loss: 1.4241\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3566 - val_loss: 1.4267\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3484 - val_loss: 1.4241\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3442 - val_loss: 1.4316\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3823 - val_loss: 1.4306\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3417 - val_loss: 1.4283\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3611 - val_loss: 1.4268\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3429 - val_loss: 1.4239\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3657 - val_loss: 1.4336\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3454 - val_loss: 1.4253\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3705 - val_loss: 1.4292\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3595 - val_loss: 1.4278\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3627 - val_loss: 1.4293\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3819 - val_loss: 1.4228\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3524 - val_loss: 1.4266\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3578 - val_loss: 1.4282\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3584 - val_loss: 1.4248\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3566 - val_loss: 1.4214\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3609 - val_loss: 1.4254\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3778 - val_loss: 1.4278\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3582 - val_loss: 1.4327\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3308 - val_loss: 1.4263\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3440 - val_loss: 1.4253\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3657 - val_loss: 1.4259\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3903 - val_loss: 1.4249\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3633 - val_loss: 1.4348\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3565 - val_loss: 1.4245\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3581 - val_loss: 1.4244\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3621 - val_loss: 1.4243\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3549 - val_loss: 1.4308\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3553 - val_loss: 1.4233\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3397 - val_loss: 1.4256\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3497 - val_loss: 1.4231\n",
      "Epoch 00069: early stopping\n",
      "0.2\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.3020 - val_loss: 2.2808\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1877 - val_loss: 2.2720\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2121 - val_loss: 2.2766\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2150 - val_loss: 2.2756\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1695 - val_loss: 2.2744\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1896 - val_loss: 2.2676\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1713 - val_loss: 2.2699\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1797 - val_loss: 2.2947\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1601 - val_loss: 2.2668\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1954 - val_loss: 2.2840\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1496 - val_loss: 2.2754\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1360 - val_loss: 2.2715\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1908 - val_loss: 2.2802\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2036 - val_loss: 2.2768\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1842 - val_loss: 2.2750\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1879 - val_loss: 2.2654\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2059 - val_loss: 2.2712\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1868 - val_loss: 2.2811\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1903 - val_loss: 2.2711\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1850 - val_loss: 2.2783\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2112 - val_loss: 2.2895\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1965 - val_loss: 2.2680\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2070 - val_loss: 2.2776\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1937 - val_loss: 2.2689\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1842 - val_loss: 2.2742\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1813 - val_loss: 2.2682\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1921 - val_loss: 2.2638\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1674 - val_loss: 2.2659\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1842 - val_loss: 2.2727\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1637 - val_loss: 2.2751\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1861 - val_loss: 2.2695\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1831 - val_loss: 2.2681\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - ETA: 0s - loss: 2.158 - 1s 2ms/step - loss: 2.1603 - val_loss: 2.2810\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1805 - val_loss: 2.2803\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1896 - val_loss: 2.2720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1995 - val_loss: 2.2829\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1359 - val_loss: 2.2671\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1821 - val_loss: 2.2707\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1658 - val_loss: 2.2720\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1385 - val_loss: 2.2715\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1499 - val_loss: 2.2835\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1719 - val_loss: 2.2775\n",
      "Epoch 00042: early stopping\n",
      "0.3\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.6157 - val_loss: 2.6869\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5506 - val_loss: 2.7016\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5973 - val_loss: 2.6802\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6087 - val_loss: 2.6799\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6064 - val_loss: 2.6862\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5804 - val_loss: 2.6756\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5819 - val_loss: 2.6735\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6216 - val_loss: 2.6908\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5905 - val_loss: 2.6784\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5809 - val_loss: 2.6842\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5693 - val_loss: 2.6848\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6092 - val_loss: 2.6711\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6290 - val_loss: 2.6805\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6356 - val_loss: 2.6803\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5699 - val_loss: 2.6830\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5568 - val_loss: 2.6861\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5760 - val_loss: 2.6989\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5536 - val_loss: 2.7041\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5808 - val_loss: 2.6764\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5448 - val_loss: 2.6762\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5552 - val_loss: 2.6685\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5564 - val_loss: 2.6810\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5590 - val_loss: 2.6704\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5647 - val_loss: 2.6833\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5827 - val_loss: 2.6861\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6199 - val_loss: 2.6794\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5619 - val_loss: 2.6839\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5865 - val_loss: 2.6811\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5916 - val_loss: 2.6758\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5776 - val_loss: 2.6764\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6154 - val_loss: 2.6778\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5511 - val_loss: 2.6696\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6044 - val_loss: 2.6926\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6043 - val_loss: 2.6742\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6429 - val_loss: 2.6769\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5647 - val_loss: 2.6761\n",
      "Epoch 00036: early stopping\n",
      "0.4\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.6785 - val_loss: 2.7915\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7046 - val_loss: 2.8186\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7424 - val_loss: 2.7996\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6924 - val_loss: 2.7852\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6832 - val_loss: 2.7776\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7053 - val_loss: 2.7920\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7217 - val_loss: 2.7835\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6895 - val_loss: 2.8139\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7231 - val_loss: 2.7780\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6933 - val_loss: 2.7757\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7083 - val_loss: 2.7784\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6868 - val_loss: 2.7808\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7219 - val_loss: 2.7834\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6449 - val_loss: 2.7779\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7143 - val_loss: 2.7902\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6411 - val_loss: 2.7815\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7335 - val_loss: 2.7765\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6877 - val_loss: 2.7750\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7051 - val_loss: 2.7851\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6901 - val_loss: 2.7912\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6407 - val_loss: 2.7876\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6613 - val_loss: 2.7711\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6993 - val_loss: 2.7884\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6644 - val_loss: 2.7713\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6966 - val_loss: 2.7905\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7052 - val_loss: 2.7828\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6630 - val_loss: 2.7925\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6712 - val_loss: 2.7739\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6784 - val_loss: 2.7689\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6857 - val_loss: 2.7647\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6500 - val_loss: 2.7726\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6912 - val_loss: 2.7840\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7026 - val_loss: 2.7730\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7221 - val_loss: 2.7687\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6888 - val_loss: 2.7692\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6826 - val_loss: 2.7724\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6558 - val_loss: 2.7681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6575 - val_loss: 2.7747\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6318 - val_loss: 2.7766\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7081 - val_loss: 2.7615\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6909 - val_loss: 2.7684\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6549 - val_loss: 2.7799\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6593 - val_loss: 2.7682\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6527 - val_loss: 2.7671\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6691 - val_loss: 2.7664\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7291 - val_loss: 2.7693\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6652 - val_loss: 2.7668\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6959 - val_loss: 2.7668\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6897 - val_loss: 2.7672\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7225 - val_loss: 2.7662\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6753 - val_loss: 2.7756\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6841 - val_loss: 2.7794\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6841 - val_loss: 2.7779\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6534 - val_loss: 2.7720\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6993 - val_loss: 2.7695\n",
      "Epoch 00055: early stopping\n",
      "0.5\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.5508 - val_loss: 2.6591\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5902 - val_loss: 2.6617\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6316 - val_loss: 2.6598\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5405 - val_loss: 2.6515\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6072 - val_loss: 2.6593\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6088 - val_loss: 2.6787\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5780 - val_loss: 2.6539\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5789 - val_loss: 2.6453\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5774 - val_loss: 2.6560\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5992 - val_loss: 2.6462\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5769 - val_loss: 2.6453\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5615 - val_loss: 2.6549\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5622 - val_loss: 2.6519\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5418 - val_loss: 2.6405\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5438 - val_loss: 2.6405\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5689 - val_loss: 2.6927\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5754 - val_loss: 2.6426\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5430 - val_loss: 2.6445\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5325 - val_loss: 2.6800\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6137 - val_loss: 2.6581\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5891 - val_loss: 2.6467\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5860 - val_loss: 2.6364\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5083 - val_loss: 2.6444\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5598 - val_loss: 2.6316\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5031 - val_loss: 2.6541\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5850 - val_loss: 2.6332\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5523 - val_loss: 2.6452\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5796 - val_loss: 2.6453\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5500 - val_loss: 2.6337\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5759 - val_loss: 2.6438\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5741 - val_loss: 2.6393\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5838 - val_loss: 2.6427\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6285 - val_loss: 2.6327\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5382 - val_loss: 2.6469\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6260 - val_loss: 2.6408\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5310 - val_loss: 2.6363\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5685 - val_loss: 2.6564\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5793 - val_loss: 2.6401\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5642 - val_loss: 2.6370\n",
      "Epoch 00039: early stopping\n",
      "0.6\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.3078 - val_loss: 2.3528\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3274 - val_loss: 2.3542\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2685 - val_loss: 2.3502\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3145 - val_loss: 2.3489\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3151 - val_loss: 2.3542\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3317 - val_loss: 2.3598\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3013 - val_loss: 2.3463\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2782 - val_loss: 2.3502\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2847 - val_loss: 2.3416\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2638 - val_loss: 2.3500\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2839 - val_loss: 2.3478\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2841 - val_loss: 2.3496\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3275 - val_loss: 2.3523\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3072 - val_loss: 2.3467\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3217 - val_loss: 2.3418\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2737 - val_loss: 2.3404\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3103 - val_loss: 2.3539\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2737 - val_loss: 2.3383\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2661 - val_loss: 2.3598\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2972 - val_loss: 2.3321\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2687 - val_loss: 2.3334\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3132 - val_loss: 2.3425\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3024 - val_loss: 2.3704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2762 - val_loss: 2.3375\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3027 - val_loss: 2.3451\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2999 - val_loss: 2.3461\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2761 - val_loss: 2.3311\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2867 - val_loss: 2.3554\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3027 - val_loss: 2.3337\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2970 - val_loss: 2.3358\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2638 - val_loss: 2.3495\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2957 - val_loss: 2.3278\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2975 - val_loss: 2.3323\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2585 - val_loss: 2.3414\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3356 - val_loss: 2.3363\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2876 - val_loss: 2.3311\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2812 - val_loss: 2.3469\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3067 - val_loss: 2.3303\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2951 - val_loss: 2.3411\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3180 - val_loss: 2.3275\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3028 - val_loss: 2.3358\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3119 - val_loss: 2.3357\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2953 - val_loss: 2.3272\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3250 - val_loss: 2.3310\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2320 - val_loss: 2.3291\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2779 - val_loss: 2.3313\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2980 - val_loss: 2.3424\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3199 - val_loss: 2.3353\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3193 - val_loss: 2.3486\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2777 - val_loss: 2.3300\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2936 - val_loss: 2.3317\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3233 - val_loss: 2.3344\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2801 - val_loss: 2.3331\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2825 - val_loss: 2.3264\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2654 - val_loss: 2.3217\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2837 - val_loss: 2.3415\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2792 - val_loss: 2.3278\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2569 - val_loss: 2.3460\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2742 - val_loss: 2.3302\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2926 - val_loss: 2.3325\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2294 - val_loss: 2.3561\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2915 - val_loss: 2.3430\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2973 - val_loss: 2.3251\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2488 - val_loss: 2.3312\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2824 - val_loss: 2.3259\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2879 - val_loss: 2.3305\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2942 - val_loss: 2.3297\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2607 - val_loss: 2.3176\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2977 - val_loss: 2.3525\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3003 - val_loss: 2.3261\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2774 - val_loss: 2.3345\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2821 - val_loss: 2.3360\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3050 - val_loss: 2.3264\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2549 - val_loss: 2.3231\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2297 - val_loss: 2.3211\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3095 - val_loss: 2.3304\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2598 - val_loss: 2.3269\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2649 - val_loss: 2.3372\n",
      "Epoch 79/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3133 - val_loss: 2.3249\n",
      "Epoch 80/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2786 - val_loss: 2.3562\n",
      "Epoch 81/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2766 - val_loss: 2.3304\n",
      "Epoch 82/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2867 - val_loss: 2.3432\n",
      "Epoch 83/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3077 - val_loss: 2.3201\n",
      "Epoch 00083: early stopping\n",
      "0.7\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 1.8975 - val_loss: 1.9406\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8858 - val_loss: 1.9205\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9209 - val_loss: 1.9287\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8864 - val_loss: 1.9270\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8678 - val_loss: 1.9322\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8926 - val_loss: 1.9230\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8837 - val_loss: 1.9151\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8722 - val_loss: 1.9141\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8993 - val_loss: 1.9125\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9082 - val_loss: 1.9396\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8968 - val_loss: 1.9283\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8786 - val_loss: 1.9101\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8992 - val_loss: 1.9205\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8548 - val_loss: 1.9141\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8681 - val_loss: 1.9084\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8930 - val_loss: 1.9143\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9070 - val_loss: 1.9258\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8937 - val_loss: 1.9093\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8833 - val_loss: 1.9206\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8424 - val_loss: 1.9134\n",
      "Epoch 21/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8801 - val_loss: 1.9265\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8794 - val_loss: 1.9574\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8751 - val_loss: 1.9126\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8658 - val_loss: 1.9267\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8745 - val_loss: 1.9074\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8580 - val_loss: 1.9100\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8569 - val_loss: 1.9062\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8662 - val_loss: 1.9083\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9040 - val_loss: 1.9169\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8738 - val_loss: 1.9230\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8596 - val_loss: 1.9145\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8653 - val_loss: 1.9103\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8658 - val_loss: 1.9254\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8851 - val_loss: 1.9187\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9164 - val_loss: 1.9070\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8825 - val_loss: 1.9064\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8850 - val_loss: 1.9136\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8728 - val_loss: 1.9214\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8985 - val_loss: 1.9226\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8651 - val_loss: 1.9161\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8895 - val_loss: 1.9175\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8620 - val_loss: 1.9113\n",
      "Epoch 00042: early stopping\n",
      "0.8\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 1.3919 - val_loss: 1.4013\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4043 - val_loss: 1.4165\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3692 - val_loss: 1.4056\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3813 - val_loss: 1.4134\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3829 - val_loss: 1.4345\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3688 - val_loss: 1.4027\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3796 - val_loss: 1.4153\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3498 - val_loss: 1.4080\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3974 - val_loss: 1.4104\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3784 - val_loss: 1.4143\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3759 - val_loss: 1.3982\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3747 - val_loss: 1.3998\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3776 - val_loss: 1.4110\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4083 - val_loss: 1.3976\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3734 - val_loss: 1.4299\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3694 - val_loss: 1.4153\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3331 - val_loss: 1.4052\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3872 - val_loss: 1.4108\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3492 - val_loss: 1.4071\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3929 - val_loss: 1.3995\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3660 - val_loss: 1.4067\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3873 - val_loss: 1.3948\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3686 - val_loss: 1.4045\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3585 - val_loss: 1.3973\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3667 - val_loss: 1.4036\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3584 - val_loss: 1.3993\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3795 - val_loss: 1.4005\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3734 - val_loss: 1.4000\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3554 - val_loss: 1.4414\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3751 - val_loss: 1.4008\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3562 - val_loss: 1.4032\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4026 - val_loss: 1.3971\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3610 - val_loss: 1.3993\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3513 - val_loss: 1.3936\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3824 - val_loss: 1.4054\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3842 - val_loss: 1.4179\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3650 - val_loss: 1.4000\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3587 - val_loss: 1.3971\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3609 - val_loss: 1.3981\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3505 - val_loss: 1.4095\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3596 - val_loss: 1.3950\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3555 - val_loss: 1.3946\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3494 - val_loss: 1.3941\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3613 - val_loss: 1.4024\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3756 - val_loss: 1.4218\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3833 - val_loss: 1.4025\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3646 - val_loss: 1.4222\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3837 - val_loss: 1.4059\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3656 - val_loss: 1.3939\n",
      "Epoch 00049: early stopping\n",
      "0.9\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 0.7660 - val_loss: 0.7840\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7633 - val_loss: 0.7784\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7703 - val_loss: 0.7812\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7632 - val_loss: 0.7788\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7639 - val_loss: 0.7746\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7857 - val_loss: 0.7770\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7520 - val_loss: 0.7816\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7689 - val_loss: 0.7838\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7793 - val_loss: 0.7819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7568 - val_loss: 0.7749\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7455 - val_loss: 0.7909\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7606 - val_loss: 0.7809\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7641 - val_loss: 0.7723\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7614 - val_loss: 0.7751\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7646 - val_loss: 0.7743\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7576 - val_loss: 0.7768\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7416 - val_loss: 0.7771\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7691 - val_loss: 0.7736\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7499 - val_loss: 0.7805\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7639 - val_loss: 0.7765\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7573 - val_loss: 0.7870\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7508 - val_loss: 0.7781\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7794 - val_loss: 0.7752\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7617 - val_loss: 0.7740\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7499 - val_loss: 0.7789\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7479 - val_loss: 0.7733\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7455 - val_loss: 0.7786\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7578 - val_loss: 0.7884\n",
      "Epoch 00028: early stopping\n",
      "3\n",
      "0.1\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.2945 - val_loss: 1.4050\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4262 - val_loss: 1.3846\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4058 - val_loss: 1.3855\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4038 - val_loss: 1.3800\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4037 - val_loss: 1.3822\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3923 - val_loss: 1.3679\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3840 - val_loss: 1.3668\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4036 - val_loss: 1.3700\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3893 - val_loss: 1.3761\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3904 - val_loss: 1.3637\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3628 - val_loss: 1.3644\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3973 - val_loss: 1.3637\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3835 - val_loss: 1.3629\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3911 - val_loss: 1.3644\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3740 - val_loss: 1.3691\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3865 - val_loss: 1.3683\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3644 - val_loss: 1.3595\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3697 - val_loss: 1.3586\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3835 - val_loss: 1.3640\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3904 - val_loss: 1.3655\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3726 - val_loss: 1.3608\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3831 - val_loss: 1.3579\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3886 - val_loss: 1.3570\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3786 - val_loss: 1.3803\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3637 - val_loss: 1.3654\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3862 - val_loss: 1.3587\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3616 - val_loss: 1.3607\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3806 - val_loss: 1.3622\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3672 - val_loss: 1.3560\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3501 - val_loss: 1.3568\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3783 - val_loss: 1.3697\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3867 - val_loss: 1.3594\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3717 - val_loss: 1.3593\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3568 - val_loss: 1.3625\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3613 - val_loss: 1.3672\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3736 - val_loss: 1.3568\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3756 - val_loss: 1.3579\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3443 - val_loss: 1.3560\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3831 - val_loss: 1.3603\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3710 - val_loss: 1.3642\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3741 - val_loss: 1.3568\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3686 - val_loss: 1.3577\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3859 - val_loss: 1.3615\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3731 - val_loss: 1.3710\n",
      "Epoch 00044: early stopping\n",
      "0.2\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 3ms/step - loss: 2.2577 - val_loss: 2.2117\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2144 - val_loss: 2.2028\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2085 - val_loss: 2.2022\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1950 - val_loss: 2.2059\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2099 - val_loss: 2.2062\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2185 - val_loss: 2.1979\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2095 - val_loss: 2.2066\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1990 - val_loss: 2.1980\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1712 - val_loss: 2.2335\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2070 - val_loss: 2.1924\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1692 - val_loss: 2.1940\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1781 - val_loss: 2.1944\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1631 - val_loss: 2.2033\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2190 - val_loss: 2.2024\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1873 - val_loss: 2.2075\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1534 - val_loss: 2.2078\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1853 - val_loss: 2.1974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1880 - val_loss: 2.1920\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1715 - val_loss: 2.1946\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1949 - val_loss: 2.1881\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1841 - val_loss: 2.2013\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2127 - val_loss: 2.1924\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1588 - val_loss: 2.1904\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2077 - val_loss: 2.1905\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1699 - val_loss: 2.1905\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1961 - val_loss: 2.1922\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2147 - val_loss: 2.2129\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1954 - val_loss: 2.1902\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1715 - val_loss: 2.1895\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1791 - val_loss: 2.1927\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1647 - val_loss: 2.1868\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2053 - val_loss: 2.1911\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1945 - val_loss: 2.1879\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1813 - val_loss: 2.1934\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2159 - val_loss: 2.1918\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1565 - val_loss: 2.1864\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2182 - val_loss: 2.1946\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1585 - val_loss: 2.1868\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1891 - val_loss: 2.1904\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1692 - val_loss: 2.2124\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1646 - val_loss: 2.1900\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1656 - val_loss: 2.1921\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1824 - val_loss: 2.1906\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1604 - val_loss: 2.2041\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1983 - val_loss: 2.1887\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1787 - val_loss: 2.1880\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1972 - val_loss: 2.1917\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1974 - val_loss: 2.1871\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2024 - val_loss: 2.2021\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1552 - val_loss: 2.1864\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2009 - val_loss: 2.1921\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1604 - val_loss: 2.2012\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1593 - val_loss: 2.1868\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1573 - val_loss: 2.1934\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1993 - val_loss: 2.1928\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1833 - val_loss: 2.1864\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1850 - val_loss: 2.1929\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1686 - val_loss: 2.1850\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1802 - val_loss: 2.1875\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1649 - val_loss: 2.1943\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1948 - val_loss: 2.1932\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1736 - val_loss: 2.2012\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1950 - val_loss: 2.1989\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1817 - val_loss: 2.1868\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1657 - val_loss: 2.1926\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1904 - val_loss: 2.1926\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1676 - val_loss: 2.1900\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1692 - val_loss: 2.1852\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1489 - val_loss: 2.1935\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2024 - val_loss: 2.1896\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1802 - val_loss: 2.1929\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1894 - val_loss: 2.1868\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1419 - val_loss: 2.1925\n",
      "Epoch 00073: early stopping\n",
      "0.3\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.6063 - val_loss: 2.6226\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5721 - val_loss: 2.6164\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6061 - val_loss: 2.6153\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5867 - val_loss: 2.6224\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5226 - val_loss: 2.6100\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6270 - val_loss: 2.6150\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5486 - val_loss: 2.6178\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6173 - val_loss: 2.6239\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5992 - val_loss: 2.6148\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5734 - val_loss: 2.6074\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6194 - val_loss: 2.6028\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5685 - val_loss: 2.6132\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5673 - val_loss: 2.6508\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5894 - val_loss: 2.6110\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5750 - val_loss: 2.6086\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5833 - val_loss: 2.6120\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5802 - val_loss: 2.6082\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6372 - val_loss: 2.6171\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5738 - val_loss: 2.6161\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5853 - val_loss: 2.6214\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5791 - val_loss: 2.6098\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5907 - val_loss: 2.6064\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5886 - val_loss: 2.6099\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5696 - val_loss: 2.6119\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5697 - val_loss: 2.6067\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6120 - val_loss: 2.6101\n",
      "Epoch 00026: early stopping\n",
      "0.4\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.7225 - val_loss: 2.7334\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6759 - val_loss: 2.7405\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7001 - val_loss: 2.7287\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7177 - val_loss: 2.7325\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7454 - val_loss: 2.7452\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6798 - val_loss: 2.7299\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6673 - val_loss: 2.7386\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7110 - val_loss: 2.7230\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6993 - val_loss: 2.7314\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6876 - val_loss: 2.7252\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6906 - val_loss: 2.7249\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6938 - val_loss: 2.7234\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6995 - val_loss: 2.7261\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6951 - val_loss: 2.7272\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7137 - val_loss: 2.7180\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6964 - val_loss: 2.7202\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6761 - val_loss: 2.7377\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6727 - val_loss: 2.7634\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7091 - val_loss: 2.7282\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6420 - val_loss: 2.7204\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6916 - val_loss: 2.7339\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6662 - val_loss: 2.7225\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6417 - val_loss: 2.7235\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6874 - val_loss: 2.7426\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6771 - val_loss: 2.7212\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7247 - val_loss: 2.7165\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6483 - val_loss: 2.7223\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6928 - val_loss: 2.7447\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6746 - val_loss: 2.7408\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7352 - val_loss: 2.7177\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6887 - val_loss: 2.7214\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7306 - val_loss: 2.7202\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6746 - val_loss: 2.7293\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6315 - val_loss: 2.7236\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6967 - val_loss: 2.7209\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6694 - val_loss: 2.7184\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6819 - val_loss: 2.7226\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6824 - val_loss: 2.7330\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6900 - val_loss: 2.7241\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7353 - val_loss: 2.7243\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7255 - val_loss: 2.7202\n",
      "Epoch 00041: early stopping\n",
      "0.5\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.5596 - val_loss: 2.6196\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5562 - val_loss: 2.6172\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5831 - val_loss: 2.6157\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5539 - val_loss: 2.6104\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6093 - val_loss: 2.6260\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5921 - val_loss: 2.6238\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5279 - val_loss: 2.6239\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5701 - val_loss: 2.6158\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5854 - val_loss: 2.6291\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5372 - val_loss: 2.6104\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5720 - val_loss: 2.6222\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5641 - val_loss: 2.6197\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6167 - val_loss: 2.6308\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5570 - val_loss: 2.6116\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6009 - val_loss: 2.6272\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6048 - val_loss: 2.6114\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5720 - val_loss: 2.6181\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5566 - val_loss: 2.6207\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5793 - val_loss: 2.6132\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5929 - val_loss: 2.6204\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5737 - val_loss: 2.6241\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5841 - val_loss: 2.6223\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5439 - val_loss: 2.6207\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5543 - val_loss: 2.6133\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5090 - val_loss: 2.6247\n",
      "Epoch 00025: early stopping\n",
      "0.6\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.2624 - val_loss: 2.3360\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2886 - val_loss: 2.3493\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2779 - val_loss: 2.3382\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3121 - val_loss: 2.3371\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3260 - val_loss: 2.3313\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2967 - val_loss: 2.3411\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2830 - val_loss: 2.3261\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3247 - val_loss: 2.3448\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3006 - val_loss: 2.3460\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2929 - val_loss: 2.3282\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2810 - val_loss: 2.3338\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2984 - val_loss: 2.3309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2756 - val_loss: 2.3362\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2616 - val_loss: 2.3310\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2670 - val_loss: 2.3385\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2921 - val_loss: 2.3306\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2949 - val_loss: 2.3331\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2616 - val_loss: 2.3316\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2785 - val_loss: 2.3244\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2972 - val_loss: 2.3298\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2749 - val_loss: 2.3309\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2815 - val_loss: 2.3321\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3011 - val_loss: 2.3514\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2743 - val_loss: 2.3478\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2900 - val_loss: 2.3369\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2957 - val_loss: 2.3297\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3120 - val_loss: 2.3397\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3094 - val_loss: 2.3248\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2803 - val_loss: 2.3293\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2838 - val_loss: 2.3316\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2852 - val_loss: 2.3277\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3042 - val_loss: 2.3293\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2329 - val_loss: 2.3305\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3188 - val_loss: 2.3280\n",
      "Epoch 00034: early stopping\n",
      "0.7\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 1.8823 - val_loss: 1.9221\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8660 - val_loss: 1.9353\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8906 - val_loss: 1.9190\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9324 - val_loss: 1.9189\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8706 - val_loss: 1.9189\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8761 - val_loss: 1.9177\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9220 - val_loss: 1.9248\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8761 - val_loss: 1.9149\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8613 - val_loss: 1.9198\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8777 - val_loss: 1.9157\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8847 - val_loss: 1.9156\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8968 - val_loss: 1.9302\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8949 - val_loss: 1.9180\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8623 - val_loss: 1.9110\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8944 - val_loss: 1.9135\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8655 - val_loss: 1.9095\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8542 - val_loss: 1.9074\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8945 - val_loss: 1.9150\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8915 - val_loss: 1.9275\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9120 - val_loss: 1.9200\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8644 - val_loss: 1.9163\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8776 - val_loss: 1.9178\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9108 - val_loss: 1.9129\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8493 - val_loss: 1.9127\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8487 - val_loss: 1.9265\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8874 - val_loss: 1.9135\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8747 - val_loss: 1.9144\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8871 - val_loss: 1.9142\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8826 - val_loss: 1.9173\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8652 - val_loss: 1.9092\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8630 - val_loss: 1.9120\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8790 - val_loss: 1.9144\n",
      "Epoch 00032: early stopping\n",
      "0.8\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 1.3999 - val_loss: 1.4072\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3853 - val_loss: 1.4015\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3494 - val_loss: 1.4122\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3845 - val_loss: 1.3964\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3690 - val_loss: 1.3951\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3680 - val_loss: 1.3936\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3695 - val_loss: 1.3974\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3700 - val_loss: 1.3937\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3682 - val_loss: 1.3974\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3710 - val_loss: 1.3931\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3944 - val_loss: 1.4028\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3524 - val_loss: 1.3950\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3938 - val_loss: 1.4074\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3783 - val_loss: 1.3929\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3650 - val_loss: 1.3979\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3561 - val_loss: 1.3958\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3654 - val_loss: 1.3866\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3614 - val_loss: 1.3981\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3821 - val_loss: 1.3925\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3813 - val_loss: 1.3972\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3397 - val_loss: 1.4011\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3767 - val_loss: 1.3850\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3721 - val_loss: 1.4183\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3726 - val_loss: 1.3959\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3644 - val_loss: 1.3857\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3773 - val_loss: 1.3874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3745 - val_loss: 1.3842\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3676 - val_loss: 1.3886\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3664 - val_loss: 1.4036\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3433 - val_loss: 1.3929\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3880 - val_loss: 1.3999\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3451 - val_loss: 1.3861\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3497 - val_loss: 1.3896\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3816 - val_loss: 1.3872\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3787 - val_loss: 1.3911\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3911 - val_loss: 1.3943\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3499 - val_loss: 1.3907\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3615 - val_loss: 1.3877\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3629 - val_loss: 1.3840\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3661 - val_loss: 1.3864\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3704 - val_loss: 1.3823\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3587 - val_loss: 1.3885\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3423 - val_loss: 1.3882\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3316 - val_loss: 1.3835\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3635 - val_loss: 1.3952\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3783 - val_loss: 1.3891\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3738 - val_loss: 1.3861\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3637 - val_loss: 1.3857\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3612 - val_loss: 1.3821\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3764 - val_loss: 1.3860\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3734 - val_loss: 1.3842\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3902 - val_loss: 1.3943\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3479 - val_loss: 1.3847\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3688 - val_loss: 1.3866\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3595 - val_loss: 1.4048\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3647 - val_loss: 1.3801\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3514 - val_loss: 1.3854\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3545 - val_loss: 1.3845\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3566 - val_loss: 1.3847\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3666 - val_loss: 1.3908\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3538 - val_loss: 1.3840\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3790 - val_loss: 1.3850\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3687 - val_loss: 1.3822\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3571 - val_loss: 1.3868\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3366 - val_loss: 1.3863\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3738 - val_loss: 1.3981\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3791 - val_loss: 1.3816\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3821 - val_loss: 1.3873\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3699 - val_loss: 1.3821\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3519 - val_loss: 1.3912\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3608 - val_loss: 1.3848\n",
      "Epoch 00071: early stopping\n",
      "0.9\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 3ms/step - loss: 0.7762 - val_loss: 0.7812\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7583 - val_loss: 0.7747\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7592 - val_loss: 0.7900\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7613 - val_loss: 0.7715\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7660 - val_loss: 0.7868\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7549 - val_loss: 0.7708\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7583 - val_loss: 0.7768\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7518 - val_loss: 0.7797\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7614 - val_loss: 0.7719\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7440 - val_loss: 0.7714\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7625 - val_loss: 0.7798\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7578 - val_loss: 0.7682\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7559 - val_loss: 0.7757\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7486 - val_loss: 0.7754\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7612 - val_loss: 0.7729\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7519 - val_loss: 0.7679\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7575 - val_loss: 0.7686\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - ETA: 0s - loss: 0.749 - 1s 2ms/step - loss: 0.7498 - val_loss: 0.7715\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7573 - val_loss: 0.7723\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7656 - val_loss: 0.7670\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7561 - val_loss: 0.7722\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7686 - val_loss: 0.7791\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7432 - val_loss: 0.7734\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7623 - val_loss: 0.7757\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7795 - val_loss: 0.7695\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7613 - val_loss: 0.7751\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7628 - val_loss: 0.7655\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7619 - val_loss: 0.7675\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7511 - val_loss: 0.7670\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7558 - val_loss: 0.7790\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7498 - val_loss: 0.7688\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7571 - val_loss: 0.7685\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7544 - val_loss: 0.7716\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7561 - val_loss: 0.7718\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7511 - val_loss: 0.7701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7707 - val_loss: 0.7793\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7663 - val_loss: 0.7738\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7586 - val_loss: 0.7652\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7526 - val_loss: 0.7709\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7476 - val_loss: 0.7673\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7527 - val_loss: 0.7678\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7428 - val_loss: 0.7690\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7484 - val_loss: 0.7676\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7555 - val_loss: 0.7691\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7584 - val_loss: 0.7648\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7536 - val_loss: 0.7652\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7597 - val_loss: 0.7648\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7469 - val_loss: 0.7674\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7546 - val_loss: 0.7911\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7523 - val_loss: 0.7655\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7548 - val_loss: 0.7715\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7631 - val_loss: 0.7699\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7540 - val_loss: 0.7714\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7622 - val_loss: 0.7682\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7456 - val_loss: 0.7753\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7567 - val_loss: 0.7657\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7621 - val_loss: 0.7666\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7581 - val_loss: 0.7751\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7552 - val_loss: 0.7677\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7587 - val_loss: 0.7675\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7534 - val_loss: 0.7728\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7647 - val_loss: 0.7624\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7576 - val_loss: 0.7627\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7405 - val_loss: 0.7684\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7489 - val_loss: 0.7728\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7552 - val_loss: 0.7701\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7595 - val_loss: 0.7627\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7450 - val_loss: 0.7787\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7542 - val_loss: 0.7663\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7596 - val_loss: 0.7664\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7595 - val_loss: 0.7656\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7553 - val_loss: 0.7729\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7557 - val_loss: 0.7639\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7503 - val_loss: 0.7700\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7327 - val_loss: 0.7693\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7652 - val_loss: 0.7861\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7479 - val_loss: 0.7632\n",
      "Epoch 00077: early stopping\n",
      "4\n",
      "0.1\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.3269 - val_loss: 1.4505\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4228 - val_loss: 1.4379\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3802 - val_loss: 1.4339\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4056 - val_loss: 1.4278\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4008 - val_loss: 1.4196\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.4002 - val_loss: 1.4161\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3778 - val_loss: 1.4140\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3973 - val_loss: 1.4161\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3702 - val_loss: 1.4095\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3712 - val_loss: 1.4088\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3720 - val_loss: 1.4094\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3898 - val_loss: 1.4082\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3539 - val_loss: 1.4060\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3414 - val_loss: 1.4190\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3883 - val_loss: 1.4116\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3922 - val_loss: 1.4074\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3559 - val_loss: 1.4053\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3812 - val_loss: 1.4111\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3861 - val_loss: 1.4186\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3785 - val_loss: 1.4052\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3910 - val_loss: 1.4032\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3557 - val_loss: 1.3990\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3651 - val_loss: 1.4014\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3665 - val_loss: 1.3989\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3695 - val_loss: 1.4009\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3581 - val_loss: 1.4000\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3535 - val_loss: 1.3998\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3708 - val_loss: 1.4046\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3579 - val_loss: 1.4048\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3765 - val_loss: 1.3991\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3675 - val_loss: 1.3962\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3650 - val_loss: 1.3978\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3625 - val_loss: 1.4053\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3598 - val_loss: 1.4057\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3582 - val_loss: 1.4289\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3568 - val_loss: 1.3983\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3454 - val_loss: 1.4033\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3739 - val_loss: 1.3981\n",
      "Epoch 39/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3568 - val_loss: 1.3974\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3562 - val_loss: 1.3946\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3614 - val_loss: 1.3959\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3794 - val_loss: 1.4032\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3662 - val_loss: 1.3939\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3603 - val_loss: 1.3994\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3512 - val_loss: 1.3973\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3562 - val_loss: 1.3943\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3566 - val_loss: 1.4007\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3466 - val_loss: 1.3970\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3644 - val_loss: 1.3968\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3629 - val_loss: 1.4026\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3567 - val_loss: 1.3946\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3746 - val_loss: 1.3983\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3245 - val_loss: 1.3993\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3483 - val_loss: 1.4051\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3637 - val_loss: 1.3929\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3517 - val_loss: 1.4010\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3631 - val_loss: 1.3911\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3638 - val_loss: 1.4127\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3594 - val_loss: 1.4021\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3729 - val_loss: 1.4050\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3324 - val_loss: 1.4088\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3546 - val_loss: 1.3969\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3694 - val_loss: 1.3947\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3555 - val_loss: 1.3895\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3517 - val_loss: 1.3990\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3487 - val_loss: 1.4017\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3612 - val_loss: 1.3937\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3435 - val_loss: 1.3964\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3606 - val_loss: 1.3943\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3569 - val_loss: 1.4116\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3652 - val_loss: 1.3995\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3362 - val_loss: 1.3950\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3487 - val_loss: 1.3937\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3476 - val_loss: 1.4009\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3846 - val_loss: 1.4053\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3623 - val_loss: 1.4030\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3546 - val_loss: 1.3976\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3448 - val_loss: 1.3979\n",
      "Epoch 79/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3449 - val_loss: 1.3932\n",
      "Epoch 00079: early stopping\n",
      "0.2\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.2290 - val_loss: 2.2451\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2090 - val_loss: 2.2450\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1949 - val_loss: 2.2501\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2028 - val_loss: 2.2435\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2000 - val_loss: 2.2651\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1913 - val_loss: 2.2512\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1609 - val_loss: 2.2453\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1698 - val_loss: 2.2478\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1610 - val_loss: 2.2529\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1651 - val_loss: 2.2429\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1701 - val_loss: 2.2628\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1646 - val_loss: 2.2412\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1479 - val_loss: 2.2491\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1781 - val_loss: 2.2398\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1425 - val_loss: 2.2416\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1651 - val_loss: 2.2435\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1915 - val_loss: 2.2406\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1795 - val_loss: 2.2397\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1690 - val_loss: 2.2403\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2009 - val_loss: 2.2400\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1904 - val_loss: 2.2419\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1705 - val_loss: 2.2417\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2069 - val_loss: 2.2441\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1500 - val_loss: 2.2354\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1674 - val_loss: 2.2409\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1358 - val_loss: 2.2512\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1740 - val_loss: 2.2418\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1616 - val_loss: 2.2423\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1525 - val_loss: 2.2401\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1767 - val_loss: 2.2482\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1355 - val_loss: 2.2444\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1722 - val_loss: 2.2359\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1606 - val_loss: 2.2375\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1778 - val_loss: 2.2422\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1598 - val_loss: 2.2560\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1591 - val_loss: 2.2455\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1642 - val_loss: 2.2492\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1721 - val_loss: 2.2443\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.1486 - val_loss: 2.2482\n",
      "Epoch 00039: early stopping\n",
      "0.3\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.6178 - val_loss: 2.6710\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6040 - val_loss: 2.6800\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5958 - val_loss: 2.6843\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5890 - val_loss: 2.6699\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5851 - val_loss: 2.6641\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5985 - val_loss: 2.6674\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5830 - val_loss: 2.6607\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5776 - val_loss: 2.6686\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5771 - val_loss: 2.6626\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6554 - val_loss: 2.6614\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6121 - val_loss: 2.6650\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5887 - val_loss: 2.6605\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5455 - val_loss: 2.6637\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5581 - val_loss: 2.6604\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6059 - val_loss: 2.6611\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5678 - val_loss: 2.6643\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5842 - val_loss: 2.6659\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6434 - val_loss: 2.6581\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5916 - val_loss: 2.6608\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5778 - val_loss: 2.6622\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5875 - val_loss: 2.6549\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6248 - val_loss: 2.6647\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5986 - val_loss: 2.6579\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5809 - val_loss: 2.6596\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5906 - val_loss: 2.6627\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5658 - val_loss: 2.6612\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5519 - val_loss: 2.6603\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5762 - val_loss: 2.6712\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5729 - val_loss: 2.6596\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5768 - val_loss: 2.6543\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6028 - val_loss: 2.6612\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5551 - val_loss: 2.6629\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5846 - val_loss: 2.6624\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5730 - val_loss: 2.6529\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5798 - val_loss: 2.6565\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5478 - val_loss: 2.6530\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5741 - val_loss: 2.6733\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5348 - val_loss: 2.6607\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5604 - val_loss: 2.6558\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5939 - val_loss: 2.6624\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5865 - val_loss: 2.6565\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5356 - val_loss: 2.6540\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5720 - val_loss: 2.6616\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5676 - val_loss: 2.6556\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5674 - val_loss: 2.6549\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5794 - val_loss: 2.6560\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5829 - val_loss: 2.6526\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5492 - val_loss: 2.6626\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5673 - val_loss: 2.6599\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5988 - val_loss: 2.6559\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5635 - val_loss: 2.6596\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5766 - val_loss: 2.6585\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5526 - val_loss: 2.6522\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5768 - val_loss: 2.6483\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5779 - val_loss: 2.6589\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5890 - val_loss: 2.6491\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6272 - val_loss: 2.6560\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5706 - val_loss: 2.6592\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5547 - val_loss: 2.6592\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5469 - val_loss: 2.6583\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5499 - val_loss: 2.6502\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5524 - val_loss: 2.6521\n",
      "Epoch 63/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5804 - val_loss: 2.6563\n",
      "Epoch 64/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5844 - val_loss: 2.6479\n",
      "Epoch 65/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5770 - val_loss: 2.6598\n",
      "Epoch 66/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5637 - val_loss: 2.6572\n",
      "Epoch 67/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6018 - val_loss: 2.6673\n",
      "Epoch 68/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5861 - val_loss: 2.6491\n",
      "Epoch 69/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5833 - val_loss: 2.6520\n",
      "Epoch 70/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5832 - val_loss: 2.6506\n",
      "Epoch 71/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5749 - val_loss: 2.6595\n",
      "Epoch 72/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5379 - val_loss: 2.6505\n",
      "Epoch 73/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5562 - val_loss: 2.6490\n",
      "Epoch 74/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5378 - val_loss: 2.6470\n",
      "Epoch 75/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5448 - val_loss: 2.6614\n",
      "Epoch 76/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5759 - val_loss: 2.6661\n",
      "Epoch 77/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5898 - val_loss: 2.6579\n",
      "Epoch 78/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5750 - val_loss: 2.6521\n",
      "Epoch 79/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5236 - val_loss: 2.6522\n",
      "Epoch 80/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5525 - val_loss: 2.6510\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5852 - val_loss: 2.6463\n",
      "Epoch 82/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5892 - val_loss: 2.6611\n",
      "Epoch 83/500\n",
      "329/329 [==============================] - ETA: 0s - loss: 2.557 - 1s 2ms/step - loss: 2.5579 - val_loss: 2.6488\n",
      "Epoch 84/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5621 - val_loss: 2.6553\n",
      "Epoch 85/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5827 - val_loss: 2.6532\n",
      "Epoch 86/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5837 - val_loss: 2.6602\n",
      "Epoch 87/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5462 - val_loss: 2.6536\n",
      "Epoch 88/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5620 - val_loss: 2.6663\n",
      "Epoch 89/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5311 - val_loss: 2.6498\n",
      "Epoch 90/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5381 - val_loss: 2.6598\n",
      "Epoch 91/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5322 - val_loss: 2.6625\n",
      "Epoch 92/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5502 - val_loss: 2.6548\n",
      "Epoch 93/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5666 - val_loss: 2.6520\n",
      "Epoch 94/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5772 - val_loss: 2.6483\n",
      "Epoch 95/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5379 - val_loss: 2.6482\n",
      "Epoch 96/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5727 - val_loss: 2.6629\n",
      "Epoch 00096: early stopping\n",
      "0.4\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 3ms/step - loss: 2.7352 - val_loss: 2.7558\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6703 - val_loss: 2.7661\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6696 - val_loss: 2.7540\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6948 - val_loss: 2.7744\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7046 - val_loss: 2.7580\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6260 - val_loss: 2.7447\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6502 - val_loss: 2.7437\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6654 - val_loss: 2.7459\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7046 - val_loss: 2.7468\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6680 - val_loss: 2.7450\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6726 - val_loss: 2.7750\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6475 - val_loss: 2.7575\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6331 - val_loss: 2.7436\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6962 - val_loss: 2.7579\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6399 - val_loss: 2.7476\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6115 - val_loss: 2.7520\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6737 - val_loss: 2.7433\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6887 - val_loss: 2.7642\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6644 - val_loss: 2.7407\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6183 - val_loss: 2.7391\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6402 - val_loss: 2.7508\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6585 - val_loss: 2.7388\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6357 - val_loss: 2.7681\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.7035 - val_loss: 2.7436\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6736 - val_loss: 2.7455\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6839 - val_loss: 2.7446\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6493 - val_loss: 2.7509\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6417 - val_loss: 2.7441\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6765 - val_loss: 2.7424\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6432 - val_loss: 2.7450\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6515 - val_loss: 2.7432\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6128 - val_loss: 2.7435\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6201 - val_loss: 2.7487\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6493 - val_loss: 2.7490\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6827 - val_loss: 2.7520\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6947 - val_loss: 2.7578\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5936 - val_loss: 2.7570\n",
      "Epoch 00037: early stopping\n",
      "0.5\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.5929 - val_loss: 2.6441\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5780 - val_loss: 2.6415\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6015 - val_loss: 2.6436\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5616 - val_loss: 2.6364\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6064 - val_loss: 2.6327\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5962 - val_loss: 2.6315\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6035 - val_loss: 2.6311\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5553 - val_loss: 2.6393\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5605 - val_loss: 2.6359\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5576 - val_loss: 2.6340\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5682 - val_loss: 2.6344\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.6091 - val_loss: 2.6637\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5272 - val_loss: 2.6529\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5755 - val_loss: 2.6250\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5512 - val_loss: 2.6397\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5447 - val_loss: 2.6416\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5460 - val_loss: 2.6332\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5175 - val_loss: 2.6352\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5883 - val_loss: 2.6312\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5541 - val_loss: 2.6315\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5538 - val_loss: 2.6327\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5594 - val_loss: 2.6312\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5360 - val_loss: 2.6387\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5841 - val_loss: 2.6308\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5526 - val_loss: 2.6285\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5662 - val_loss: 2.6275\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5647 - val_loss: 2.6309\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5965 - val_loss: 2.6369\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.5561 - val_loss: 2.6287\n",
      "Epoch 00029: early stopping\n",
      "0.6\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 2.3411 - val_loss: 2.3510\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3239 - val_loss: 2.3547\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2948 - val_loss: 2.3536\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3301 - val_loss: 2.3609\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3335 - val_loss: 2.3454\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3259 - val_loss: 2.3514\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2793 - val_loss: 2.3504\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3126 - val_loss: 2.3448\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3094 - val_loss: 2.3424\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2872 - val_loss: 2.3395\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2538 - val_loss: 2.3380\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2746 - val_loss: 2.3392\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2087 - val_loss: 2.3346\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3158 - val_loss: 2.3369\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2914 - val_loss: 2.3344\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2668 - val_loss: 2.3388\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2619 - val_loss: 2.3294\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3062 - val_loss: 2.3365\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2935 - val_loss: 2.3391\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2687 - val_loss: 2.3282\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3049 - val_loss: 2.3357\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2783 - val_loss: 2.3450\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3012 - val_loss: 2.3317\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2888 - val_loss: 2.3302\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2911 - val_loss: 2.3345\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2823 - val_loss: 2.3275\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3016 - val_loss: 2.3309\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2761 - val_loss: 2.3280\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3336 - val_loss: 2.3252\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2774 - val_loss: 2.3241\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3032 - val_loss: 2.3280\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2861 - val_loss: 2.3245\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3215 - val_loss: 2.3265\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2698 - val_loss: 2.3442\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2830 - val_loss: 2.3300\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2903 - val_loss: 2.3276\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2743 - val_loss: 2.3357\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2750 - val_loss: 2.3281\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2828 - val_loss: 2.3272\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2785 - val_loss: 2.3359\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3112 - val_loss: 2.3300\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2948 - val_loss: 2.3233\n",
      "Epoch 43/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2615 - val_loss: 2.3233\n",
      "Epoch 44/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3082 - val_loss: 2.3290\n",
      "Epoch 45/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2813 - val_loss: 2.3258\n",
      "Epoch 46/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2619 - val_loss: 2.3320\n",
      "Epoch 47/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2979 - val_loss: 2.3220\n",
      "Epoch 48/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2691 - val_loss: 2.3316\n",
      "Epoch 49/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2727 - val_loss: 2.3306\n",
      "Epoch 50/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3191 - val_loss: 2.3427\n",
      "Epoch 51/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2893 - val_loss: 2.3260\n",
      "Epoch 52/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2581 - val_loss: 2.3338\n",
      "Epoch 53/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.3038 - val_loss: 2.3279\n",
      "Epoch 54/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2486 - val_loss: 2.3341\n",
      "Epoch 55/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2702 - val_loss: 2.3419\n",
      "Epoch 56/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2515 - val_loss: 2.3240\n",
      "Epoch 57/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2906 - val_loss: 2.3274\n",
      "Epoch 58/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2530 - val_loss: 2.3291\n",
      "Epoch 59/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2692 - val_loss: 2.3240\n",
      "Epoch 60/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2348 - val_loss: 2.3317\n",
      "Epoch 61/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2568 - val_loss: 2.3350\n",
      "Epoch 62/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 2.2720 - val_loss: 2.3258\n",
      "Epoch 00062: early stopping\n",
      "0.7\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 5ms/step - loss: 1.8821 - val_loss: 1.9380\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8730 - val_loss: 1.9279\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8758 - val_loss: 1.9289\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8969 - val_loss: 1.9179\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8644 - val_loss: 1.9329\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8650 - val_loss: 1.9541\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8742 - val_loss: 1.9308\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8739 - val_loss: 1.9216\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8897 - val_loss: 1.9193\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8368 - val_loss: 1.9233\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8931 - val_loss: 1.9251\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8640 - val_loss: 1.9174\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8884 - val_loss: 1.9316\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8808 - val_loss: 1.9276\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9041 - val_loss: 1.9208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8699 - val_loss: 1.9259\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.9050 - val_loss: 1.9175\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8886 - val_loss: 1.9216\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8516 - val_loss: 1.9191\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8980 - val_loss: 1.9389\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8575 - val_loss: 1.9138\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8736 - val_loss: 1.9480\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8716 - val_loss: 1.9161\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8664 - val_loss: 1.9223\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8878 - val_loss: 1.9209\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8733 - val_loss: 1.9174\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8829 - val_loss: 1.9149\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8832 - val_loss: 1.9168\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8520 - val_loss: 1.9201\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8574 - val_loss: 1.9216\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8761 - val_loss: 1.9150\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8440 - val_loss: 1.9172\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8505 - val_loss: 1.9162\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8912 - val_loss: 1.9254\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8690 - val_loss: 1.9140\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.8600 - val_loss: 1.9178\n",
      "Epoch 00036: early stopping\n",
      "0.8\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 2ms/step - loss: 1.3588 - val_loss: 1.4353\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3621 - val_loss: 1.4139\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3674 - val_loss: 1.4074\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3575 - val_loss: 1.4066\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3751 - val_loss: 1.4159\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3979 - val_loss: 1.4034\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3563 - val_loss: 1.4077\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3622 - val_loss: 1.4053\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3585 - val_loss: 1.4060\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3746 - val_loss: 1.4082\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3832 - val_loss: 1.4139\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3695 - val_loss: 1.4053\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3765 - val_loss: 1.4164\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3548 - val_loss: 1.4215\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3978 - val_loss: 1.3996\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3918 - val_loss: 1.4073\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3671 - val_loss: 1.4064\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3691 - val_loss: 1.3949\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3624 - val_loss: 1.4021\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3671 - val_loss: 1.4017\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3431 - val_loss: 1.4049\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3678 - val_loss: 1.3988\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3721 - val_loss: 1.4070\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3607 - val_loss: 1.4000\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3570 - val_loss: 1.4052\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3579 - val_loss: 1.3975\n",
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3911 - val_loss: 1.4038\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3522 - val_loss: 1.4141\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3849 - val_loss: 1.3961\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3695 - val_loss: 1.4003\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3617 - val_loss: 1.3966\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3617 - val_loss: 1.4208\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 1.3749 - val_loss: 1.4176\n",
      "Epoch 00033: early stopping\n",
      "0.9\n",
      "Epoch 1/500\n",
      "329/329 [==============================] - 2s 3ms/step - loss: 0.7640 - val_loss: 0.7924\n",
      "Epoch 2/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7602 - val_loss: 0.7904\n",
      "Epoch 3/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7687 - val_loss: 0.7878\n",
      "Epoch 4/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7657 - val_loss: 0.7897\n",
      "Epoch 5/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7675 - val_loss: 0.7893\n",
      "Epoch 6/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7611 - val_loss: 0.8027\n",
      "Epoch 7/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7557 - val_loss: 0.7918\n",
      "Epoch 8/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7515 - val_loss: 0.7892\n",
      "Epoch 9/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7584 - val_loss: 0.7919\n",
      "Epoch 10/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7615 - val_loss: 0.7933\n",
      "Epoch 11/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7564 - val_loss: 0.7897\n",
      "Epoch 12/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7590 - val_loss: 0.7927\n",
      "Epoch 13/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7695 - val_loss: 0.7863\n",
      "Epoch 14/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7759 - val_loss: 0.7823\n",
      "Epoch 15/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7424 - val_loss: 0.7936\n",
      "Epoch 16/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7497 - val_loss: 0.7948\n",
      "Epoch 17/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7611 - val_loss: 0.7932\n",
      "Epoch 18/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7700 - val_loss: 0.7878\n",
      "Epoch 19/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7553 - val_loss: 0.7897\n",
      "Epoch 20/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7610 - val_loss: 0.7836\n",
      "Epoch 21/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7525 - val_loss: 0.7809\n",
      "Epoch 22/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7531 - val_loss: 0.7887\n",
      "Epoch 23/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7622 - val_loss: 0.7862\n",
      "Epoch 24/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7532 - val_loss: 0.7906\n",
      "Epoch 25/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7555 - val_loss: 0.7879\n",
      "Epoch 26/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7584 - val_loss: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7502 - val_loss: 0.7775\n",
      "Epoch 28/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7598 - val_loss: 0.7898\n",
      "Epoch 29/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7520 - val_loss: 0.7781\n",
      "Epoch 30/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7390 - val_loss: 0.7811\n",
      "Epoch 31/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7585 - val_loss: 0.7886\n",
      "Epoch 32/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7527 - val_loss: 0.7876\n",
      "Epoch 33/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7606 - val_loss: 0.7845\n",
      "Epoch 34/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7613 - val_loss: 0.7825\n",
      "Epoch 35/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7476 - val_loss: 0.7812\n",
      "Epoch 36/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7587 - val_loss: 0.7822\n",
      "Epoch 37/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7589 - val_loss: 0.7835\n",
      "Epoch 38/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7416 - val_loss: 0.7787\n",
      "Epoch 39/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7493 - val_loss: 0.8021\n",
      "Epoch 40/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7460 - val_loss: 0.7807\n",
      "Epoch 41/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7343 - val_loss: 0.7841\n",
      "Epoch 42/500\n",
      "329/329 [==============================] - 1s 2ms/step - loss: 0.7454 - val_loss: 0.7789\n",
      "Epoch 00042: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>-0.00027</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3888 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        q_0.1     q_0.2     q_0.3     q_0.4     q_0.5     q_0.6     q_0.7  \\\n",
       "0    -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "1    -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "2    -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "3    -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "4    -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "3883 -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "3884 -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "3885 -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "3886 -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "3887 -0.00027 -0.000271 -0.000211 -0.000109  0.000061  0.000144  0.000296   \n",
       "\n",
       "         q_0.8     q_0.9  \n",
       "0     0.000191  0.000273  \n",
       "1     0.000191  0.000273  \n",
       "2     0.000191  0.000273  \n",
       "3     0.000191  0.000273  \n",
       "4     0.000191  0.000273  \n",
       "...        ...       ...  \n",
       "3883  0.000191  0.000273  \n",
       "3884  0.000191  0.000273  \n",
       "3885  0.000191  0.000273  \n",
       "3886  0.000191  0.000273  \n",
       "3887  0.000191  0.000273  \n",
       "\n",
       "[3888 rows x 9 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "pred_1 = pd.DataFrame()\n",
    "for n_fold, (train_idx, val_idx) in enumerate(folds.split(x_train_1_scaler)):\n",
    "    train_x, train_y = x_train_1_scaler.loc[train_idx], y_train_1.loc[train_idx]\n",
    "    val_x, val_y = x_train_1_scaler.loc[val_idx], y_train_1.loc[val_idx]\n",
    "    print(n_fold)\n",
    "    for j in quantile_list:\n",
    "        print(j)\n",
    "        quantile = j\n",
    "        model.compile(loss=lambda y,f: tilted_loss(quantile,y,f), optimizer='adam')\n",
    "        model.fit(train_x, train_y, validation_data = (val_x, val_y), epochs=500,  batch_size=128, callbacks=[es])\n",
    "        pred_1 = pd.concat([pred_1, pd.DataFrame(model.predict(x_test_1_scaler))], axis = 1)\n",
    "        \n",
    "result_1 = pred_1.iloc[:,0:9]\n",
    "result_2 = pred_1.iloc[:,9:18]\n",
    "result_3 = pred_1.iloc[:,18:27]\n",
    "result_4 = pred_1.iloc[:,27:36]\n",
    "result_5 = pred_1.iloc[:,36:45]\n",
    "\n",
    "result = (result_1+result_2+result_3+result_4+result_5)/5\n",
    "result.columns = sub.iloc[:,1:].columns\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[sub.id.str.contains(\"Day7\"), \"q_0.1\":] = result.sort_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.05\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.1027 - val_loss: 0.7901\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7815 - val_loss: 0.7829\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7868 - val_loss: 0.7769\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7777 - val_loss: 0.7760\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7938 - val_loss: 0.7725\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7759 - val_loss: 0.7787\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7775 - val_loss: 0.7727\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7853 - val_loss: 0.7706\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7863 - val_loss: 0.7721\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7753 - val_loss: 0.7712\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7729 - val_loss: 0.7712\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7761 - val_loss: 0.7739\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7788 - val_loss: 0.7685\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7754 - val_loss: 0.7702\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7680 - val_loss: 0.7679\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7814 - val_loss: 0.7696\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7824 - val_loss: 0.7703\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7759 - val_loss: 0.7673\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - ETA: 0s - loss: 0.771 - 1s 2ms/step - loss: 0.7717 - val_loss: 0.7675\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7708 - val_loss: 0.7669\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7821 - val_loss: 0.7670\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7771 - val_loss: 0.7685\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7694 - val_loss: 0.7673\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7660 - val_loss: 0.7681\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7772 - val_loss: 0.7686\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7767 - val_loss: 0.7672\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7763 - val_loss: 0.7678\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7656 - val_loss: 0.7688\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7851 - val_loss: 0.7691\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7696 - val_loss: 0.7699\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7618 - val_loss: 0.7688\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7759 - val_loss: 0.7686\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7761 - val_loss: 0.7687\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7651 - val_loss: 0.7666\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7757 - val_loss: 0.7670\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7704 - val_loss: 0.7683\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7756 - val_loss: 0.7674\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7718 - val_loss: 0.7669\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7773 - val_loss: 0.7679\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7699 - val_loss: 0.7682\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7710 - val_loss: 0.7694\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7728 - val_loss: 0.7717\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7691 - val_loss: 0.7709\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7777 - val_loss: 0.7701\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7752 - val_loss: 0.7664\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7796 - val_loss: 0.7685\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7716 - val_loss: 0.7686\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7614 - val_loss: 0.7648\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7656 - val_loss: 0.7676\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7595 - val_loss: 0.7760\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7669 - val_loss: 0.7650\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7777 - val_loss: 0.7693\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7694 - val_loss: 0.7676\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7687 - val_loss: 0.7652\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7662 - val_loss: 0.7669\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7648 - val_loss: 0.7738\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7692 - val_loss: 0.7692\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7689 - val_loss: 0.7673\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7841 - val_loss: 0.7712\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7622 - val_loss: 0.7674\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7740 - val_loss: 0.7663\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7642 - val_loss: 0.7696\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7643 - val_loss: 0.7687\n",
      "Epoch 00063: early stopping\n",
      "0.15\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.9566 - val_loss: 1.9267\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8976 - val_loss: 1.9264\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9103 - val_loss: 1.9216\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8936 - val_loss: 1.9216\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9187 - val_loss: 1.9223\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9068 - val_loss: 1.9253\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8929 - val_loss: 1.9217\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9020 - val_loss: 1.9276\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8706 - val_loss: 1.9226\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9057 - val_loss: 1.9193\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9123 - val_loss: 1.9217\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8919 - val_loss: 1.9174\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8938 - val_loss: 1.9276\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9364 - val_loss: 1.9213\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8843 - val_loss: 1.9225\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8763 - val_loss: 1.9265\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8988 - val_loss: 1.9237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9102 - val_loss: 1.9191\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9193 - val_loss: 1.9164\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9033 - val_loss: 1.9124\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9178 - val_loss: 1.9147\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9110 - val_loss: 1.9206\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8771 - val_loss: 1.9188\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9239 - val_loss: 1.9183\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9078 - val_loss: 1.9226\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8778 - val_loss: 1.9125\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8747 - val_loss: 1.9164\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9062 - val_loss: 1.9163\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8958 - val_loss: 1.9186\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8745 - val_loss: 1.9108\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8830 - val_loss: 1.9255\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8905 - val_loss: 1.9097\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9117 - val_loss: 1.9131\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8664 - val_loss: 1.9123\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8786 - val_loss: 1.9175\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9011 - val_loss: 1.9127\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8879 - val_loss: 1.9089\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8973 - val_loss: 1.9114\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9074 - val_loss: 1.9157\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8810 - val_loss: 1.9237\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9252 - val_loss: 1.9123\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8900 - val_loss: 1.9110\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9027 - val_loss: 1.9243\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8658 - val_loss: 1.9259\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8828 - val_loss: 1.9131\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8889 - val_loss: 1.9095\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8788 - val_loss: 1.9115\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8969 - val_loss: 1.9170\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9043 - val_loss: 1.9144\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8814 - val_loss: 1.9136\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8740 - val_loss: 1.9159\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8803 - val_loss: 1.9107\n",
      "Epoch 00052: early stopping\n",
      "0.25\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.5840 - val_loss: 2.5875\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5586 - val_loss: 2.6048\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5419 - val_loss: 2.6044\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5657 - val_loss: 2.5897\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5432 - val_loss: 2.5852\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5786 - val_loss: 2.5812\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5844 - val_loss: 2.5789\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5451 - val_loss: 2.5898\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5604 - val_loss: 2.5804\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.4778 - val_loss: 2.5849\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5718 - val_loss: 2.5842\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5169 - val_loss: 2.5830\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5472 - val_loss: 2.5822\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5333 - val_loss: 2.5812\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5763 - val_loss: 2.5818\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5263 - val_loss: 2.5807\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5543 - val_loss: 2.5833\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5283 - val_loss: 2.5821\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5910 - val_loss: 2.5825\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5265 - val_loss: 2.5823\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5380 - val_loss: 2.5998\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5439 - val_loss: 2.5831\n",
      "Epoch 00022: early stopping\n",
      "0.35\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.8592 - val_loss: 2.8691\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8211 - val_loss: 2.8771\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8094 - val_loss: 2.8746\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8219 - val_loss: 2.8691\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8201 - val_loss: 2.8602\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8264 - val_loss: 2.8682\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8459 - val_loss: 2.8693\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7995 - val_loss: 2.8691\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8435 - val_loss: 2.8597\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8012 - val_loss: 2.8812\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8528 - val_loss: 2.8616\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8593 - val_loss: 2.8486\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7910 - val_loss: 2.8526\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7712 - val_loss: 2.8595\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8004 - val_loss: 2.8543\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8015 - val_loss: 2.9021\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8256 - val_loss: 2.8548\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8173 - val_loss: 2.8487\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8210 - val_loss: 2.8530\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8106 - val_loss: 2.8521\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8332 - val_loss: 2.8459\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8263 - val_loss: 2.8658\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8165 - val_loss: 2.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8551 - val_loss: 2.8724\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8512 - val_loss: 2.8560\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8613 - val_loss: 2.8544\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8086 - val_loss: 2.8645\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8232 - val_loss: 2.8447\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8188 - val_loss: 2.8567\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7921 - val_loss: 2.8503\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8173 - val_loss: 2.8502\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7683 - val_loss: 2.8558\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8285 - val_loss: 2.8459\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7843 - val_loss: 2.8522\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8426 - val_loss: 2.8471\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8313 - val_loss: 2.8525\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8564 - val_loss: 2.8520\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7849 - val_loss: 2.8597\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8256 - val_loss: 2.8577\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8001 - val_loss: 2.8570\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8437 - val_loss: 2.8505\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8287 - val_loss: 2.8516\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7882 - val_loss: 2.8486\n",
      "Epoch 00043: early stopping\n",
      "0.45\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.8377 - val_loss: 2.8363\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8659 - val_loss: 2.8291\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8381 - val_loss: 2.8185\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8418 - val_loss: 2.8430\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8484 - val_loss: 2.8316\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8366 - val_loss: 2.8248\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7975 - val_loss: 2.8247\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8261 - val_loss: 2.8333\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8000 - val_loss: 2.8362\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8140 - val_loss: 2.8230\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8210 - val_loss: 2.8187\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7809 - val_loss: 2.8425\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8004 - val_loss: 2.8229\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8364 - val_loss: 2.8297\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7986 - val_loss: 2.8218\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8519 - val_loss: 2.8240\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8425 - val_loss: 2.8348\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7652 - val_loss: 2.8155\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7859 - val_loss: 2.8177\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7780 - val_loss: 2.8447\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8525 - val_loss: 2.8182\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7651 - val_loss: 2.8216\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7887 - val_loss: 2.8220\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8537 - val_loss: 2.8220\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8075 - val_loss: 2.8295\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7987 - val_loss: 2.8200\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8781 - val_loss: 2.8152\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8503 - val_loss: 2.8208\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8512 - val_loss: 2.8271\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7897 - val_loss: 2.8302\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8219 - val_loss: 2.8404\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8377 - val_loss: 2.8151\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8120 - val_loss: 2.8129\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8001 - val_loss: 2.8281\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8566 - val_loss: 2.8209\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8251 - val_loss: 2.8292\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8111 - val_loss: 2.8175\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8434 - val_loss: 2.8094\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8297 - val_loss: 2.8244\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7930 - val_loss: 2.8235\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7982 - val_loss: 2.8143\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8626 - val_loss: 2.8193\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8157 - val_loss: 2.8177\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8399 - val_loss: 2.8361\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7925 - val_loss: 2.8176\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8038 - val_loss: 2.8100\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8082 - val_loss: 2.8187\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7928 - val_loss: 2.8382\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8184 - val_loss: 2.8221\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8525 - val_loss: 2.8129\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7340 - val_loss: 2.8224\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8253 - val_loss: 2.8170\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7774 - val_loss: 2.8208\n",
      "Epoch 00053: early stopping\n",
      "0.55\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.5968 - val_loss: 2.5855\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6224 - val_loss: 2.5821\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5953 - val_loss: 2.5822\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6228 - val_loss: 2.5897\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5746 - val_loss: 2.5765\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5904 - val_loss: 2.5843\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5691 - val_loss: 2.5761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5431 - val_loss: 2.5848\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5498 - val_loss: 2.5778\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5995 - val_loss: 2.5799\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5433 - val_loss: 2.5774\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6040 - val_loss: 2.5750\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5921 - val_loss: 2.5781\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5963 - val_loss: 2.5763\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5603 - val_loss: 2.5733\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5807 - val_loss: 2.5836\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5456 - val_loss: 2.5844\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5885 - val_loss: 2.5777\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5699 - val_loss: 2.5823\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5874 - val_loss: 2.5803\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5882 - val_loss: 2.5695\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6440 - val_loss: 2.5941\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5767 - val_loss: 2.5762\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5710 - val_loss: 2.5738\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5350 - val_loss: 2.5735\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5790 - val_loss: 2.5689\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5669 - val_loss: 2.5815\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6015 - val_loss: 2.5866\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6165 - val_loss: 2.5759\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5654 - val_loss: 2.5752\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6099 - val_loss: 2.5705\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5510 - val_loss: 2.5819\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5942 - val_loss: 2.5740\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5622 - val_loss: 2.5792\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5585 - val_loss: 2.5739\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5677 - val_loss: 2.5853\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5674 - val_loss: 2.5724\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5737 - val_loss: 2.5841\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5968 - val_loss: 2.5904\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6317 - val_loss: 2.5888\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5450 - val_loss: 2.5866\n",
      "Epoch 00041: early stopping\n",
      "0.65\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.1841 - val_loss: 2.2222\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2215 - val_loss: 2.2071\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2146 - val_loss: 2.2011\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2274 - val_loss: 2.2086\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2315 - val_loss: 2.2032\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2212 - val_loss: 2.2061\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2060 - val_loss: 2.2020\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2652 - val_loss: 2.2092\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1987 - val_loss: 2.2179\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2080 - val_loss: 2.2009\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1899 - val_loss: 2.2028\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1811 - val_loss: 2.2012\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2093 - val_loss: 2.2043\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2306 - val_loss: 2.2004\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1800 - val_loss: 2.1977\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2296 - val_loss: 2.2047\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2263 - val_loss: 2.2199\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1965 - val_loss: 2.2148\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1924 - val_loss: 2.2196\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2071 - val_loss: 2.1969\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2296 - val_loss: 2.1981\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2124 - val_loss: 2.2003\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2127 - val_loss: 2.1980\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2279 - val_loss: 2.1961\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2065 - val_loss: 2.2030\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2172 - val_loss: 2.2025\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2107 - val_loss: 2.1970\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2083 - val_loss: 2.1956\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2394 - val_loss: 2.2039\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1804 - val_loss: 2.1977\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2137 - val_loss: 2.2040\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2105 - val_loss: 2.2009\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2275 - val_loss: 2.1992\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1901 - val_loss: 2.1992\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2163 - val_loss: 2.1990\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1910 - val_loss: 2.2111\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2291 - val_loss: 2.1972\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2188 - val_loss: 2.2047\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1958 - val_loss: 2.1954\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2259 - val_loss: 2.2033\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2498 - val_loss: 2.2166\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2039 - val_loss: 2.1978\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2060 - val_loss: 2.2107\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2127 - val_loss: 2.1954\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2394 - val_loss: 2.1959\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1890 - val_loss: 2.2160\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2141 - val_loss: 2.2005\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2426 - val_loss: 2.2003\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1923 - val_loss: 2.1969\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1850 - val_loss: 2.2107\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1674 - val_loss: 2.2106\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2286 - val_loss: 2.2062\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2184 - val_loss: 2.1964\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1779 - val_loss: 2.1910\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2256 - val_loss: 2.1935\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1813 - val_loss: 2.1986\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1849 - val_loss: 2.1963\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2187 - val_loss: 2.2016\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1669 - val_loss: 2.2120\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2047 - val_loss: 2.2063\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1994 - val_loss: 2.2029\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2161 - val_loss: 2.1971\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1992 - val_loss: 2.1981\n",
      "Epoch 64/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1509 - val_loss: 2.1957\n",
      "Epoch 65/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2404 - val_loss: 2.1964\n",
      "Epoch 66/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1678 - val_loss: 2.2063\n",
      "Epoch 67/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2318 - val_loss: 2.1921\n",
      "Epoch 68/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1906 - val_loss: 2.1981\n",
      "Epoch 69/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1988 - val_loss: 2.2018\n",
      "Epoch 00069: early stopping\n",
      "0.75\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.7520 - val_loss: 1.7144\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7104 - val_loss: 1.7132\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7153 - val_loss: 1.7078\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6814 - val_loss: 1.7052\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7459 - val_loss: 1.7098\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7089 - val_loss: 1.7062\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6873 - val_loss: 1.7243\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6868 - val_loss: 1.7108\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7358 - val_loss: 1.7214\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7256 - val_loss: 1.7131\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6788 - val_loss: 1.7047\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7236 - val_loss: 1.7095\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6831 - val_loss: 1.7348\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7021 - val_loss: 1.7053\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7214 - val_loss: 1.7093\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7575 - val_loss: 1.7074\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7172 - val_loss: 1.7311\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7021 - val_loss: 1.7029\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7029 - val_loss: 1.7101\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6984 - val_loss: 1.7088\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7012 - val_loss: 1.7064\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7195 - val_loss: 1.7116\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6871 - val_loss: 1.7072\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7023 - val_loss: 1.7033\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7201 - val_loss: 1.6995\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6949 - val_loss: 1.7088\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7130 - val_loss: 1.7086\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6803 - val_loss: 1.7040\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7106 - val_loss: 1.7017\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6837 - val_loss: 1.7059\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6939 - val_loss: 1.7052\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7480 - val_loss: 1.7023\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6955 - val_loss: 1.7276\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7221 - val_loss: 1.7018\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7211 - val_loss: 1.6997\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7288 - val_loss: 1.6992\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7236 - val_loss: 1.7011\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6843 - val_loss: 1.6980\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7143 - val_loss: 1.7128\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6998 - val_loss: 1.6994\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7039 - val_loss: 1.7003\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7405 - val_loss: 1.7076\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7102 - val_loss: 1.7015\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7191 - val_loss: 1.6981\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7069 - val_loss: 1.7039\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6937 - val_loss: 1.6977\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7234 - val_loss: 1.7092\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6779 - val_loss: 1.7037\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7110 - val_loss: 1.7021\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6897 - val_loss: 1.7040\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7215 - val_loss: 1.7028\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7015 - val_loss: 1.7117\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6967 - val_loss: 1.6980\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6771 - val_loss: 1.6974\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7121 - val_loss: 1.7018\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6952 - val_loss: 1.7114\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6996 - val_loss: 1.6941\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7297 - val_loss: 1.7051\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7160 - val_loss: 1.6943\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7135 - val_loss: 1.7090\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7284 - val_loss: 1.6973\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7050 - val_loss: 1.6946\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6994 - val_loss: 1.7114\n",
      "Epoch 64/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7036 - val_loss: 1.6994\n",
      "Epoch 65/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7118 - val_loss: 1.6936\n",
      "Epoch 66/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6655 - val_loss: 1.6925\n",
      "Epoch 67/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6916 - val_loss: 1.6994\n",
      "Epoch 68/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7142 - val_loss: 1.6984\n",
      "Epoch 69/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6815 - val_loss: 1.6923\n",
      "Epoch 70/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7014 - val_loss: 1.6936\n",
      "Epoch 71/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7048 - val_loss: 1.6941\n",
      "Epoch 72/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7263 - val_loss: 1.6954\n",
      "Epoch 73/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6972 - val_loss: 1.6929\n",
      "Epoch 74/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7189 - val_loss: 1.7105\n",
      "Epoch 75/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7096 - val_loss: 1.6938\n",
      "Epoch 76/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7053 - val_loss: 1.7079\n",
      "Epoch 77/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7260 - val_loss: 1.7027\n",
      "Epoch 78/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7012 - val_loss: 1.6920\n",
      "Epoch 79/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6911 - val_loss: 1.7026\n",
      "Epoch 80/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6771 - val_loss: 1.7035\n",
      "Epoch 81/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7021 - val_loss: 1.6961\n",
      "Epoch 82/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6797 - val_loss: 1.7097\n",
      "Epoch 83/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7228 - val_loss: 1.6937\n",
      "Epoch 84/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6923 - val_loss: 1.6996\n",
      "Epoch 85/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7418 - val_loss: 1.7008\n",
      "Epoch 86/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7032 - val_loss: 1.7018\n",
      "Epoch 87/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7106 - val_loss: 1.6968\n",
      "Epoch 88/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6923 - val_loss: 1.6965\n",
      "Epoch 89/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7423 - val_loss: 1.6909\n",
      "Epoch 90/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6942 - val_loss: 1.6970\n",
      "Epoch 91/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6852 - val_loss: 1.7022\n",
      "Epoch 92/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7310 - val_loss: 1.6903\n",
      "Epoch 93/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7008 - val_loss: 1.6955\n",
      "Epoch 94/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7055 - val_loss: 1.6997\n",
      "Epoch 95/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6981 - val_loss: 1.6922\n",
      "Epoch 96/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6995 - val_loss: 1.6908\n",
      "Epoch 97/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7010 - val_loss: 1.6933\n",
      "Epoch 98/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7136 - val_loss: 1.7136\n",
      "Epoch 99/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7491 - val_loss: 1.6980\n",
      "Epoch 100/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6973 - val_loss: 1.6940\n",
      "Epoch 101/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6847 - val_loss: 1.6919\n",
      "Epoch 102/500\n",
      "328/328 [==============================] - ETA: 0s - loss: 1.700 - 1s 2ms/step - loss: 1.7005 - val_loss: 1.7036\n",
      "Epoch 103/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6771 - val_loss: 1.6986\n",
      "Epoch 104/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7271 - val_loss: 1.6907\n",
      "Epoch 105/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6747 - val_loss: 1.7010\n",
      "Epoch 106/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6919 - val_loss: 1.6933\n",
      "Epoch 107/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6933 - val_loss: 1.6905\n",
      "Epoch 00107: early stopping\n",
      "0.85\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.1236 - val_loss: 1.1105\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1092 - val_loss: 1.1242\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1259 - val_loss: 1.1135\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1255 - val_loss: 1.1169\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1104 - val_loss: 1.1093\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1115 - val_loss: 1.1083\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1097 - val_loss: 1.1202\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1126 - val_loss: 1.1189\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1189 - val_loss: 1.1102\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1011 - val_loss: 1.1308\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1304 - val_loss: 1.1043\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1181 - val_loss: 1.1107\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1331 - val_loss: 1.1133\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1020 - val_loss: 1.1066\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1262 - val_loss: 1.1066\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1162 - val_loss: 1.1095\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1081 - val_loss: 1.1135\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1297 - val_loss: 1.1108\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1007 - val_loss: 1.1133\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0844 - val_loss: 1.1086\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0936 - val_loss: 1.1158\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1091 - val_loss: 1.1097\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1120 - val_loss: 1.1193\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1280 - val_loss: 1.1099\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1229 - val_loss: 1.1076\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0875 - val_loss: 1.1165\n",
      "Epoch 00026: early stopping\n",
      "1\n",
      "0.05\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.0060 - val_loss: 0.7941\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7924 - val_loss: 0.7911\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7869 - val_loss: 0.7869\n",
      "Epoch 4/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7798 - val_loss: 0.7862\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7808 - val_loss: 0.7844\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7797 - val_loss: 0.7830\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7871 - val_loss: 0.7826\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7688 - val_loss: 0.7838\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7829 - val_loss: 0.7856\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7702 - val_loss: 0.7858\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7749 - val_loss: 0.7846\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7787 - val_loss: 0.7826\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7710 - val_loss: 0.7835\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7735 - val_loss: 0.7820\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7622 - val_loss: 0.7826\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7794 - val_loss: 0.7817\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7696 - val_loss: 0.7845\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7791 - val_loss: 0.7848\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7825 - val_loss: 0.7799\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7678 - val_loss: 0.7815\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7858 - val_loss: 0.7814\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7749 - val_loss: 0.7800\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7621 - val_loss: 0.7767\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7837 - val_loss: 0.7807\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7766 - val_loss: 0.7792\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7663 - val_loss: 0.7772\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7821 - val_loss: 0.7789\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7619 - val_loss: 0.7791\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7799 - val_loss: 0.7791\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7645 - val_loss: 0.7768\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7728 - val_loss: 0.7760\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7695 - val_loss: 0.7772\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7563 - val_loss: 0.7781\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7603 - val_loss: 0.7748\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7698 - val_loss: 0.7745\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7640 - val_loss: 0.7790\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7680 - val_loss: 0.7789\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7762 - val_loss: 0.7797\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7658 - val_loss: 0.7752\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7640 - val_loss: 0.7759\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7632 - val_loss: 0.7772\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7623 - val_loss: 0.7774\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7638 - val_loss: 0.7744\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7798 - val_loss: 0.7779\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7618 - val_loss: 0.7737\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7650 - val_loss: 0.7789\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7790 - val_loss: 0.7766\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7671 - val_loss: 0.7733\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7673 - val_loss: 0.7786\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7601 - val_loss: 0.7774\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7594 - val_loss: 0.7748\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7610 - val_loss: 0.7752\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7674 - val_loss: 0.7766\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7626 - val_loss: 0.7756\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7635 - val_loss: 0.7753\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7666 - val_loss: 0.7754\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7660 - val_loss: 0.7769\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7650 - val_loss: 0.7769\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7569 - val_loss: 0.7744\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7679 - val_loss: 0.7769\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7744 - val_loss: 0.7780\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7653 - val_loss: 0.7810\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7629 - val_loss: 0.7780\n",
      "Epoch 00063: early stopping\n",
      "0.15\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.9512 - val_loss: 1.9029\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9152 - val_loss: 1.9063\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9095 - val_loss: 1.9106\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9330 - val_loss: 1.9139\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9108 - val_loss: 1.9078\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8985 - val_loss: 1.9106\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9061 - val_loss: 1.9093\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8860 - val_loss: 1.9068\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8942 - val_loss: 1.9095\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8884 - val_loss: 1.9110\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8998 - val_loss: 1.9114\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8858 - val_loss: 1.9108\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8666 - val_loss: 1.9147\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8863 - val_loss: 1.9039\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8770 - val_loss: 1.9087\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8886 - val_loss: 1.9119\n",
      "Epoch 00016: early stopping\n",
      "0.25\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.6042 - val_loss: 2.5836\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5458 - val_loss: 2.5707\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5490 - val_loss: 2.5693\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5206 - val_loss: 2.5796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5812 - val_loss: 2.5725\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5504 - val_loss: 2.5737\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5033 - val_loss: 2.5652\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5717 - val_loss: 2.5779\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5366 - val_loss: 2.5747\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5029 - val_loss: 2.5743\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5382 - val_loss: 2.5669\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5508 - val_loss: 2.5736\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.4757 - val_loss: 2.5682\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5585 - val_loss: 2.5679\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5275 - val_loss: 2.5744\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5358 - val_loss: 2.5741\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5413 - val_loss: 2.5651\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5296 - val_loss: 2.5716\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5511 - val_loss: 2.5818\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5147 - val_loss: 2.5842\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5984 - val_loss: 2.5687\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5428 - val_loss: 2.5698\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5432 - val_loss: 2.5771\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5363 - val_loss: 2.5708\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5588 - val_loss: 2.5881\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5652 - val_loss: 2.5814\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5457 - val_loss: 2.5806\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5707 - val_loss: 2.5747\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5616 - val_loss: 2.5714\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.4981 - val_loss: 2.5811\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5422 - val_loss: 2.5743\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5118 - val_loss: 2.5724\n",
      "Epoch 00032: early stopping\n",
      "0.35\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.8420 - val_loss: 2.8741\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8437 - val_loss: 2.8683\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8530 - val_loss: 2.8647\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8650 - val_loss: 2.8837\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8244 - val_loss: 2.8695\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7990 - val_loss: 2.8685\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8020 - val_loss: 2.8670\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8147 - val_loss: 2.8705\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7971 - val_loss: 2.8838\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7994 - val_loss: 2.8687\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8080 - val_loss: 2.8713\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8235 - val_loss: 2.8733\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8044 - val_loss: 2.8669\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8312 - val_loss: 2.8654\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8195 - val_loss: 2.8815\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8384 - val_loss: 2.8831\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8124 - val_loss: 2.8626\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8152 - val_loss: 2.8690\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8458 - val_loss: 2.8702\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7794 - val_loss: 2.8801\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8573 - val_loss: 2.8722\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8044 - val_loss: 2.8701\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8148 - val_loss: 2.8702\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8247 - val_loss: 2.8756\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8045 - val_loss: 2.8742\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8030 - val_loss: 2.8727\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7973 - val_loss: 2.8650\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8541 - val_loss: 2.8677\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8066 - val_loss: 2.8615\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7669 - val_loss: 2.8653\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8235 - val_loss: 2.8629\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8188 - val_loss: 2.8661\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7920 - val_loss: 2.8930\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8094 - val_loss: 2.8665\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7806 - val_loss: 2.8820\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8339 - val_loss: 2.8653\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7914 - val_loss: 2.8663\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8461 - val_loss: 2.8640\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7772 - val_loss: 2.8672\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8247 - val_loss: 2.8674\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8266 - val_loss: 2.8636\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7763 - val_loss: 2.8667\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8667 - val_loss: 2.8710\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7938 - val_loss: 2.8691\n",
      "Epoch 00044: early stopping\n",
      "0.45\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.8453 - val_loss: 2.8762\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8369 - val_loss: 2.8752\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7933 - val_loss: 2.8646\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8033 - val_loss: 2.8588\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8166 - val_loss: 2.8746\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8080 - val_loss: 2.8574\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8427 - val_loss: 2.8644\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7735 - val_loss: 2.8650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7831 - val_loss: 2.8659\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8228 - val_loss: 2.8607\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8240 - val_loss: 2.8614\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7662 - val_loss: 2.8551\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7978 - val_loss: 2.8538\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7786 - val_loss: 2.8740\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7871 - val_loss: 2.8713\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7508 - val_loss: 2.8596\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8026 - val_loss: 2.8673\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8241 - val_loss: 2.8572\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7604 - val_loss: 2.8606\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8149 - val_loss: 2.8534\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8240 - val_loss: 2.8577\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8152 - val_loss: 2.8554\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8118 - val_loss: 2.8542\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8015 - val_loss: 2.8776\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8139 - val_loss: 2.8615\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7884 - val_loss: 2.8565\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7777 - val_loss: 2.8513\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7991 - val_loss: 2.8512\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8111 - val_loss: 2.8495\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8179 - val_loss: 2.8538\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7543 - val_loss: 2.8538\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7905 - val_loss: 2.8503\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8246 - val_loss: 2.8773\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7660 - val_loss: 2.8488\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7802 - val_loss: 2.8721\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8319 - val_loss: 2.8485\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7862 - val_loss: 2.8519\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7943 - val_loss: 2.8561\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7795 - val_loss: 2.8533\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8157 - val_loss: 2.8511\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7746 - val_loss: 2.8581\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7836 - val_loss: 2.8585\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7863 - val_loss: 2.8559\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8054 - val_loss: 2.8626\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7159 - val_loss: 2.8674\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8061 - val_loss: 2.8509\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8126 - val_loss: 2.8498\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7829 - val_loss: 2.8473\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7993 - val_loss: 2.8658\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7816 - val_loss: 2.8521\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8243 - val_loss: 2.8544\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7614 - val_loss: 2.8525\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7613 - val_loss: 2.8609\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7412 - val_loss: 2.8516\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8013 - val_loss: 2.8480\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8202 - val_loss: 2.8654\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8093 - val_loss: 2.8506\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7556 - val_loss: 2.8567\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7493 - val_loss: 2.8503\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7849 - val_loss: 2.8651\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7633 - val_loss: 2.8657\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7840 - val_loss: 2.8589\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7761 - val_loss: 2.8569\n",
      "Epoch 00063: early stopping\n",
      "0.55\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.5627 - val_loss: 2.6356\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6267 - val_loss: 2.6341\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5790 - val_loss: 2.6359\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6261 - val_loss: 2.6391\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5760 - val_loss: 2.6294\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5566 - val_loss: 2.6374\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5710 - val_loss: 2.6296\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5199 - val_loss: 2.6392\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5665 - val_loss: 2.6331\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5710 - val_loss: 2.6382\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5775 - val_loss: 2.6360\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5819 - val_loss: 2.6303\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5890 - val_loss: 2.6357\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5699 - val_loss: 2.6293\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5477 - val_loss: 2.6513\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5372 - val_loss: 2.6310\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5550 - val_loss: 2.6373\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5151 - val_loss: 2.6457\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5842 - val_loss: 2.6305\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5591 - val_loss: 2.6303\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5773 - val_loss: 2.6304\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5957 - val_loss: 2.6318\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5774 - val_loss: 2.6379\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5786 - val_loss: 2.6446\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5421 - val_loss: 2.6369\n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6033 - val_loss: 2.6392\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5819 - val_loss: 2.6265\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6228 - val_loss: 2.6327\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5543 - val_loss: 2.6297\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5666 - val_loss: 2.6284\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5492 - val_loss: 2.6343\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5728 - val_loss: 2.6261\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5701 - val_loss: 2.6362\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5401 - val_loss: 2.6301\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5737 - val_loss: 2.6305\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5816 - val_loss: 2.6304\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5689 - val_loss: 2.6487\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5967 - val_loss: 2.6633\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5791 - val_loss: 2.6269\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5787 - val_loss: 2.6316\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5334 - val_loss: 2.6301\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5253 - val_loss: 2.6282\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5323 - val_loss: 2.6330\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5368 - val_loss: 2.6297\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5648 - val_loss: 2.6286\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5407 - val_loss: 2.6295\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5884 - val_loss: 2.6339\n",
      "Epoch 00047: early stopping\n",
      "0.65\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.2126 - val_loss: 2.2582\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2169 - val_loss: 2.2595\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1946 - val_loss: 2.2611\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2215 - val_loss: 2.2499\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2029 - val_loss: 2.2605\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2017 - val_loss: 2.2480\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1991 - val_loss: 2.2429\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1661 - val_loss: 2.2560\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2013 - val_loss: 2.2586\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1980 - val_loss: 2.2502\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1715 - val_loss: 2.2556\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1970 - val_loss: 2.2512\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2152 - val_loss: 2.2475\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2074 - val_loss: 2.2473\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2065 - val_loss: 2.2526\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2027 - val_loss: 2.2571\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1953 - val_loss: 2.2495\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1876 - val_loss: 2.2588\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1951 - val_loss: 2.2456\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1946 - val_loss: 2.2445\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1594 - val_loss: 2.2476\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2217 - val_loss: 2.2545\n",
      "Epoch 00022: early stopping\n",
      "0.75\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.7304 - val_loss: 1.7486\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7131 - val_loss: 1.7475\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7040 - val_loss: 1.7653\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6846 - val_loss: 1.7444\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7008 - val_loss: 1.7453\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7012 - val_loss: 1.7450\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7028 - val_loss: 1.7416\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7099 - val_loss: 1.7411\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7099 - val_loss: 1.7400\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7356 - val_loss: 1.7391\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6939 - val_loss: 1.7422\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7112 - val_loss: 1.7390\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7061 - val_loss: 1.7437\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7105 - val_loss: 1.7371\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7088 - val_loss: 1.7584\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6959 - val_loss: 1.7432\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6958 - val_loss: 1.7390\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6891 - val_loss: 1.7486\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6884 - val_loss: 1.7423\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7024 - val_loss: 1.7503\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6763 - val_loss: 1.7484\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6981 - val_loss: 1.7442\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6973 - val_loss: 1.7543\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6814 - val_loss: 1.7425\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7070 - val_loss: 1.7417\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 3ms/step - loss: 1.6931 - val_loss: 1.7457\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7102 - val_loss: 1.7433\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6913 - val_loss: 1.7416\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7115 - val_loss: 1.7354\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7051 - val_loss: 1.7431\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6913 - val_loss: 1.7428\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6817 - val_loss: 1.7456\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6913 - val_loss: 1.7383\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6964 - val_loss: 1.7580\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7001 - val_loss: 1.7401\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6993 - val_loss: 1.7418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6830 - val_loss: 1.7339\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6831 - val_loss: 1.7399\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6820 - val_loss: 1.7354\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6756 - val_loss: 1.7436\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6851 - val_loss: 1.7401\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7116 - val_loss: 1.7464\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6793 - val_loss: 1.7355\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6684 - val_loss: 1.7423\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6972 - val_loss: 1.7591\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6880 - val_loss: 1.7404\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6862 - val_loss: 1.7393\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7039 - val_loss: 1.7367\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7132 - val_loss: 1.7441\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6859 - val_loss: 1.7417\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6826 - val_loss: 1.7412\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7050 - val_loss: 1.7457\n",
      "Epoch 00052: early stopping\n",
      "0.85\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.1149 - val_loss: 1.1433\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1175 - val_loss: 1.1403\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1265 - val_loss: 1.1456\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1400 - val_loss: 1.1419\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1349 - val_loss: 1.1441\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1273 - val_loss: 1.1447\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0939 - val_loss: 1.1397\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1163 - val_loss: 1.1507\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1273 - val_loss: 1.1484\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1190 - val_loss: 1.1406\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1114 - val_loss: 1.1477\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1140 - val_loss: 1.1349\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0787 - val_loss: 1.1392\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0967 - val_loss: 1.1443\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1191 - val_loss: 1.1453\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1154 - val_loss: 1.1483\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0895 - val_loss: 1.1481\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1266 - val_loss: 1.1392\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0958 - val_loss: 1.1432\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0901 - val_loss: 1.1430\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1160 - val_loss: 1.1413\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1057 - val_loss: 1.1329\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1039 - val_loss: 1.1378\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1172 - val_loss: 1.1381\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0994 - val_loss: 1.1426\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1054 - val_loss: 1.1363\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1072 - val_loss: 1.1420\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1155 - val_loss: 1.1369\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1066 - val_loss: 1.1340\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1266 - val_loss: 1.1391\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1095 - val_loss: 1.1338\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1103 - val_loss: 1.1370\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0951 - val_loss: 1.1403\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1234 - val_loss: 1.1439\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1320 - val_loss: 1.1412\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1027 - val_loss: 1.1359\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1219 - val_loss: 1.1380\n",
      "Epoch 00037: early stopping\n",
      "2\n",
      "0.05\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.0704 - val_loss: 0.8223\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7980 - val_loss: 0.8050\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7864 - val_loss: 0.7943\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7912 - val_loss: 0.7917\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7766 - val_loss: 0.7888\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7775 - val_loss: 0.7878\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7903 - val_loss: 0.7883\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7761 - val_loss: 0.7884\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7718 - val_loss: 0.7881\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7752 - val_loss: 0.7872\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7733 - val_loss: 0.7869\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7718 - val_loss: 0.7869\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7814 - val_loss: 0.7860\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7709 - val_loss: 0.7877\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7578 - val_loss: 0.7855\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7777 - val_loss: 0.7864\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7770 - val_loss: 0.7894\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7644 - val_loss: 0.7877\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7673 - val_loss: 0.7872\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7708 - val_loss: 0.7881\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7725 - val_loss: 0.7886\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7682 - val_loss: 0.7871\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7737 - val_loss: 0.7873\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7611 - val_loss: 0.7861\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7606 - val_loss: 0.7855\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7735 - val_loss: 0.7857\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7737 - val_loss: 0.7848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7738 - val_loss: 0.7861\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7652 - val_loss: 0.7842\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7720 - val_loss: 0.7851\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7712 - val_loss: 0.7847\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7620 - val_loss: 0.7848\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7642 - val_loss: 0.7851\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7629 - val_loss: 0.7869\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7651 - val_loss: 0.7869\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7794 - val_loss: 0.7926\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7660 - val_loss: 0.7848\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7726 - val_loss: 0.7850\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7662 - val_loss: 0.7881\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7673 - val_loss: 0.7847\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7601 - val_loss: 0.7884\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7712 - val_loss: 0.7846\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7639 - val_loss: 0.7856\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7791 - val_loss: 0.7873\n",
      "Epoch 00044: early stopping\n",
      "0.15\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 1.9408 - val_loss: 1.9213\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8780 - val_loss: 1.9199\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9178 - val_loss: 1.9270\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9074 - val_loss: 1.9234\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8938 - val_loss: 1.9155\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9191 - val_loss: 1.9326\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9133 - val_loss: 1.9201\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8981 - val_loss: 1.9194\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8791 - val_loss: 1.9240\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8995 - val_loss: 1.9176\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8904 - val_loss: 1.9205\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8945 - val_loss: 1.9190\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9079 - val_loss: 1.9143\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9042 - val_loss: 1.9188\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9098 - val_loss: 1.9130\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9133 - val_loss: 1.9262\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9055 - val_loss: 1.9140\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8734 - val_loss: 1.9171\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9333 - val_loss: 1.9236\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8705 - val_loss: 1.9237\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8961 - val_loss: 1.9186\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9006 - val_loss: 1.9148\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8874 - val_loss: 1.9266\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8568 - val_loss: 1.9351\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8617 - val_loss: 1.9193\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8544 - val_loss: 1.9224\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9059 - val_loss: 1.9140\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8783 - val_loss: 1.9224\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8839 - val_loss: 1.9160\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8846 - val_loss: 1.9101\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8603 - val_loss: 1.9141\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8637 - val_loss: 1.9226\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8967 - val_loss: 1.9220\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8979 - val_loss: 1.9146\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8868 - val_loss: 1.9205\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8592 - val_loss: 1.9200\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8545 - val_loss: 1.9228\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9053 - val_loss: 1.9202\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8927 - val_loss: 1.9183\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8935 - val_loss: 1.9182\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8829 - val_loss: 1.9170\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8778 - val_loss: 1.9169\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8617 - val_loss: 1.9196\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8767 - val_loss: 1.9232\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9026 - val_loss: 1.9168\n",
      "Epoch 00045: early stopping\n",
      "0.25\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.5746 - val_loss: 2.5710\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5424 - val_loss: 2.5663\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5611 - val_loss: 2.5703\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5766 - val_loss: 2.5619\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5489 - val_loss: 2.5589\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5445 - val_loss: 2.5585\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5665 - val_loss: 2.5556\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5270 - val_loss: 2.5590\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5280 - val_loss: 2.5580\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5154 - val_loss: 2.5596\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5528 - val_loss: 2.5533\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5614 - val_loss: 2.5587\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5810 - val_loss: 2.5584\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5386 - val_loss: 2.5647\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5384 - val_loss: 2.5570\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5517 - val_loss: 2.5543\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5235 - val_loss: 2.5714\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5706 - val_loss: 2.5544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5411 - val_loss: 2.5614\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5023 - val_loss: 2.5623\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5237 - val_loss: 2.5662\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5239 - val_loss: 2.5777\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.4933 - val_loss: 2.5633\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5488 - val_loss: 2.5684\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5369 - val_loss: 2.5547\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5149 - val_loss: 2.5875\n",
      "Epoch 00026: early stopping\n",
      "0.35\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.8593 - val_loss: 2.8676\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7970 - val_loss: 2.8529\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8102 - val_loss: 2.8571\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8122 - val_loss: 2.8720\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8294 - val_loss: 2.8650\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7831 - val_loss: 2.8678\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8038 - val_loss: 2.8493\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8293 - val_loss: 2.8650\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8684 - val_loss: 2.8609\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7772 - val_loss: 2.8664\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7957 - val_loss: 2.8579\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8203 - val_loss: 2.8606\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8234 - val_loss: 2.8495\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8084 - val_loss: 2.8556\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8058 - val_loss: 2.8624\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8229 - val_loss: 2.8523\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8048 - val_loss: 2.8481\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8255 - val_loss: 2.8579\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8333 - val_loss: 2.8617\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7914 - val_loss: 2.8507\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8335 - val_loss: 2.8497\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8187 - val_loss: 2.8585\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8072 - val_loss: 2.8624\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8097 - val_loss: 2.8617\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7983 - val_loss: 2.8892\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8616 - val_loss: 2.8556\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7464 - val_loss: 2.8504\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8140 - val_loss: 2.8591\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8055 - val_loss: 2.8545\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7882 - val_loss: 2.8576\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8218 - val_loss: 2.8462\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7812 - val_loss: 2.8478\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7413 - val_loss: 2.8632\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8195 - val_loss: 2.8500\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7696 - val_loss: 2.8480\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7935 - val_loss: 2.8556\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8341 - val_loss: 2.8566\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8461 - val_loss: 2.8577\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7988 - val_loss: 2.8534\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8107 - val_loss: 2.8577\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8536 - val_loss: 2.8647\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8521 - val_loss: 2.8607\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7998 - val_loss: 2.8715\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7821 - val_loss: 2.8580\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8015 - val_loss: 2.8510\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8119 - val_loss: 2.8509\n",
      "Epoch 00046: early stopping\n",
      "0.45\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.8509 - val_loss: 2.8680\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8366 - val_loss: 2.8575\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7394 - val_loss: 2.8606\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7884 - val_loss: 2.8527\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8174 - val_loss: 2.8571\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7503 - val_loss: 2.8572\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7708 - val_loss: 2.8512\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8078 - val_loss: 2.8597\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7977 - val_loss: 2.8537\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8260 - val_loss: 2.8562\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7945 - val_loss: 2.8498\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7942 - val_loss: 2.8637\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8079 - val_loss: 2.8656\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8056 - val_loss: 2.8657\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8049 - val_loss: 2.8590\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8200 - val_loss: 2.8533\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8272 - val_loss: 2.8576\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7935 - val_loss: 2.8611\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7952 - val_loss: 2.8469\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7912 - val_loss: 2.8670\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8334 - val_loss: 2.8482\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8005 - val_loss: 2.8580\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7628 - val_loss: 2.8509\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7589 - val_loss: 2.8599\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7985 - val_loss: 2.8510\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7678 - val_loss: 2.8601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7807 - val_loss: 2.8535\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8076 - val_loss: 2.8602\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7626 - val_loss: 2.8583\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7859 - val_loss: 2.8598\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7702 - val_loss: 2.8524\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7958 - val_loss: 2.8436\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7762 - val_loss: 2.8676\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8354 - val_loss: 2.8476\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7953 - val_loss: 2.8505\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8036 - val_loss: 2.8571\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7995 - val_loss: 2.8493\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7809 - val_loss: 2.8478\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7502 - val_loss: 2.8570\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8182 - val_loss: 2.8515\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7970 - val_loss: 2.8599\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7995 - val_loss: 2.8454\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7874 - val_loss: 2.8460\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7950 - val_loss: 2.8443\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8149 - val_loss: 2.8590\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7486 - val_loss: 2.8500\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7569 - val_loss: 2.8435\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7599 - val_loss: 2.8550\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8113 - val_loss: 2.8481\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7371 - val_loss: 2.8534\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8297 - val_loss: 2.8713\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7931 - val_loss: 2.8527\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7919 - val_loss: 2.8414\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7831 - val_loss: 2.8465\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8106 - val_loss: 2.8762\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7698 - val_loss: 2.8476\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7967 - val_loss: 2.8588\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8005 - val_loss: 2.8485\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7758 - val_loss: 2.8473\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7475 - val_loss: 2.8631\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7557 - val_loss: 2.8698\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8058 - val_loss: 2.8530\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7840 - val_loss: 2.8502\n",
      "Epoch 64/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8025 - val_loss: 2.8519\n",
      "Epoch 65/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7564 - val_loss: 2.8505\n",
      "Epoch 66/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8091 - val_loss: 2.8451\n",
      "Epoch 67/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7881 - val_loss: 2.8619\n",
      "Epoch 68/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7499 - val_loss: 2.8511\n",
      "Epoch 00068: early stopping\n",
      "0.55\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.5892 - val_loss: 2.6485\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5541 - val_loss: 2.6523\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5914 - val_loss: 2.6358\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5559 - val_loss: 2.6362\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5472 - val_loss: 2.6507\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5723 - val_loss: 2.6470\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5410 - val_loss: 2.6355\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5802 - val_loss: 2.6320\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5427 - val_loss: 2.6348\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5428 - val_loss: 2.6275\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5515 - val_loss: 2.6659\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5388 - val_loss: 2.6337\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5538 - val_loss: 2.6480\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5547 - val_loss: 2.6381\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5499 - val_loss: 2.6286\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5296 - val_loss: 2.6431\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5546 - val_loss: 2.6358\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5785 - val_loss: 2.6376\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5845 - val_loss: 2.6389\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5606 - val_loss: 2.6359\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5787 - val_loss: 2.6318\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5701 - val_loss: 2.6376\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5409 - val_loss: 2.6379\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5831 - val_loss: 2.6437\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5669 - val_loss: 2.6341\n",
      "Epoch 00025: early stopping\n",
      "0.65\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 3s 2ms/step - loss: 2.2001 - val_loss: 2.2723\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1854 - val_loss: 2.2742\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2251 - val_loss: 2.2608\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1967 - val_loss: 2.2578\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2057 - val_loss: 2.2696\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1678 - val_loss: 2.2630\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1613 - val_loss: 2.2570\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1582 - val_loss: 2.2574\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2093 - val_loss: 2.2653\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1858 - val_loss: 2.2635\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1717 - val_loss: 2.2627\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1851 - val_loss: 2.2555\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1788 - val_loss: 2.2628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1922 - val_loss: 2.2494\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1983 - val_loss: 2.2526\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1898 - val_loss: 2.2570\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1925 - val_loss: 2.2589\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1973 - val_loss: 2.2696\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2138 - val_loss: 2.2550\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2026 - val_loss: 2.2559\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2087 - val_loss: 2.2587\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1304 - val_loss: 2.2495\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1880 - val_loss: 2.2517\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2281 - val_loss: 2.2518\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2043 - val_loss: 2.2446\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1692 - val_loss: 2.2579\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1937 - val_loss: 2.2583\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1867 - val_loss: 2.2475\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1878 - val_loss: 2.2510\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1831 - val_loss: 2.2555\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1733 - val_loss: 2.2615\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1640 - val_loss: 2.2485\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1957 - val_loss: 2.2533\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1869 - val_loss: 2.2546\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1633 - val_loss: 2.2547\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1447 - val_loss: 2.2482\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1663 - val_loss: 2.2557\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1949 - val_loss: 2.2457\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1943 - val_loss: 2.2515\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1831 - val_loss: 2.2660\n",
      "Epoch 00040: early stopping\n",
      "0.75\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.6929 - val_loss: 1.7545\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6972 - val_loss: 1.7618\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6837 - val_loss: 1.7581\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7154 - val_loss: 1.7538\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7007 - val_loss: 1.7461\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7311 - val_loss: 1.7466\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7033 - val_loss: 1.7432\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6918 - val_loss: 1.7445\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6887 - val_loss: 1.7473\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7154 - val_loss: 1.7518\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7015 - val_loss: 1.7457\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7052 - val_loss: 1.7477\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6803 - val_loss: 1.7440\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6902 - val_loss: 1.7399\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7160 - val_loss: 1.7435\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7330 - val_loss: 1.7473\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6918 - val_loss: 1.7507\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6926 - val_loss: 1.7410\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7050 - val_loss: 1.7414\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7151 - val_loss: 1.7466\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7052 - val_loss: 1.7435\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6759 - val_loss: 1.7431\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6892 - val_loss: 1.7402\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6841 - val_loss: 1.7719\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7224 - val_loss: 1.7483\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6804 - val_loss: 1.7398\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7302 - val_loss: 1.7529\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6526 - val_loss: 1.7426\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7186 - val_loss: 1.7414\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7114 - val_loss: 1.7457\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6701 - val_loss: 1.7454\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6994 - val_loss: 1.7412\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7176 - val_loss: 1.7388\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6835 - val_loss: 1.7376\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7224 - val_loss: 1.7756\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6934 - val_loss: 1.7452\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6979 - val_loss: 1.7588\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6782 - val_loss: 1.7532\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6539 - val_loss: 1.7554\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6690 - val_loss: 1.7409\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6897 - val_loss: 1.7368\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7104 - val_loss: 1.7520\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6728 - val_loss: 1.7395\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7175 - val_loss: 1.7426\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6864 - val_loss: 1.7355\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6941 - val_loss: 1.7454\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6905 - val_loss: 1.7382\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6864 - val_loss: 1.7399\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6883 - val_loss: 1.7338\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6749 - val_loss: 1.7471\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6510 - val_loss: 1.7368\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6999 - val_loss: 1.7543\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6970 - val_loss: 1.7508\n",
      "Epoch 54/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7000 - val_loss: 1.7345\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6849 - val_loss: 1.7332\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6742 - val_loss: 1.7437\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7118 - val_loss: 1.7344\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6948 - val_loss: 1.7425\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6833 - val_loss: 1.7467\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6880 - val_loss: 1.7469\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6817 - val_loss: 1.7404\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6960 - val_loss: 1.7337\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6848 - val_loss: 1.7319\n",
      "Epoch 64/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7057 - val_loss: 1.7358\n",
      "Epoch 65/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6744 - val_loss: 1.7307\n",
      "Epoch 66/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7022 - val_loss: 1.7317\n",
      "Epoch 67/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7085 - val_loss: 1.7447\n",
      "Epoch 68/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7259 - val_loss: 1.7350\n",
      "Epoch 69/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7195 - val_loss: 1.7452\n",
      "Epoch 70/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6978 - val_loss: 1.7354\n",
      "Epoch 71/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6977 - val_loss: 1.7392\n",
      "Epoch 72/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6951 - val_loss: 1.7338\n",
      "Epoch 73/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7017 - val_loss: 1.7408\n",
      "Epoch 74/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6900 - val_loss: 1.7435\n",
      "Epoch 75/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7070 - val_loss: 1.7317\n",
      "Epoch 76/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7218 - val_loss: 1.7350\n",
      "Epoch 77/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6840 - val_loss: 1.7344\n",
      "Epoch 78/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7157 - val_loss: 1.7410\n",
      "Epoch 79/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6724 - val_loss: 1.7372\n",
      "Epoch 80/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6777 - val_loss: 1.7530\n",
      "Epoch 00080: early stopping\n",
      "0.85\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 1.1344 - val_loss: 1.1442\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1173 - val_loss: 1.1542\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1210 - val_loss: 1.1406\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1125 - val_loss: 1.1423\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1103 - val_loss: 1.1424\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1204 - val_loss: 1.1437\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1139 - val_loss: 1.1397\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0988 - val_loss: 1.1534\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0895 - val_loss: 1.1368\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1124 - val_loss: 1.1431\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1143 - val_loss: 1.1362\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1207 - val_loss: 1.1410\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1214 - val_loss: 1.1527\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1147 - val_loss: 1.1388\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1109 - val_loss: 1.1365\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1320 - val_loss: 1.1360\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0844 - val_loss: 1.1339\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1057 - val_loss: 1.1467\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1202 - val_loss: 1.1365\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0881 - val_loss: 1.1369\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1169 - val_loss: 1.1353\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1244 - val_loss: 1.1326\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0959 - val_loss: 1.1351\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1163 - val_loss: 1.1364\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0937 - val_loss: 1.1409\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1091 - val_loss: 1.1504\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1172 - val_loss: 1.1299\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1125 - val_loss: 1.1348\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1155 - val_loss: 1.1334\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1054 - val_loss: 1.1382\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0965 - val_loss: 1.1327\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1155 - val_loss: 1.1313\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1187 - val_loss: 1.1308\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0936 - val_loss: 1.1321\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1016 - val_loss: 1.1457\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0938 - val_loss: 1.1370\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0957 - val_loss: 1.1311\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1265 - val_loss: 1.1353\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1004 - val_loss: 1.1316\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1173 - val_loss: 1.1285\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0999 - val_loss: 1.1327\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0912 - val_loss: 1.1303\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0953 - val_loss: 1.1299\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1091 - val_loss: 1.1366\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1012 - val_loss: 1.1337\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1141 - val_loss: 1.1403\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1038 - val_loss: 1.1379\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0991 - val_loss: 1.1335\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1006 - val_loss: 1.1363\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1031 - val_loss: 1.1380\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0894 - val_loss: 1.1343\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0980 - val_loss: 1.1474\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1132 - val_loss: 1.1330\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1158 - val_loss: 1.1367\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0926 - val_loss: 1.1452\n",
      "Epoch 00055: early stopping\n",
      "3\n",
      "0.05\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.0583 - val_loss: 0.8092\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.8066 - val_loss: 0.7815\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7936 - val_loss: 0.7782\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7861 - val_loss: 0.7785\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7774 - val_loss: 0.7755\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7687 - val_loss: 0.7765\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7763 - val_loss: 0.7757\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7742 - val_loss: 0.7730\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7718 - val_loss: 0.7767\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7761 - val_loss: 0.7780\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7727 - val_loss: 0.7754\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7746 - val_loss: 0.7715\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7825 - val_loss: 0.7730\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7727 - val_loss: 0.7729\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7732 - val_loss: 0.7686\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7794 - val_loss: 0.7703\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7665 - val_loss: 0.7711\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7617 - val_loss: 0.7707\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7680 - val_loss: 0.7718\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7755 - val_loss: 0.7704\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7789 - val_loss: 0.7713\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7707 - val_loss: 0.7698\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7809 - val_loss: 0.7708\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7685 - val_loss: 0.7713\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7770 - val_loss: 0.7751\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7791 - val_loss: 0.7720\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7767 - val_loss: 0.7697\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7673 - val_loss: 0.7695\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7668 - val_loss: 0.7712\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7698 - val_loss: 0.7718\n",
      "Epoch 00030: early stopping\n",
      "0.15\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 1.9566 - val_loss: 1.8957\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9373 - val_loss: 1.8991\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9160 - val_loss: 1.8945\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9106 - val_loss: 1.9005\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9123 - val_loss: 1.9116\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9170 - val_loss: 1.8914\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8867 - val_loss: 1.8934\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8772 - val_loss: 1.9001\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9131 - val_loss: 1.9034\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8751 - val_loss: 1.9053\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8989 - val_loss: 1.8982\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9307 - val_loss: 1.8917\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9227 - val_loss: 1.8932\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8775 - val_loss: 1.8970\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8748 - val_loss: 1.8979\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8936 - val_loss: 1.9023\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8680 - val_loss: 1.9010\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9083 - val_loss: 1.8882\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8987 - val_loss: 1.8915\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8914 - val_loss: 1.8952\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8952 - val_loss: 1.8950\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8780 - val_loss: 1.8999\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8489 - val_loss: 1.8997\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9296 - val_loss: 1.9023\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9028 - val_loss: 1.9032\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8845 - val_loss: 1.9066\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8919 - val_loss: 1.8886\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8574 - val_loss: 1.8973\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8941 - val_loss: 1.8900\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8849 - val_loss: 1.8953\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8699 - val_loss: 1.8927\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8872 - val_loss: 1.8995\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8775 - val_loss: 1.8954\n",
      "Epoch 00033: early stopping\n",
      "0.25\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.5563 - val_loss: 2.5746\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5639 - val_loss: 2.5704\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5293 - val_loss: 2.5823\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5217 - val_loss: 2.5654\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5511 - val_loss: 2.5757\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5163 - val_loss: 2.5709\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5538 - val_loss: 2.5603\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5319 - val_loss: 2.5598\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5376 - val_loss: 2.5781\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5310 - val_loss: 2.5676\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5566 - val_loss: 2.5578\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5521 - val_loss: 2.5599\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5045 - val_loss: 2.5791\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5102 - val_loss: 2.5777\n",
      "Epoch 15/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5222 - val_loss: 2.5758\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5428 - val_loss: 2.5640\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5449 - val_loss: 2.5634\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5552 - val_loss: 2.5644\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5729 - val_loss: 2.5786\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5505 - val_loss: 2.5612\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5327 - val_loss: 2.5606\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5571 - val_loss: 2.5604\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5234 - val_loss: 2.5618\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5061 - val_loss: 2.5633\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5328 - val_loss: 2.5643\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5213 - val_loss: 2.5784\n",
      "Epoch 00026: early stopping\n",
      "0.35\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.8683 - val_loss: 2.8613\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8168 - val_loss: 2.8491\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8517 - val_loss: 2.8397\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7884 - val_loss: 2.8511\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8172 - val_loss: 2.8506\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8137 - val_loss: 2.8450\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8530 - val_loss: 2.8595\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8257 - val_loss: 2.8535\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8443 - val_loss: 2.8472\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8557 - val_loss: 2.8436\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7709 - val_loss: 2.8627\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7997 - val_loss: 2.8455\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7912 - val_loss: 2.8714\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7989 - val_loss: 2.8477\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7883 - val_loss: 2.8445\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8498 - val_loss: 2.8428\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8126 - val_loss: 2.8368\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7863 - val_loss: 2.8547\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8509 - val_loss: 2.8394\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8123 - val_loss: 2.8392\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8190 - val_loss: 2.8389\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8271 - val_loss: 2.8548\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7902 - val_loss: 2.8481\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7726 - val_loss: 2.8321\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7608 - val_loss: 2.8380\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7526 - val_loss: 2.8564\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8041 - val_loss: 2.8536\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7834 - val_loss: 2.8376\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8051 - val_loss: 2.8547\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8330 - val_loss: 2.8421\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7998 - val_loss: 2.8476\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7980 - val_loss: 2.8373\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8043 - val_loss: 2.8349\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8124 - val_loss: 2.8651\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7977 - val_loss: 2.8388\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7642 - val_loss: 2.8377\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7709 - val_loss: 2.8432\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7930 - val_loss: 2.8749\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8229 - val_loss: 2.8448\n",
      "Epoch 00039: early stopping\n",
      "0.45\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.8471 - val_loss: 2.8367\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7659 - val_loss: 2.8271\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7754 - val_loss: 2.8240\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8065 - val_loss: 2.8268\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7944 - val_loss: 2.8307\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7987 - val_loss: 2.8286\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7693 - val_loss: 2.8249\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8030 - val_loss: 2.8309\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8229 - val_loss: 2.8320\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7760 - val_loss: 2.8391\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8348 - val_loss: 2.8285\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8280 - val_loss: 2.8360\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7562 - val_loss: 2.8354\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8295 - val_loss: 2.8475\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7828 - val_loss: 2.8314\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7529 - val_loss: 2.8253\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8140 - val_loss: 2.8192\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7825 - val_loss: 2.8255\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8057 - val_loss: 2.8232\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7734 - val_loss: 2.8488\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7945 - val_loss: 2.8238\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7974 - val_loss: 2.8232\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8071 - val_loss: 2.8269\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8037 - val_loss: 2.8288\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8022 - val_loss: 2.8273\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7893 - val_loss: 2.8295\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8175 - val_loss: 2.8292\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7827 - val_loss: 2.8348\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7921 - val_loss: 2.8390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7273 - val_loss: 2.8431\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7634 - val_loss: 2.8519\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8007 - val_loss: 2.8320\n",
      "Epoch 00032: early stopping\n",
      "0.55\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.6028 - val_loss: 2.6036\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6135 - val_loss: 2.6009\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6147 - val_loss: 2.6032\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6027 - val_loss: 2.5985\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5801 - val_loss: 2.5979\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5589 - val_loss: 2.6030\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5717 - val_loss: 2.6042\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6000 - val_loss: 2.5961\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5416 - val_loss: 2.6024\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6154 - val_loss: 2.5955\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6109 - val_loss: 2.6063\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5460 - val_loss: 2.5978\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5878 - val_loss: 2.5975\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6144 - val_loss: 2.5926\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6109 - val_loss: 2.6036\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6111 - val_loss: 2.6009\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5782 - val_loss: 2.6001\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5967 - val_loss: 2.5979\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5566 - val_loss: 2.5960\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5636 - val_loss: 2.5918\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5703 - val_loss: 2.5954\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5922 - val_loss: 2.5992\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5430 - val_loss: 2.5932\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5917 - val_loss: 2.5985\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5727 - val_loss: 2.5906\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5419 - val_loss: 2.5941\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5580 - val_loss: 2.5931\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5937 - val_loss: 2.5890\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5718 - val_loss: 2.6242\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5689 - val_loss: 2.5940\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5523 - val_loss: 2.5889\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5951 - val_loss: 2.5924\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5570 - val_loss: 2.5985\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5517 - val_loss: 2.6035\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5244 - val_loss: 2.5998\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5900 - val_loss: 2.5916\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5925 - val_loss: 2.5912\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5138 - val_loss: 2.5962\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5750 - val_loss: 2.5940\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6118 - val_loss: 2.5970\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5506 - val_loss: 2.5986\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5706 - val_loss: 2.6015\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5727 - val_loss: 2.6021\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6059 - val_loss: 2.5945\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5703 - val_loss: 2.5975\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5974 - val_loss: 2.5966\n",
      "Epoch 00046: early stopping\n",
      "0.65\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.2190 - val_loss: 2.2141\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2045 - val_loss: 2.2027\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1990 - val_loss: 2.2231\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1927 - val_loss: 2.2171\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1248 - val_loss: 2.2008\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2313 - val_loss: 2.2151\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1848 - val_loss: 2.2006\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1896 - val_loss: 2.2245\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2379 - val_loss: 2.2153\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2062 - val_loss: 2.2046\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1744 - val_loss: 2.2018\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1658 - val_loss: 2.2103\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2714 - val_loss: 2.2141\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1764 - val_loss: 2.2051\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1837 - val_loss: 2.2243\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1783 - val_loss: 2.2018\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2035 - val_loss: 2.1974\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2032 - val_loss: 2.2027\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1648 - val_loss: 2.2166\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1784 - val_loss: 2.2029\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1898 - val_loss: 2.2000\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1871 - val_loss: 2.2033\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1541 - val_loss: 2.2089\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2037 - val_loss: 2.1969\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2534 - val_loss: 2.2016\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2191 - val_loss: 2.1992\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2221 - val_loss: 2.1971\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1920 - val_loss: 2.2071\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2286 - val_loss: 2.1954\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1975 - val_loss: 2.1979\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1614 - val_loss: 2.2075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1532 - val_loss: 2.1973\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1821 - val_loss: 2.1952\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2066 - val_loss: 2.1945\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2122 - val_loss: 2.1896\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2106 - val_loss: 2.1944\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1915 - val_loss: 2.2018\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1986 - val_loss: 2.1900\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1890 - val_loss: 2.1986\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2174 - val_loss: 2.2016\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1705 - val_loss: 2.2040\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1993 - val_loss: 2.1975\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2074 - val_loss: 2.1980\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1989 - val_loss: 2.1955\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1681 - val_loss: 2.1897\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1888 - val_loss: 2.2004\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1513 - val_loss: 2.1907\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1976 - val_loss: 2.2119\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1987 - val_loss: 2.1974\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1910 - val_loss: 2.2025\n",
      "Epoch 00050: early stopping\n",
      "0.75\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 1.7081 - val_loss: 1.7023\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7112 - val_loss: 1.7048\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6837 - val_loss: 1.7030\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7069 - val_loss: 1.6971\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7008 - val_loss: 1.6970\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7134 - val_loss: 1.6969\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7018 - val_loss: 1.6938\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6931 - val_loss: 1.6951\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6918 - val_loss: 1.6979\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6884 - val_loss: 1.6934\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7001 - val_loss: 1.6940\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6922 - val_loss: 1.7000\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7019 - val_loss: 1.7050\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7160 - val_loss: 1.6994\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7012 - val_loss: 1.6917\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6813 - val_loss: 1.6926\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7113 - val_loss: 1.6907\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7001 - val_loss: 1.6947\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7044 - val_loss: 1.6948\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7279 - val_loss: 1.7044\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7166 - val_loss: 1.6930\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7114 - val_loss: 1.7016\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7227 - val_loss: 1.6988\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6815 - val_loss: 1.6963\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6611 - val_loss: 1.6966\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6955 - val_loss: 1.6917\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7135 - val_loss: 1.6999\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6751 - val_loss: 1.6985\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7128 - val_loss: 1.6957\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6976 - val_loss: 1.6943\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6734 - val_loss: 1.6987\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7155 - val_loss: 1.6926\n",
      "Epoch 00032: early stopping\n",
      "0.85\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.1172 - val_loss: 1.1182\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0869 - val_loss: 1.1072\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1120 - val_loss: 1.1170\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1100 - val_loss: 1.1098\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1170 - val_loss: 1.1096\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1005 - val_loss: 1.1100\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1194 - val_loss: 1.1109\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1052 - val_loss: 1.1196\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1214 - val_loss: 1.1088\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1034 - val_loss: 1.1086\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0924 - val_loss: 1.1091\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1059 - val_loss: 1.1180\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1207 - val_loss: 1.1084\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1182 - val_loss: 1.1083\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1339 - val_loss: 1.1082\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1054 - val_loss: 1.1158\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1270 - val_loss: 1.1067\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1015 - val_loss: 1.1115\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0863 - val_loss: 1.1080\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1175 - val_loss: 1.1109\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1158 - val_loss: 1.1148\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1120 - val_loss: 1.1109\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1120 - val_loss: 1.1086\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1001 - val_loss: 1.1070\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1160 - val_loss: 1.1070\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0958 - val_loss: 1.1134\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0945 - val_loss: 1.1050\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1157 - val_loss: 1.1099\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0958 - val_loss: 1.1156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0907 - val_loss: 1.1096\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1111 - val_loss: 1.1059\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1027 - val_loss: 1.1060\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1070 - val_loss: 1.1125\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0917 - val_loss: 1.1115\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1078 - val_loss: 1.1112\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1047 - val_loss: 1.1146\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1179 - val_loss: 1.1160\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0934 - val_loss: 1.1057\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1041 - val_loss: 1.1208\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1104 - val_loss: 1.1174\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0955 - val_loss: 1.1059\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1104 - val_loss: 1.1119\n",
      "Epoch 00042: early stopping\n",
      "4\n",
      "0.05\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.1605 - val_loss: 0.8211\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7964 - val_loss: 0.7921\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7788 - val_loss: 0.7898\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7709 - val_loss: 0.7842\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7844 - val_loss: 0.7856\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7888 - val_loss: 0.7834\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7803 - val_loss: 0.7849\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7784 - val_loss: 0.7819\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7747 - val_loss: 0.7824\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7803 - val_loss: 0.7814\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7786 - val_loss: 0.7811\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7749 - val_loss: 0.7804\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7648 - val_loss: 0.7814\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7704 - val_loss: 0.7814\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7649 - val_loss: 0.7804\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7734 - val_loss: 0.7834\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7681 - val_loss: 0.7792\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7750 - val_loss: 0.7828\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7793 - val_loss: 0.7801\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7722 - val_loss: 0.7841\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7763 - val_loss: 0.7798\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7685 - val_loss: 0.7829\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7653 - val_loss: 0.7821\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7734 - val_loss: 0.7796\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7694 - val_loss: 0.7802\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7688 - val_loss: 0.7791\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7595 - val_loss: 0.7796\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7668 - val_loss: 0.7764\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7735 - val_loss: 0.7856\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7642 - val_loss: 0.7796\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7706 - val_loss: 0.7790\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7697 - val_loss: 0.7815\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7779 - val_loss: 0.7862\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7761 - val_loss: 0.7790\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7691 - val_loss: 0.7789\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7626 - val_loss: 0.7811\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7735 - val_loss: 0.7804\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7725 - val_loss: 0.7816\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7661 - val_loss: 0.7782\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7639 - val_loss: 0.7800\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7632 - val_loss: 0.7773\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7633 - val_loss: 0.7795\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 0.7685 - val_loss: 0.7808\n",
      "Epoch 00043: early stopping\n",
      "0.15\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.9573 - val_loss: 1.9464\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9087 - val_loss: 1.9269\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8915 - val_loss: 1.9201\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9036 - val_loss: 1.9262\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9040 - val_loss: 1.9239\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9037 - val_loss: 1.9306\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8873 - val_loss: 1.9268\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8839 - val_loss: 1.9263\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8433 - val_loss: 1.9256\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8902 - val_loss: 1.9226\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8843 - val_loss: 1.9192\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8867 - val_loss: 1.9224\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9089 - val_loss: 1.9112\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8935 - val_loss: 1.9074\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8878 - val_loss: 1.9231\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8739 - val_loss: 1.9188\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8667 - val_loss: 1.9166\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.9127 - val_loss: 1.9144\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8876 - val_loss: 1.9019\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8918 - val_loss: 1.9063\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8912 - val_loss: 1.9064\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8879 - val_loss: 1.9134\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8676 - val_loss: 1.9157\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8742 - val_loss: 1.9123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8688 - val_loss: 1.9202\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8670 - val_loss: 1.9165\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8998 - val_loss: 1.9143\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8731 - val_loss: 1.9032\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8936 - val_loss: 1.9184\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8705 - val_loss: 1.9097\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8489 - val_loss: 1.9101\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8485 - val_loss: 1.9122\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8763 - val_loss: 1.9099\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.8824 - val_loss: 1.9025\n",
      "Epoch 00034: early stopping\n",
      "0.25\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.5473 - val_loss: 2.5560\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5217 - val_loss: 2.5603\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5416 - val_loss: 2.5594\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5560 - val_loss: 2.5576\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5363 - val_loss: 2.5513\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5344 - val_loss: 2.5704\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5287 - val_loss: 2.5623\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5101 - val_loss: 2.5478\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5455 - val_loss: 2.5500\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5618 - val_loss: 2.5631\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5012 - val_loss: 2.5644\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5543 - val_loss: 2.5573\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5541 - val_loss: 2.5651\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5196 - val_loss: 2.5669\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5206 - val_loss: 2.5617\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5335 - val_loss: 2.5555\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5763 - val_loss: 2.5705\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5368 - val_loss: 2.5650\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5631 - val_loss: 2.5519\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5222 - val_loss: 2.5525\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5256 - val_loss: 2.5594\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5412 - val_loss: 2.5627\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5141 - val_loss: 2.5532\n",
      "Epoch 00023: early stopping\n",
      "0.35\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.7780 - val_loss: 2.8453\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8113 - val_loss: 2.8461\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8476 - val_loss: 2.8284\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8242 - val_loss: 2.8316\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8165 - val_loss: 2.8332\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8196 - val_loss: 2.8246\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8073 - val_loss: 2.8247\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8343 - val_loss: 2.8288\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7920 - val_loss: 2.8294\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8024 - val_loss: 2.8213\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7925 - val_loss: 2.8209\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8102 - val_loss: 2.8280\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8148 - val_loss: 2.8276\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8014 - val_loss: 2.8253\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7759 - val_loss: 2.8298\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8223 - val_loss: 2.8244\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7994 - val_loss: 2.8323\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8325 - val_loss: 2.8249\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7748 - val_loss: 2.8177\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8071 - val_loss: 2.8484\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7862 - val_loss: 2.8265\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7803 - val_loss: 2.8257\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8367 - val_loss: 2.8299\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8025 - val_loss: 2.8234\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - ETA: 0s - loss: 2.812 - 1s 2ms/step - loss: 2.8124 - val_loss: 2.8250\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8198 - val_loss: 2.8160\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8369 - val_loss: 2.8707\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8012 - val_loss: 2.8255\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7952 - val_loss: 2.8284\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7947 - val_loss: 2.8195\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8340 - val_loss: 2.8467\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8025 - val_loss: 2.8287\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8545 - val_loss: 2.8278\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7957 - val_loss: 2.8297\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8091 - val_loss: 2.8121\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8165 - val_loss: 2.8166\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7969 - val_loss: 2.8326\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7672 - val_loss: 2.8289\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7968 - val_loss: 2.8196\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8244 - val_loss: 2.8296\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7809 - val_loss: 2.8342\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8118 - val_loss: 2.8225\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7952 - val_loss: 2.8261\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7891 - val_loss: 2.8246\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7583 - val_loss: 2.8271\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7980 - val_loss: 2.8246\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8040 - val_loss: 2.8397\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7804 - val_loss: 2.8368\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7438 - val_loss: 2.8354\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8056 - val_loss: 2.8246\n",
      "Epoch 00050: early stopping\n",
      "0.45\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 2.8184 - val_loss: 2.8293\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7796 - val_loss: 2.8330\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8515 - val_loss: 2.8434\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8257 - val_loss: 2.8261\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8016 - val_loss: 2.8196\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8211 - val_loss: 2.8313\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7918 - val_loss: 2.8224\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8055 - val_loss: 2.8141\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8384 - val_loss: 2.8282\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8385 - val_loss: 2.8129\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7827 - val_loss: 2.8167\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7700 - val_loss: 2.8279\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8442 - val_loss: 2.8318\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8386 - val_loss: 2.8167\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7550 - val_loss: 2.8204\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8142 - val_loss: 2.8121\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8009 - val_loss: 2.8608\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8654 - val_loss: 2.8179\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7690 - val_loss: 2.8180\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7916 - val_loss: 2.8104\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7824 - val_loss: 2.8108\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8019 - val_loss: 2.8184\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8501 - val_loss: 2.8059\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8169 - val_loss: 2.8160\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7690 - val_loss: 2.8131\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7686 - val_loss: 2.8076\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7657 - val_loss: 2.8217\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7700 - val_loss: 2.8202\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7718 - val_loss: 2.8163\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7830 - val_loss: 2.8175\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7759 - val_loss: 2.8174\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8146 - val_loss: 2.8179\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8008 - val_loss: 2.8077\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7735 - val_loss: 2.8195\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7586 - val_loss: 2.8112\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8227 - val_loss: 2.8168\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7133 - val_loss: 2.8208\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8005 - val_loss: 2.7992\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8077 - val_loss: 2.8118\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7819 - val_loss: 2.8214\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8035 - val_loss: 2.8624\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8362 - val_loss: 2.8019\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8319 - val_loss: 2.8060\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7628 - val_loss: 2.8053\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7779 - val_loss: 2.8103\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8001 - val_loss: 2.8011\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8410 - val_loss: 2.8116\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8162 - val_loss: 2.8133\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8248 - val_loss: 2.8128\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8156 - val_loss: 2.8129\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.8084 - val_loss: 2.8012\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7540 - val_loss: 2.8074\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.7536 - val_loss: 2.8045\n",
      "Epoch 00053: early stopping\n",
      "0.55\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 3ms/step - loss: 2.6378 - val_loss: 2.5929\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5802 - val_loss: 2.5988\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5640 - val_loss: 2.5865\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5918 - val_loss: 2.5891\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6172 - val_loss: 2.5828\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5741 - val_loss: 2.5842\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6300 - val_loss: 2.5783\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5856 - val_loss: 2.5887\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5886 - val_loss: 2.5966\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5850 - val_loss: 2.6057\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6174 - val_loss: 2.5911\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5593 - val_loss: 2.5815\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5330 - val_loss: 2.6010\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5994 - val_loss: 2.5815\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5309 - val_loss: 2.5767\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5789 - val_loss: 2.5827\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5839 - val_loss: 2.6028\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6167 - val_loss: 2.5863\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5776 - val_loss: 2.5840\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6008 - val_loss: 2.5865\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5657 - val_loss: 2.5804\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5885 - val_loss: 2.5926\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6020 - val_loss: 2.5803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5457 - val_loss: 2.5847\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5812 - val_loss: 2.5760\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5616 - val_loss: 2.5754\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6252 - val_loss: 2.5848\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5679 - val_loss: 2.5763\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5898 - val_loss: 2.5785\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5894 - val_loss: 2.5824\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5733 - val_loss: 2.5771\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5905 - val_loss: 2.5800\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5764 - val_loss: 2.5808\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6091 - val_loss: 2.5802\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5901 - val_loss: 2.5788\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5131 - val_loss: 2.5807\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5668 - val_loss: 2.5923\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5399 - val_loss: 2.5841\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5862 - val_loss: 2.5965\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5785 - val_loss: 2.5739\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5780 - val_loss: 2.5811\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5519 - val_loss: 2.5821\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.6089 - val_loss: 2.5915\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5515 - val_loss: 2.5788\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5518 - val_loss: 2.5752\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5478 - val_loss: 2.5839\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5786 - val_loss: 2.5841\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5641 - val_loss: 2.5685\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5619 - val_loss: 2.5789\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5927 - val_loss: 2.5932\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5482 - val_loss: 2.5786\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5498 - val_loss: 2.5754\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5586 - val_loss: 2.5818\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5302 - val_loss: 2.5821\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5721 - val_loss: 2.5765\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5746 - val_loss: 2.5778\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5430 - val_loss: 2.5778\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5062 - val_loss: 2.5767\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5583 - val_loss: 2.5843\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5720 - val_loss: 2.5766\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5688 - val_loss: 2.5716\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5980 - val_loss: 2.5804\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.5964 - val_loss: 2.5768\n",
      "Epoch 00063: early stopping\n",
      "0.65\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 3s 3ms/step - loss: 2.2298 - val_loss: 2.2100\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1506 - val_loss: 2.2047\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1671 - val_loss: 2.1995\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1970 - val_loss: 2.2058\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1999 - val_loss: 2.2035\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1597 - val_loss: 2.2094\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1596 - val_loss: 2.1985\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1605 - val_loss: 2.1970\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2037 - val_loss: 2.2143\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2428 - val_loss: 2.1962\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1904 - val_loss: 2.2018\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1725 - val_loss: 2.2090\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2460 - val_loss: 2.2039\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2271 - val_loss: 2.2166\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2244 - val_loss: 2.2029\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1898 - val_loss: 2.2134\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2192 - val_loss: 2.1988\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2064 - val_loss: 2.2065\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2236 - val_loss: 2.1983\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2043 - val_loss: 2.2033\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1694 - val_loss: 2.2034\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2024 - val_loss: 2.2004\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1933 - val_loss: 2.2064\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.1976 - val_loss: 2.2036\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 2.2124 - val_loss: 2.2027\n",
      "Epoch 00025: early stopping\n",
      "0.75\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.6792 - val_loss: 1.7227\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7133 - val_loss: 1.7173\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7208 - val_loss: 1.7224\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7254 - val_loss: 1.7349\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7052 - val_loss: 1.7131\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6931 - val_loss: 1.7140\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6958 - val_loss: 1.7122\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7232 - val_loss: 1.7160\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6865 - val_loss: 1.7167\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7154 - val_loss: 1.7124\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6886 - val_loss: 1.7084\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7030 - val_loss: 1.7122\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6854 - val_loss: 1.7118\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6828 - val_loss: 1.7066\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6955 - val_loss: 1.7082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7167 - val_loss: 1.7250\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7018 - val_loss: 1.7085\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7205 - val_loss: 1.7109\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7185 - val_loss: 1.7071\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6817 - val_loss: 1.7103\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6887 - val_loss: 1.7105\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6796 - val_loss: 1.7128\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7059 - val_loss: 1.7069\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7073 - val_loss: 1.7211\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7050 - val_loss: 1.7082\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.6993 - val_loss: 1.7278\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7055 - val_loss: 1.7109\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7155 - val_loss: 1.7219\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.7261 - val_loss: 1.7068\n",
      "Epoch 00029: early stopping\n",
      "0.85\n",
      "Epoch 1/500\n",
      "328/328 [==============================] - 2s 2ms/step - loss: 1.1175 - val_loss: 1.1344\n",
      "Epoch 2/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1039 - val_loss: 1.1221\n",
      "Epoch 3/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1132 - val_loss: 1.1268\n",
      "Epoch 4/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1005 - val_loss: 1.1270\n",
      "Epoch 5/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1464 - val_loss: 1.1214\n",
      "Epoch 6/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0903 - val_loss: 1.1276\n",
      "Epoch 7/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1159 - val_loss: 1.1233\n",
      "Epoch 8/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1169 - val_loss: 1.1263\n",
      "Epoch 9/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1049 - val_loss: 1.1245\n",
      "Epoch 10/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1140 - val_loss: 1.1202\n",
      "Epoch 11/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1330 - val_loss: 1.1199\n",
      "Epoch 12/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0968 - val_loss: 1.1276\n",
      "Epoch 13/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1023 - val_loss: 1.1208\n",
      "Epoch 14/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1083 - val_loss: 1.1192\n",
      "Epoch 15/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1038 - val_loss: 1.1176\n",
      "Epoch 16/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1019 - val_loss: 1.1234\n",
      "Epoch 17/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1146 - val_loss: 1.1179\n",
      "Epoch 18/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1238 - val_loss: 1.1490\n",
      "Epoch 19/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1172 - val_loss: 1.1177\n",
      "Epoch 20/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0979 - val_loss: 1.1206\n",
      "Epoch 21/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1165 - val_loss: 1.1273\n",
      "Epoch 22/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1185 - val_loss: 1.1378\n",
      "Epoch 23/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0929 - val_loss: 1.1193\n",
      "Epoch 24/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0986 - val_loss: 1.1200\n",
      "Epoch 25/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1229 - val_loss: 1.1204\n",
      "Epoch 26/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1221 - val_loss: 1.1179\n",
      "Epoch 27/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1175 - val_loss: 1.1169\n",
      "Epoch 28/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1235 - val_loss: 1.1210\n",
      "Epoch 29/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1028 - val_loss: 1.1159\n",
      "Epoch 30/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1058 - val_loss: 1.1153\n",
      "Epoch 31/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0946 - val_loss: 1.1151\n",
      "Epoch 32/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1075 - val_loss: 1.1138\n",
      "Epoch 33/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1065 - val_loss: 1.1171\n",
      "Epoch 34/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0962 - val_loss: 1.1309\n",
      "Epoch 35/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0910 - val_loss: 1.1138\n",
      "Epoch 36/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1232 - val_loss: 1.1144\n",
      "Epoch 37/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1198 - val_loss: 1.1158\n",
      "Epoch 38/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1083 - val_loss: 1.1130\n",
      "Epoch 39/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1065 - val_loss: 1.1199\n",
      "Epoch 40/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1271 - val_loss: 1.1178\n",
      "Epoch 41/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1140 - val_loss: 1.1133\n",
      "Epoch 42/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0833 - val_loss: 1.1189\n",
      "Epoch 43/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1016 - val_loss: 1.1139\n",
      "Epoch 44/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1040 - val_loss: 1.1191\n",
      "Epoch 45/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1098 - val_loss: 1.1122\n",
      "Epoch 46/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0997 - val_loss: 1.1176\n",
      "Epoch 47/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0992 - val_loss: 1.1138\n",
      "Epoch 48/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0987 - val_loss: 1.1201\n",
      "Epoch 49/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1009 - val_loss: 1.1177\n",
      "Epoch 50/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1063 - val_loss: 1.1320\n",
      "Epoch 51/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0923 - val_loss: 1.1241\n",
      "Epoch 52/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0886 - val_loss: 1.1217\n",
      "Epoch 53/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0991 - val_loss: 1.1168\n",
      "Epoch 54/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1158 - val_loss: 1.1170\n",
      "Epoch 55/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1093 - val_loss: 1.1100\n",
      "Epoch 56/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1084 - val_loss: 1.1175\n",
      "Epoch 57/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1017 - val_loss: 1.1144\n",
      "Epoch 58/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1054 - val_loss: 1.1083\n",
      "Epoch 59/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0904 - val_loss: 1.1082\n",
      "Epoch 60/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0976 - val_loss: 1.1104\n",
      "Epoch 61/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0889 - val_loss: 1.1166\n",
      "Epoch 62/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0975 - val_loss: 1.1162\n",
      "Epoch 63/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0932 - val_loss: 1.1121\n",
      "Epoch 64/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1054 - val_loss: 1.1231\n",
      "Epoch 65/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1152 - val_loss: 1.1127\n",
      "Epoch 66/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0966 - val_loss: 1.1111\n",
      "Epoch 67/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1102 - val_loss: 1.1169\n",
      "Epoch 68/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1081 - val_loss: 1.1208\n",
      "Epoch 69/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0977 - val_loss: 1.1175\n",
      "Epoch 70/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1210 - val_loss: 1.1106\n",
      "Epoch 71/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0937 - val_loss: 1.1282\n",
      "Epoch 72/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.0890 - val_loss: 1.1132\n",
      "Epoch 73/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1063 - val_loss: 1.1149\n",
      "Epoch 74/500\n",
      "328/328 [==============================] - 1s 2ms/step - loss: 1.1227 - val_loss: 1.1128\n",
      "Epoch 00074: early stopping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3883</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3885</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3887</th>\n",
       "      <td>-0.00082</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3888 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        q_0.1     q_0.2     q_0.3     q_0.4     q_0.5     q_0.6     q_0.7  \\\n",
       "0    -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "1    -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "2    -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "3    -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "4    -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "...       ...       ...       ...       ...       ...       ...       ...   \n",
       "3883 -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "3884 -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "3885 -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "3886 -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "3887 -0.00082 -0.000278 -0.000117 -0.000086 -0.000142  0.000019  0.000097   \n",
       "\n",
       "         q_0.8     q_0.9  \n",
       "0     0.000237  0.000321  \n",
       "1     0.000237  0.000321  \n",
       "2     0.000237  0.000321  \n",
       "3     0.000237  0.000321  \n",
       "4     0.000237  0.000321  \n",
       "...        ...       ...  \n",
       "3883  0.000237  0.000321  \n",
       "3884  0.000237  0.000321  \n",
       "3885  0.000237  0.000321  \n",
       "3886  0.000237  0.000321  \n",
       "3887  0.000237  0.000321  \n",
       "\n",
       "[3888 rows x 9 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_list = [0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85]\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "pred_2 = pd.DataFrame()\n",
    "for n_fold, (train_idx, val_idx) in enumerate(folds.split(x_train_2_scaler)):\n",
    "    train_x, train_y = x_train_2_scaler.loc[train_idx], y_train_2.loc[train_idx]\n",
    "    val_x, val_y = x_train_2_scaler.loc[val_idx], y_train_2.loc[val_idx]\n",
    "    print(n_fold)\n",
    "    for j in quantile_list:\n",
    "        print(j)\n",
    "        quantile = j\n",
    "        model.compile(loss=lambda y,f: tilted_loss(quantile,y,f), optimizer='adam')\n",
    "        model.fit(train_x, train_y, validation_data = (val_x, val_y), epochs=500,  batch_size=128, callbacks=[es])\n",
    "        pred_2 = pd.concat([pred_2, pd.DataFrame(model.predict(x_test_2_scaler))], axis = 1)\n",
    "        \n",
    "result_1 = pred_2.iloc[:,0:9]\n",
    "result_2 = pred_2.iloc[:,9:18]\n",
    "result_3 = pred_2.iloc[:,18:27]\n",
    "result_4 = pred_2.iloc[:,27:36]\n",
    "result_5 = pred_2.iloc[:,36:45]\n",
    "\n",
    "result = (result_1+result_2+result_3+result_4+result_5)/5\n",
    "result.columns = sub.iloc[:,1:].columns\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[sub.id.str.contains(\"Day8\"), \"q_0.1\":] = result.sort_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.642376</td>\n",
       "      <td>10.690382</td>\n",
       "      <td>14.508972</td>\n",
       "      <td>17.311780</td>\n",
       "      <td>19.353778</td>\n",
       "      <td>20.858315</td>\n",
       "      <td>21.712103</td>\n",
       "      <td>22.673416</td>\n",
       "      <td>23.360904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.075053</td>\n",
       "      <td>15.507614</td>\n",
       "      <td>20.047825</td>\n",
       "      <td>23.376371</td>\n",
       "      <td>25.763067</td>\n",
       "      <td>27.504193</td>\n",
       "      <td>28.457959</td>\n",
       "      <td>29.446399</td>\n",
       "      <td>30.083211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000211</td>\n",
       "      <td>-0.000109</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000270</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.000117</td>\n",
       "      <td>0.071692</td>\n",
       "      <td>0.250250</td>\n",
       "      <td>0.588568</td>\n",
       "      <td>0.630258</td>\n",
       "      <td>1.113779</td>\n",
       "      <td>1.541545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.804232</td>\n",
       "      <td>19.122424</td>\n",
       "      <td>27.335909</td>\n",
       "      <td>33.868988</td>\n",
       "      <td>37.920055</td>\n",
       "      <td>41.390615</td>\n",
       "      <td>43.738308</td>\n",
       "      <td>45.976662</td>\n",
       "      <td>47.003334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.147179</td>\n",
       "      <td>68.184662</td>\n",
       "      <td>77.626648</td>\n",
       "      <td>86.494949</td>\n",
       "      <td>90.500992</td>\n",
       "      <td>94.835655</td>\n",
       "      <td>95.420494</td>\n",
       "      <td>97.376747</td>\n",
       "      <td>98.477737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             q_0.1        q_0.2        q_0.3        q_0.4        q_0.5  \\\n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  7776.000000   \n",
       "mean      5.642376    10.690382    14.508972    17.311780    19.353778   \n",
       "std       9.075053    15.507614    20.047825    23.376371    25.763067   \n",
       "min      -0.000820    -0.000278    -0.000211    -0.000109    -0.000142   \n",
       "25%      -0.000820    -0.000278    -0.000117    -0.000086     0.000061   \n",
       "50%      -0.000270    -0.000271    -0.000117     0.071692     0.250250   \n",
       "75%       8.804232    19.122424    27.335909    33.868988    37.920055   \n",
       "max      47.147179    68.184662    77.626648    86.494949    90.500992   \n",
       "\n",
       "             q_0.6        q_0.7        q_0.8        q_0.9  \n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  \n",
       "mean     20.858315    21.712103    22.673416    23.360904  \n",
       "std      27.504193    28.457959    29.446399    30.083211  \n",
       "min       0.000019     0.000097     0.000191     0.000273  \n",
       "25%       0.000144     0.000296     0.000237     0.000321  \n",
       "50%       0.588568     0.630258     1.113779     1.541545  \n",
       "75%      41.390615    43.738308    45.976662    47.003334  \n",
       "max      94.835655    95.420494    97.376747    98.477737  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:,1:] = abs(sub.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.642672</td>\n",
       "      <td>10.690472</td>\n",
       "      <td>14.509029</td>\n",
       "      <td>17.311816</td>\n",
       "      <td>19.353831</td>\n",
       "      <td>20.858302</td>\n",
       "      <td>21.712028</td>\n",
       "      <td>22.673332</td>\n",
       "      <td>23.360808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.074915</td>\n",
       "      <td>15.507473</td>\n",
       "      <td>20.047747</td>\n",
       "      <td>23.376328</td>\n",
       "      <td>25.763055</td>\n",
       "      <td>27.504275</td>\n",
       "      <td>28.458049</td>\n",
       "      <td>29.446488</td>\n",
       "      <td>30.083371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>1.115000</td>\n",
       "      <td>1.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.802500</td>\n",
       "      <td>19.120000</td>\n",
       "      <td>27.332500</td>\n",
       "      <td>33.870000</td>\n",
       "      <td>37.925000</td>\n",
       "      <td>41.392500</td>\n",
       "      <td>43.740000</td>\n",
       "      <td>45.975000</td>\n",
       "      <td>47.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47.150000</td>\n",
       "      <td>68.180000</td>\n",
       "      <td>77.630000</td>\n",
       "      <td>86.490000</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>94.840000</td>\n",
       "      <td>95.420000</td>\n",
       "      <td>97.380000</td>\n",
       "      <td>98.480000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             q_0.1        q_0.2        q_0.3        q_0.4        q_0.5  \\\n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  7776.000000   \n",
       "mean      5.642672    10.690472    14.509029    17.311816    19.353831   \n",
       "std       9.074915    15.507473    20.047747    23.376328    25.763055   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.075000     0.250000   \n",
       "75%       8.802500    19.120000    27.332500    33.870000    37.925000   \n",
       "max      47.150000    68.180000    77.630000    86.490000    90.500000   \n",
       "\n",
       "             q_0.6        q_0.7        q_0.8        q_0.9  \n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  \n",
       "mean     20.858302    21.712028    22.673332    23.360808  \n",
       "std      27.504275    28.458049    29.446488    30.083371  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.590000     0.630000     1.115000     1.545000  \n",
       "75%      41.392500    43.740000    45.975000    47.002500  \n",
       "max      94.840000    95.420000    97.380000    98.480000  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv('0126_second.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>80.csv_Day8_21h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>80.csv_Day8_22h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7773</th>\n",
       "      <td>80.csv_Day8_22h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7774</th>\n",
       "      <td>80.csv_Day8_23h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7775</th>\n",
       "      <td>80.csv_Day8_23h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7776 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  q_0.1  q_0.2  q_0.3  q_0.4  q_0.5  q_0.6  q_0.7  \\\n",
       "0       0.csv_Day7_0h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1       0.csv_Day7_0h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2       0.csv_Day7_1h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3       0.csv_Day7_1h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4       0.csv_Day7_2h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...                  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "7771  80.csv_Day8_21h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7772  80.csv_Day8_22h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7773  80.csv_Day8_22h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7774  80.csv_Day8_23h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7775  80.csv_Day8_23h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      q_0.8  q_0.9  \n",
       "0       0.0    0.0  \n",
       "1       0.0    0.0  \n",
       "2       0.0    0.0  \n",
       "3       0.0    0.0  \n",
       "4       0.0    0.0  \n",
       "...     ...    ...  \n",
       "7771    0.0    0.0  \n",
       "7772    0.0    0.0  \n",
       "7773    0.0    0.0  \n",
       "7774    0.0    0.0  \n",
       "7775    0.0    0.0  \n",
       "\n",
       "[7776 rows x 10 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in pred.index:\n",
    "    if pred['q_0.1'][i] < 0:\n",
    "        pred['q_0.1'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.2'][i] < 0:\n",
    "        pred['q_0.2'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.3'][i] < 0:\n",
    "        pred['q_0.3'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.4'][i] < 0:\n",
    "        pred['q_0.4'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.5'][i] < 0:\n",
    "        pred['q_0.5'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.6'][i] < 0:\n",
    "        pred['q_0.6'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.7'][i] < 0:\n",
    "        pred['q_0.7'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.8'][i] < 0:\n",
    "        pred['q_0.8'][i] = 0\n",
    "for i in pred.index:\n",
    "    if pred['q_0.9'][i] < 0:\n",
    "        pred['q_0.9'][i] = 0\n",
    "    \n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_neu = pred.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv_Day7_0h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.csv_Day7_0h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.csv_Day7_1h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.csv_Day7_1h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.csv_Day7_2h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>80.csv_Day8_21h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>80.csv_Day8_22h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7773</th>\n",
       "      <td>80.csv_Day8_22h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7774</th>\n",
       "      <td>80.csv_Day8_23h00m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7775</th>\n",
       "      <td>80.csv_Day8_23h30m</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7776 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  q_0.1  q_0.2  q_0.3  q_0.4  q_0.5  q_0.6  q_0.7  \\\n",
       "0       0.csv_Day7_0h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1       0.csv_Day7_0h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2       0.csv_Day7_1h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3       0.csv_Day7_1h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4       0.csv_Day7_2h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...                  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "7771  80.csv_Day8_21h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7772  80.csv_Day8_22h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7773  80.csv_Day8_22h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7774  80.csv_Day8_23h00m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "7775  80.csv_Day8_23h30m    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      q_0.8  q_0.9  \n",
       "0       0.0    0.0  \n",
       "1       0.0    0.0  \n",
       "2       0.0    0.0  \n",
       "3       0.0    0.0  \n",
       "4       0.0    0.0  \n",
       "...     ...    ...  \n",
       "7771    0.0    0.0  \n",
       "7772    0.0    0.0  \n",
       "7773    0.0    0.0  \n",
       "7774    0.0    0.0  \n",
       "7775    0.0    0.0  \n",
       "\n",
       "[7776 rows x 10 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.iloc[:,1:] = ((pred_lgbm.iloc[:,1:] + pred_neu.iloc[:,1:])/2).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_0.1</th>\n",
       "      <th>q_0.2</th>\n",
       "      <th>q_0.3</th>\n",
       "      <th>q_0.4</th>\n",
       "      <th>q_0.5</th>\n",
       "      <th>q_0.6</th>\n",
       "      <th>q_0.7</th>\n",
       "      <th>q_0.8</th>\n",
       "      <th>q_0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "      <td>7776.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.112645</td>\n",
       "      <td>10.809464</td>\n",
       "      <td>14.131773</td>\n",
       "      <td>17.161394</td>\n",
       "      <td>18.947621</td>\n",
       "      <td>20.499542</td>\n",
       "      <td>22.147126</td>\n",
       "      <td>23.016097</td>\n",
       "      <td>24.472436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.740513</td>\n",
       "      <td>15.586740</td>\n",
       "      <td>19.482012</td>\n",
       "      <td>23.101532</td>\n",
       "      <td>25.166958</td>\n",
       "      <td>27.000063</td>\n",
       "      <td>28.946973</td>\n",
       "      <td>29.845326</td>\n",
       "      <td>31.362877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>1.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.232500</td>\n",
       "      <td>19.820000</td>\n",
       "      <td>27.355000</td>\n",
       "      <td>34.202500</td>\n",
       "      <td>38.050000</td>\n",
       "      <td>41.540000</td>\n",
       "      <td>45.335000</td>\n",
       "      <td>47.145000</td>\n",
       "      <td>50.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49.790000</td>\n",
       "      <td>69.790000</td>\n",
       "      <td>79.270000</td>\n",
       "      <td>87.570000</td>\n",
       "      <td>91.610000</td>\n",
       "      <td>94.380000</td>\n",
       "      <td>99.010000</td>\n",
       "      <td>98.540000</td>\n",
       "      <td>103.330000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             q_0.1        q_0.2        q_0.3        q_0.4        q_0.5  \\\n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  7776.000000   \n",
       "mean      6.112645    10.809464    14.131773    17.161394    18.947621   \n",
       "std       9.740513    15.586740    19.482012    23.101532    25.166958   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.030000     0.120000   \n",
       "75%      10.232500    19.820000    27.355000    34.202500    38.050000   \n",
       "max      49.790000    69.790000    79.270000    87.570000    91.610000   \n",
       "\n",
       "             q_0.6        q_0.7        q_0.8        q_0.9  \n",
       "count  7776.000000  7776.000000  7776.000000  7776.000000  \n",
       "mean     20.499542    22.147126    23.016097    24.472436  \n",
       "std      27.000063    28.946973    29.845326    31.362877  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.130000     0.290000     0.565000     1.140000  \n",
       "75%      41.540000    45.335000    47.145000    50.002500  \n",
       "max      94.380000    99.010000    98.540000   103.330000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_neu.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
